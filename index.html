<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="description" content="Strategic AWS Certified Developer Associate DVA-C02 Exam Trainer - Master timing and pass the exam">
    <meta name="theme-color" content="#FF9900">
    <link rel="manifest" href="manifest.json">
    <meta name="theme-color" content="#FF9900">
    <title>AWS DVA-C02 Strategic Exam Trainer</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --primary: #FF6B6B;
            --primary-dark: #EE5A5A;
            --primary-light: #FF8787;
            --card-bg: rgba(255, 255, 255, 0.15);
            --card-hover: rgba(255, 255, 255, 0.25);
            --text-white: #FFFFFF;
            --text-light: rgba(255, 255, 255, 0.95);
            --success: #48C774;
            --danger: #FF3860;
            --warning: #FFB900;
            --shadow: 0 10px 40px rgba(0, 0, 0, 0.2);
            --shadow-card: 0 8px 32px rgba(0, 0, 0, 0.15);
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            min-height: 100vh;
            background: linear-gradient(135deg, #FF6B6B 0%, #FF8787 100%);
            display: flex;
            flex-direction: column;
            align-items: center;
            padding: 0;
            overflow-x: hidden;
        }

        .main-header {
            width: 100%;
            text-align: center;
            padding: 40px 20px 30px;
            background: rgba(0, 0, 0, 0.1);
        }

        .main-header h1 {
            color: var(--text-white);
            font-size: 2.8em;
            font-weight: 700;
            margin-bottom: 10px;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 15px;
        }

        .main-header .tagline {
            color: var(--text-light);
            font-size: 1.1em;
            font-weight: 300;
            opacity: 0.95;
        }

        .container {
            width: 100%;
            max-width: 1200px;
            padding: 20px;
            animation: fadeIn 0.6s ease-out;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .hero-section {
            text-align: center;
            margin-bottom: 40px;
            padding: 20px;
        }

        .hero-section h2 {
            color: var(--text-white);
            font-size: 2.2em;
            font-weight: 600;
            margin-bottom: 15px;
        }

        .hero-section p {
            color: var(--text-light);
            font-size: 1.15em;
            max-width: 700px;
            margin: 0 auto;
        }

        .features-card {
            background: var(--card-bg);
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
            border-radius: 20px;
            padding: 30px;
            margin-bottom: 40px;
            box-shadow: var(--shadow-card);
            border: 1px solid rgba(255, 255, 255, 0.2);
        }

        .features-card h3 {
            color: var(--text-white);
            font-size: 1.6em;
            margin-bottom: 20px;
            text-align: center;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 10px;
        }

        .features-card .subtitle {
            text-align: center;
            color: var(--text-light);
            margin-bottom: 25px;
            font-size: 1.05em;
        }

        .features-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
        }

        .feature-item {
            display: flex;
            align-items: center;
            gap: 12px;
            color: var(--text-light);
            font-size: 0.95em;
        }

        .feature-item .icon {
            font-size: 1.3em;
        }

        .timer-section {
            display: flex;
            justify-content: center;
            gap: 20px;
            margin: 20px auto;
            max-width: 1200px;
            padding: 0 20px;
            flex-wrap: wrap;
        }

        .timer-box {
            flex: 1;
            min-width: 180px;
            background: rgba(255, 255, 255, 0.2);
            backdrop-filter: blur(10px);
            border-radius: 15px;
            padding: 20px;
            text-align: center;
            border: 1px solid rgba(255, 255, 255, 0.3);
            box-shadow: 0 5px 20px rgba(0,0,0,0.1);
        }

        .timer-label {
            color: var(--text-light);
            font-size: 0.9em;
            margin-bottom: 8px;
            text-transform: uppercase;
            letter-spacing: 1px;
        }

        .timer-value {
            color: var(--text-white);
            font-size: 2em;
            font-weight: bold;
            font-feature-settings: "tnum";
        }

        .pace-indicator {
            padding: 6px 12px;
            border-radius: 20px;
            font-weight: bold;
            color: white;
            font-size: 0.85em;
            margin-top: 8px;
            display: inline-block;
        }

        .pace-on-track { background: var(--success); }
        .pace-behind { background: var(--warning); }
        .pace-danger { background: var(--danger); }

        .mode-selection {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 25px;
            margin-bottom: 40px;
        }

        .mode-card {
            background: var(--card-bg);
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.25);
            border-radius: 20px;
            padding: 30px;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: var(--shadow-card);
        }

        .mode-card:hover {
            transform: translateY(-8px);
            background: var(--card-hover);
            box-shadow: 0 15px 40px rgba(0,0,0,0.25);
        }

        .mode-icon {
            font-size: 3em;
            margin-bottom: 15px;
            display: block;
        }

        .mode-title {
            color: var(--text-white);
            font-size: 1.5em;
            font-weight: 600;
            margin-bottom: 10px;
        }

        .mode-desc {
            color: var(--text-light);
            font-size: 0.95em;
            line-height: 1.5;
            margin-bottom: 20px;
        }

        .mode-features {
            list-style: none;
            text-align: left;
        }

        .mode-features li {
            color: var(--text-light);
            font-size: 0.9em;
            padding: 6px 0;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .mode-features li:before {
            content: '✓';
            color: var(--text-white);
            font-weight: bold;
        }

        .install-banner {
            background: rgba(255, 255, 255, 0.25);
            backdrop-filter: blur(10px);
            border-radius: 15px;
            padding: 20px;
            margin: 20px;
            display: none;
            align-items: center;
            justify-content: space-between;
            box-shadow: var(--shadow-card);
            animation: slideDown 0.5s ease;
        }

        @keyframes slideDown {
            from {
                opacity: 0;
                transform: translateY(-20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .install-banner .content {
            display: flex;
            align-items: center;
            gap: 15px;
            color: var(--text-white);
        }

        .install-banner .icon {
            font-size: 2em;
        }

        .install-banner .text h4 {
            font-size: 1.1em;
            margin-bottom: 3px;
        }

        .install-banner .text p {
            font-size: 0.9em;
            opacity: 0.9;
        }

        .install-banner .actions {
            display: flex;
            gap: 10px;
        }

        .install-btn {
            background: white;
            color: var(--primary);
            border: none;
            padding: 10px 25px;
            border-radius: 25px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .install-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 5px 15px rgba(255,255,255,0.3);
        }

        .install-btn.secondary {
            background: transparent;
            color: white;
            border: 2px solid white;
        }

        .question-container {
            background: rgba(255, 255, 255, 0.98);
            border-radius: 20px;
            padding: 30px;
            margin: 20px auto;
            max-width: 1000px;
            color: #333;
            display: none;
            box-shadow: 0 10px 40px rgba(0,0,0,0.15);
        }

        .question-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 25px;
            flex-wrap: wrap;
            gap: 15px;
        }

        .question-info {
            display: flex;
            gap: 12px;
            flex-wrap: wrap;
        }

        .badge {
            padding: 6px 14px;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: 600;
        }

        .badge-domain {
            background: var(--primary);
            color: white;
        }

        .badge-difficulty-easy {
            background: var(--success);
            color: white;
        }

        .badge-difficulty-medium {
            background: var(--warning);
            color: white;
        }

        .badge-difficulty-hard {
            background: var(--danger);
            color: white;
        }

        .question-actions {
            display: flex;
            gap: 12px;
        }

        .action-btn {
            padding: 8px 18px;
            border: none;
            border-radius: 20px;
            cursor: pointer;
            font-weight: 600;
            transition: all 0.3s ease;
            font-size: 0.9em;
        }

        .flag-btn {
            background: var(--warning);
            color: white;
        }

        .flag-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 4px 12px rgba(255,185,0,0.3);
        }

        .flag-btn.flagged {
            background: var(--danger);
        }

        .skip-btn {
            background: var(--primary);
            color: white;
        }

        .skip-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 4px 12px rgba(255,107,107,0.3);
        }

        .scenario {
            background: #fff5f5;
            border-left: 4px solid var(--primary);
            padding: 18px;
            margin-bottom: 25px;
            border-radius: 10px;
            line-height: 1.6;
            color: #444;
        }

        .question-text {
            font-size: 1.15em;
            font-weight: 600;
            margin-bottom: 20px;
            line-height: 1.5;
            color: #232F3E;
        }

        .options {
            display: flex;
            flex-direction: column;
            gap: 12px;
        }

        .option {
            background: white;
            border: 2px solid #e0e0e0;
            border-radius: 10px;
            padding: 15px;
            cursor: pointer;
            transition: all 0.3s ease;
            display: flex;
            align-items: flex-start;
            gap: 12px;
            line-height: 1.5;
        }

        .option:hover {
            border-color: var(--aws-orange);
            transform: translateX(5px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }

        .option.selected {
            background: #FFF3E0;
            border-color: var(--aws-orange);
        }

        .option.correct {
            background: #E8F5E9;
            border-color: var(--success);
        }

        .option.incorrect {
            background: #FFEBEE;
            border-color: var(--danger);
        }

        .option-letter {
            min-width: 30px;
            height: 30px;
            border-radius: 50%;
            background: var(--aws-blue);
            color: white;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
        }

        .option-text {
            flex: 1;
        }

        .explanation {
            background: linear-gradient(135deg, #f0f8ff, #f5faff);
            border-radius: 12px;
            padding: 25px;
            margin-top: 25px;
            display: none;
            animation: fadeIn 0.5s ease;
            border: 1px solid #d1e7ff;
        }

        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .explanation h3 {
            color: var(--primary);
            margin-bottom: 15px;
            font-size: 1.3em;
        }

        .explanation-section {
            margin-bottom: 18px;
        }

        .explanation-section h4 {
            color: #555;
            margin-bottom: 10px;
            font-size: 1.1em;
        }

        .strategy-tip {
            background: #FFF5F5;
            border-left: 4px solid var(--primary);
            padding: 15px;
            margin-top: 15px;
            border-radius: 8px;
        }

        .controls {
            display: flex;
            justify-content: space-between;
            gap: 15px;
            margin-top: 25px;
        }

        .controls .btn {
            flex: 1;
        }

        .btn {
            padding: 12px 32px;
            border: none;
            border-radius: 25px;
            font-size: 1em;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            color: white;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .btn-primary {
            background: linear-gradient(135deg, var(--primary), var(--primary-dark));
        }

        .btn-primary:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 20px rgba(255,107,107,0.4);
        }

        .btn-secondary {
            background: linear-gradient(135deg, #6c757d, #5a6268);
        }

        .btn-secondary:hover {
            transform: translateY(-2px);
            box-shadow: 0 8px 20px rgba(108,117,125,0.4);
        }

        .results-container {
            background: rgba(255, 255, 255, 0.98);
            border-radius: 20px;
            padding: 35px;
            color: #333;
            display: none;
            max-width: 1000px;
            margin: 20px auto;
            box-shadow: 0 10px 40px rgba(0,0,0,0.15);
        }

        .results-header {
            text-align: center;
            margin-bottom: 30px;
        }

        .score-circle {
            width: 150px;
            height: 150px;
            margin: 0 auto 20px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 3em;
            font-weight: bold;
            color: white;
            position: relative;
            background: conic-gradient(var(--primary) 0deg, var(--primary) var(--score-deg), #e0e0e0 var(--score-deg));
        }

        .score-text {
            background: white;
            width: 130px;
            height: 130px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            color: #333;
        }

        .pass-fail {
            font-size: 1.5em;
            font-weight: bold;
            margin-bottom: 10px;
        }

        .pass { color: var(--success); }
        .fail { color: var(--danger); }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .stat-card {
            background: #f8f9fa;
            border-radius: 10px;
            padding: 20px;
            text-align: center;
        }

        .stat-label {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 5px;
        }

        .stat-value {
            font-size: 1.8em;
            font-weight: bold;
            color: var(--aws-blue);
        }

        .domain-breakdown {
            margin-top: 30px;
        }

        .domain-item {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 15px;
            margin-bottom: 10px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .domain-name {
            font-weight: 600;
            color: #333;
        }

        .domain-score {
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .progress-bar {
            width: 100px;
            height: 8px;
            background: #e0e0e0;
            border-radius: 4px;
            overflow: hidden;
        }

        .progress-fill {
            height: 100%;
            background: var(--primary);
            transition: width 0.5s ease;
        }

        .loading {
            text-align: center;
            color: white;
            font-size: 1.2em;
            padding: 40px;
        }

        .spinner {
            border: 4px solid rgba(255,255,255,0.3);
            border-top: 4px solid white;
            border-radius: 50%;
            width: 50px;
            height: 50px;
            animation: spin 1s linear infinite;
            margin: 20px auto;
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        @media (max-width: 768px) {
            .main-header h1 {
                font-size: 2em;
            }

            .hero-section h2 {
                font-size: 1.8em;
            }

            .mode-selection {
                grid-template-columns: 1fr;
            }

            .controls {
                flex-direction: column;
            }

            .controls .btn {
                width: 100%;
            }

            .timer-section {
                flex-direction: column;
            }

            .timer-box {
                min-width: 100%;
            }

            .features-grid {
                grid-template-columns: 1fr;
            }
        }

        .review-mode {
            background: rgba(255, 255, 255, 0.2);
            backdrop-filter: blur(10px);
            border-radius: 15px;
            padding: 20px;
            margin: 20px auto;
            max-width: 1000px;
            display: none;
            box-shadow: 0 5px 20px rgba(0,0,0,0.1);
        }

        .review-mode h3 {
            color: var(--text-white);
            margin-bottom: 15px;
        }

        .review-navigation {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin-top: 15px;
        }

        .review-btn {
            width: 45px;
            height: 45px;
            border: 2px solid rgba(255,255,255,0.4);
            background: rgba(255,255,255,0.15);
            border-radius: 10px;
            cursor: pointer;
            font-weight: 600;
            transition: all 0.3s ease;
            color: white;
        }

        .review-btn.answered {
            background: var(--success);
            color: white;
            border-color: var(--success);
        }

        .review-btn.flagged {
            background: var(--warning);
            color: white;
            border-color: var(--warning);
        }

        .review-btn.current {
            background: var(--primary);
            color: white;
            transform: scale(1.1);
            border-color: var(--primary);
        }
    </style>
</head>
<body>
    <div class="main-header">
        <h1>🎯 AWS DVA-C02 Strategic Exam Trainer</h1>
        <p class="tagline">Master exam timing and strategy with 500+ realistic scenarios · Turn 72% fails into 85+ passes</p>
    </div>

    <div class="install-banner" id="installBanner">
        <div class="content">
            <span class="icon">📱</span>
            <div class="text">
                <h4>Install for Offline Practice</h4>
                <p>Practice anytime, anywhere - even without internet</p>
            </div>
        </div>
        <div class="actions">
            <button class="install-btn" onclick="installPWA()">Install App</button>
            <button class="install-btn secondary" onclick="dismissInstall()">Later</button>
        </div>
    </div>

    <div class="container">
        <div class="hero-section">
            <h2>Strategic Exam Training</h2>
            <p>Failed by 2 questions? Time management, not knowledge, was the issue. Master the strategy.</p>
        </div>

        <div class="features-card">
            <h3>🎯 Professional Question Bank</h3>
            <p class="subtitle"><strong>500+ Questions</strong> covering all DVA-C02 services per official exam guide</p>
            <div class="features-grid">
                <div class="feature-item">
                    <span class="icon">🔒</span>
                    <span>150+ Security questions</span>
                </div>
                <div class="feature-item">
                    <span class="icon">🚀</span>
                    <span>130+ Deployment scenarios</span>
                </div>
                <div class="feature-item">
                    <span class="icon">⚡</span>
                    <span>120+ Performance cases</span>
                </div>
                <div class="feature-item">
                    <span class="icon">💰</span>
                    <span>100+ Cost optimization</span>
                </div>
                <div class="feature-item">
                    <span class="icon">🔄</span>
                    <span>Smart question rotation</span>
                </div>
                <div class="feature-item">
                    <span class="icon">🧠</span>
                    <span>Anti-memorization system</span>
                </div>
            </div>
        </div>

        <div class="mode-selection" id="modeSelection">
            <div class="mode-card" onclick="startMode('triage')">
                <span class="mode-icon">⚡</span>
                <div class="mode-title">Triage Training</div>
                <p class="mode-desc">Learn to identify quick wins vs time sinks in under 30 seconds</p>
                <ul class="mode-features">
                    <li>Question difficulty recognition</li>
                    <li>30-second decision drills</li>
                    <li>Strategic skip/answer patterns</li>
                </ul>
            </div>

            <div class="mode-card" onclick="startMode('speed')">
                <span class="mode-icon">🔥</span>
                <div class="mode-title">Speed Rounds</div>
                <p class="mode-desc">90-second maximum per question - build exam pace muscle memory</p>
                <ul class="mode-features">
                    <li>Hard 90-second time limits</li>
                    <li>Automatic progression</li>
                    <li>Pace analytics</li>
                </ul>
            </div>

            <div class="mode-card" onclick="startMode('exam')">
                <span class="mode-icon">🎯</span>
                <div class="mode-title">Full Exam Simulation</div>
                <p class="mode-desc">Real 130-minute, 65-question exam with strategic coaching</p>
                <ul class="mode-features">
                    <li>Authentic time pressure</li>
                    <li>Real exam difficulty</li>
                    <li>Strategic alerts & guidance</li>
                </ul>
            </div>

            <div class="mode-card" onclick="startMode('strategic')">
                <span class="mode-icon">🧠</span>
                <div class="mode-title">Strategic Practice</div>
                <p class="mode-desc">Learn mark/skip/return workflow with complex scenarios</p>
                <ul class="mode-features">
                    <li>Flag & review system</li>
                    <li>Time allocation practice</li>
                    <li>Decision tree training</li>
                </ul>
            </div>
        </div>

        <div class="timer-section" id="timerSection" style="display: none;">
            <div class="timer-box">
                <div class="timer-label">Global Timer</div>
                <div class="timer-value" id="globalTimer">00:00</div>
            </div>
            <div class="timer-box">
                <div class="timer-label">Question Time</div>
                <div class="timer-value" id="questionTimer">00:00</div>
            </div>
            <div class="timer-box">
                <div class="timer-label">Pace Status</div>
                <div class="timer-value" id="paceValue">--</div>
                <div class="pace-indicator pace-on-track" id="paceIndicator">ON TRACK</div>
            </div>
        </div>

        <div class="review-mode" id="reviewMode">
            <h3>📋 Review Navigation</h3>
            <div class="review-navigation" id="reviewNavigation"></div>
        </div>

        <div class="question-container" id="questionContainer">
            <div class="question-header">
                <div class="question-info">
                    <span class="badge badge-domain" id="domainBadge">Domain 1</span>
                    <span class="badge badge-difficulty-medium" id="difficultyBadge">Medium</span>
                    <span class="badge" id="questionNumber">Q1 of 65</span>
                </div>
                <div class="question-actions">
                    <button class="action-btn flag-btn" id="flagBtn" onclick="toggleFlag()">🚩 Flag</button>
                    <button class="action-btn skip-btn" onclick="skipQuestion()">⏭️ Skip</button>
                </div>
            </div>
            
            <div class="scenario" id="scenario"></div>
            <div class="question-text" id="questionText"></div>
            
            <div class="options" id="options"></div>
            
            <div class="explanation" id="explanation">
                <h3>📚 Explanation</h3>
                <div class="explanation-section">
                    <h4>✅ Correct Answer:</h4>
                    <p id="correctExplanation"></p>
                </div>
                <div class="explanation-section">
                    <h4>❌ Why Other Options Are Wrong:</h4>
                    <div id="wrongExplanations"></div>
                </div>
                <div class="strategy-tip">
                    <h4>💡 Exam Strategy Tip:</h4>
                    <p id="strategyTip"></p>
                </div>
            </div>
            
            <div class="controls">
                <button class="btn btn-secondary" id="prevBtn" onclick="previousQuestion()">Previous</button>
                <button class="btn btn-primary" id="submitBtn" onclick="submitAnswer()">Submit Answer</button>
                <button class="btn btn-primary" id="nextBtn" onclick="nextQuestion()" style="display: none;">Next Question</button>
            </div>
        </div>

        <div class="results-container" id="resultsContainer">
            <div class="results-header">
                <h2>📊 Exam Results</h2>
                <div class="score-circle" style="--score-deg: 0deg;">
                    <div class="score-text" id="scoreText">0%</div>
                </div>
                <div class="pass-fail" id="passFail">CALCULATING...</div>
                <p id="resultMessage"></p>
            </div>
            
            <div class="stats-grid">
                <div class="stat-card">
                    <div class="stat-label">Questions Correct</div>
                    <div class="stat-value" id="correctCount">0/0</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Time Used</div>
                    <div class="stat-value" id="timeUsed">00:00</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Avg Time/Question</div>
                    <div class="stat-value" id="avgTime">0s</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Questions Flagged</div>
                    <div class="stat-value" id="flaggedCount">0</div>
                </div>
            </div>
            
            <div class="domain-breakdown">
                <h3>Domain Performance</h3>
                <div id="domainResults"></div>
            </div>
            
            <div class="controls">
                <button class="btn btn-primary" onclick="reviewAnswers()">Review Answers</button>
                <button class="btn btn-secondary" onclick="resetQuiz()">Start New Session</button>
            </div>
        </div>

        <div class="loading" id="loading" style="display: none;">
            <div class="spinner"></div>
            <p>Loading questions...</p>
        </div>
    </div>

    <script>
        // Map questionBank to questions array
if (typeof questionBank !== 'undefined') {
    // If questionBank is an object with categories
    if (typeof questionBank === 'object' && !Array.isArray(questionBank)) {
        // Combine all questions from all categories into one array
        window.questions = [];
        for (let category in questionBank) {
            if (Array.isArray(questionBank[category])) {
                window.questions = window.questions.concat(questionBank[category]);
                console.log(`Added ${questionBank[category].length} questions from ${category}`);
            }
        }
        console.log(`Total questions loaded: ${window.questions.length}`);
    } else if (Array.isArray(questionBank)) {
        // If it's already an array
        window.questions = questionBank;
    }
} else {
    console.error('No question bank found!');
    // Create fallback questions
    window.questions = [
        {
            question: "Sample question for testing",
            options: ["Option A", "Option B", "Option C", "Option D"],
            correct: 0,
            explanation: "This is a test question"
        }
    ];
}

// Verify questions are loaded
console.log('Questions ready:', window.questions?.length || 0);
        // Question Bank - AWS DVA-C02 Specific Questions
        const questionBank = [
            // Domain 1: Development with AWS Services (32%)
{
    id: 'dva_001',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A development team is building a serverless application that processes user uploads. Files are stored in S3, metadata is stored in DynamoDB, and Lambda functions handle processing. The application experiences occasional spikes where thousands of files are uploaded simultaneously, causing some Lambda invocations to be throttled.",
    question: "What is the MOST cost-effective solution to handle these traffic spikes while maintaining processing reliability?",
    options: [
        "Configure S3 Event Notifications to send messages to an SQS queue, and configure Lambda to poll from the queue with reserved concurrency",
        "Increase the Lambda concurrent execution limit to 10,000 and implement exponential backoff in the upload client",
        "Use AWS Step Functions to orchestrate the processing workflow with built-in retry logic",
        "Configure S3 Transfer Acceleration and increase Lambda memory to 10GB for faster processing"
    ],
    correct: 0,
    explanation: {
        correct: "Using SQS as a buffer between S3 and Lambda provides reliable message delivery and automatic retry logic. Reserved concurrency prevents throttling while the queue handles traffic spikes, making this the most cost-effective solution.",
        whyWrong: {
            1: "Simply increasing concurrent execution limits doesn't guarantee processing and could lead to higher costs without solving the root cause",
            2: "Step Functions adds unnecessary complexity and cost for this use case when SQS provides simpler buffering",
            3: "Transfer Acceleration and increased memory don't address the throttling issue and significantly increase costs"
        },
        examStrategy: "Look for solutions that use managed services for buffering (SQS, Kinesis) when dealing with traffic spikes and Lambda throttling."
    }
},
            {
                id: 'dva_002',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "hard",
                timeRecommendation: 150,
                scenario: "An e-commerce application uses DynamoDB for its product catalog with a partition key of ProductID and sort key of Timestamp. The application needs to frequently query products by category and price range. Current query patterns are causing performance issues and high read costs.",
                question: "Which solution provides the BEST query performance while minimizing costs?",
                options: [
                    "Create a Global Secondary Index with Category as partition key and Price as sort key",
                    "Migrate to Amazon RDS MySQL and create composite indexes on Category and Price",
                    "Use DynamoDB Streams to replicate data to ElasticSearch for complex queries",
                    "Implement scan operations with FilterExpression for Category and Price attributes"
                ],
                correct: 0,
                explanation: {
                    correct: "A GSI with Category as partition key and Price as sort key enables efficient queries for the exact access pattern needed. This provides the best performance at the lowest cost for DynamoDB.",
                    whyWrong: {
                        1: "Migrating to RDS involves significant refactoring and loses DynamoDB's scalability benefits",
                        2: "ElasticSearch adds complexity and cost for a simple query pattern that GSI can handle",
                        3: "Scan operations are expensive and slow, especially as the table grows"
                    },
                    examStrategy: "GSIs are the go-to solution for alternative query patterns in DynamoDB. Remember: up to 20 GSIs per table."
                }
            },
            {
                id: 'dva_003',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "easy",
                timeRecommendation: 90,
                scenario: "A Lambda function processes messages from an SQS queue and occasionally encounters errors due to malformed data. The team wants to ensure problematic messages don't block the queue while maintaining the ability to analyze failures.",
                question: "What is the MOST appropriate solution?",
                options: [
                    "Configure an SQS Dead Letter Queue with maxReceiveCount set to 3",
                    "Implement try-catch blocks in Lambda and delete messages on error",
                    "Set the SQS visibility timeout to 12 hours for manual intervention",
                    "Use SQS FIFO queue to ensure ordered processing"
                ],
                correct: 0,
                explanation: {
                    correct: "A Dead Letter Queue automatically moves messages that fail processing multiple times, preventing queue blocking while preserving failed messages for analysis.",
                    whyWrong: {
                        1: "Deleting messages on error loses valuable debugging information",
                        2: "12-hour visibility timeout blocks the queue and doesn't solve the root problem",
                        3: "FIFO queues don't address error handling and reduce throughput"
                    },
                    examStrategy: "DLQ is the standard pattern for handling failures in SQS processing. Always consider it for error scenarios."
                }
            },
            {
                id: 'dva_004',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "A mobile application needs to upload photos directly to S3 without going through the application backend. The uploads should be secure and the mobile app should not store AWS credentials.",
                question: "Which approach provides the MOST secure solution?",
                options: [
                    "Use AWS Cognito to authenticate users and obtain temporary credentials via STS",
                    "Generate pre-signed URLs on the backend and send them to the mobile app",
                    "Embed IAM access keys in the mobile app with restricted S3 permissions",
                    "Use S3 Transfer Acceleration with public write access and CloudFront for security"
                ],
                correct: 0,
                explanation: {
                    correct: "Cognito with STS provides temporary, scoped credentials perfect for mobile apps. It's the most secure and scalable approach for direct S3 uploads from mobile devices.",
                    whyWrong: {
                        1: "Pre-signed URLs work but require backend calls for each upload, adding latency",
                        2: "Embedding IAM credentials in mobile apps is a critical security violation",
                        3: "Public write access is extremely insecure and Transfer Acceleration doesn't provide authentication"
                    },
                    examStrategy: "Cognito + STS is the standard pattern for mobile app authentication and AWS service access. Never embed credentials."
                }
            },
            {
                id: 'dva_005',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "hard",
                timeRecommendation: 150,
                scenario: "A financial services application requires all API responses to be cached for exactly 5 minutes for regulatory compliance. The cache must be invalidated immediately when underlying data changes. The application uses API Gateway with Lambda backends.",
                question: "Which caching strategy meets these requirements?",
                options: [
                    "Enable API Gateway caching with 300-second TTL and use cache invalidation API when data changes",
                    "Implement ElastiCache Redis with 5-minute TTL and Lambda functions for cache management",
                    "Use CloudFront with 5-minute cache headers and create invalidations on data updates",
                    "Store responses in DynamoDB with TTL attribute set to 5 minutes"
                ],
                correct: 0,
                explanation: {
                    correct: "API Gateway caching with explicit TTL and programmatic invalidation provides the exact control needed for compliance while being the simplest to implement.",
                    whyWrong: {
                        1: "ElastiCache adds complexity and requires Lambda invocation for every request to check cache",
                        2: "CloudFront invalidations have costs and delays, not suitable for immediate invalidation needs",
                        3: "DynamoDB with TTL doesn't provide immediate invalidation and adds unnecessary database calls"
                    },
                    examStrategy: "API Gateway caching is ideal for API-level caching with precise control. Remember the 300-second default TTL."
                }
            },

            // Domain 2: Security (26%)
            {
                id: 'dva_006',
                domain: "Domain 2: Security",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "A healthcare application stores sensitive patient data in RDS MySQL. Compliance requires encryption at rest, in transit, and the ability to rotate encryption keys annually. The application must maintain high availability during key rotation.",
                question: "Which configuration meets ALL these requirements?",
                options: [
                    "Enable RDS encryption with AWS KMS CMK, force SSL connections, and use automatic key rotation",
                    "Use AWS Secrets Manager for database credentials and application-level encryption with rotated keys",
                    "Implement RDS Multi-AZ with TDE, SSL certificates, and manual key rotation procedures",
                    "Configure Aurora Serverless with AWS KMS default keys and IAM database authentication"
                ],
                correct: 0,
                explanation: {
                    correct: "RDS encryption with KMS CMK provides encryption at rest, SSL ensures encryption in transit, and KMS automatic rotation handles key rotation without downtime.",
                    whyWrong: {
                        1: "Application-level encryption doesn't meet the at-rest requirement for the database itself",
                        2: "TDE requires Enterprise Edition and manual rotation causes downtime",
                        3: "Default KMS keys cannot be rotated manually and don't meet the annual rotation requirement"
                    },
                    examStrategy: "KMS CMKs with automatic rotation are the standard for RDS encryption requirements. Remember: automatic rotation = no downtime."
                }
            },
            {
                id: 'dva_007',
                domain: "Domain 2: Security",
                difficulty: "hard",
                timeRecommendation: 150,
                scenario: "A multi-tenant SaaS application needs to implement row-level security in DynamoDB. Each tenant's data must be completely isolated, and developers should not have access to production tenant data. The solution must support 10,000+ tenants.",
                question: "What is the MOST scalable and secure approach?",
                options: [
                    "Use fine-grained access control with IAM policies containing dynamodb:LeadingKeys condition",
                    "Create separate DynamoDB tables for each tenant with unique IAM roles",
                    "Implement client-side encryption with tenant-specific KMS keys",
                    "Use DynamoDB global tables with tenant-specific AWS accounts"
                ],
                correct: 0,
                explanation: {
                    correct: "Fine-grained access control with LeadingKeys condition enables row-level security at scale, perfect for multi-tenant applications with a partition key per tenant.",
                    whyWrong: {
                        1: "Separate tables for 10,000+ tenants is unmanageable and hits service limits",
                        2: "Client-side encryption doesn't prevent developer access to encrypted data",
                        3: "Separate AWS accounts for each tenant is extremely complex and costly"
                    },
                    examStrategy: "LeadingKeys condition is the key to DynamoDB row-level security. Perfect for multi-tenant scenarios."
                }
            },
            {
                id: 'dva_008',
                domain: "Domain 2: Security",
                difficulty: "easy",
                timeRecommendation: 90,
                scenario: "A Lambda function needs to access secrets like database passwords and API keys. The secrets must be rotated regularly without code changes.",
                question: "What is the RECOMMENDED approach?",
                options: [
                    "Store secrets in AWS Secrets Manager and retrieve them using the AWS SDK",
                    "Use Lambda environment variables encrypted with KMS",
                    "Store encrypted secrets in S3 and decrypt in Lambda code",
                    "Hard-code encrypted values and use KMS for decryption"
                ],
                correct: 0,
                explanation: {
                    correct: "Secrets Manager is designed specifically for this use case, providing automatic rotation, versioning, and easy SDK integration.",
                    whyWrong: {
                        1: "Environment variables require Lambda redeployment for rotation",
                        2: "S3 requires custom rotation logic and additional complexity",
                        3: "Hard-coded values violate security best practices even when encrypted"
                    },
                    examStrategy: "Secrets Manager is the go-to service for managing secrets with rotation. Parameter Store for static configs."
                }
            },
            {
                id: 'dva_009',
                domain: "Domain 2: Security",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "An API Gateway REST API needs to authenticate requests from a React single-page application. The solution must support social logins (Google, Facebook) and custom user registration with email/password.",
                question: "Which authentication approach is MOST appropriate?",
                options: [
                    "Use Cognito User Pools with Cognito Authorizer in API Gateway",
                    "Implement Lambda Authorizer with JWT validation for social tokens",
                    "Use IAM authorization with STS AssumeRoleWithWebIdentity",
                    "Configure API Gateway API Keys with usage plans"
                ],
                correct: 0,
                explanation: {
                    correct: "Cognito User Pools natively support both social providers and custom registration, with built-in API Gateway integration via Cognito Authorizer.",
                    whyWrong: {
                        1: "Lambda Authorizer requires custom implementation of social login integration",
                        2: "IAM with AssumeRole doesn't support custom user registration easily",
                        3: "API Keys are for usage tracking, not user authentication"
                    },
                    examStrategy: "Cognito User Pools = user authentication. Cognito Identity Pools = AWS resource access."
                }
            },
            {
                id: 'dva_010',
                domain: "Domain 2: Security",
                difficulty: "hard",
                timeRecommendation: 140,
                scenario: "A company needs to implement API throttling that varies by customer tier: Free (10 req/sec), Pro (100 req/sec), Enterprise (1000 req/sec). The solution must track usage for billing and prevent abuse.",
                question: "Which combination provides the MOST complete solution?",
                options: [
                    "API Gateway Usage Plans with API Keys and CloudWatch metrics for billing",
                    "Lambda@Edge with DynamoDB for rate counting and CloudFront for caching",
                    "ALB with WAF rate-based rules and custom Lambda for tier management",
                    "API Gateway with Lambda Authorizer implementing token bucket algorithm"
                ],
                correct: 0,
                explanation: {
                    correct: "Usage Plans with API Keys are purpose-built for tier-based throttling with built-in metrics for billing, providing a complete managed solution.",
                    whyWrong: {
                        1: "Lambda@Edge adds complexity and latency for a problem API Gateway solves natively",
                        2: "WAF rate rules don't support tier-based throttling efficiently",
                        3: "Custom token bucket implementation is unnecessary when Usage Plans exist"
                    },
                    examStrategy: "API Gateway Usage Plans are the standard for API monetization and tier-based access control."
                }
            },

            // Domain 3: Deployment (24%)
            {
                id: 'dva_011',
                domain: "Domain 3: Deployment",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "A development team uses CodePipeline for CI/CD. They need to deploy a containerized application to ECS Fargate with automatic rollback on failures and gradual traffic shifting to minimize risk.",
                question: "Which deployment configuration achieves these requirements?",
                options: [
                    "Use CodeDeploy with ECS blue/green deployment and CloudWatch alarms for automatic rollback",
                    "Implement Lambda functions to orchestrate ECS service updates with custom health checks",
                    "Configure CodeBuild to push to ECR and trigger ECS rolling updates",
                    "Use CloudFormation StackSets with drift detection for multi-region deployment"
                ],
                correct: 0,
                explanation: {
                    correct: "CodeDeploy's blue/green deployment for ECS provides traffic shifting, automatic rollback based on CloudWatch alarms, and zero-downtime deployments.",
                    whyWrong: {
                        1: "Custom Lambda orchestration is complex and lacks CodeDeploy's built-in safety features",
                        2: "Rolling updates don't provide easy rollback or traffic shifting capabilities",
                        3: "StackSets are for multi-account/region, not for ECS blue/green deployments"
                    },
                    examStrategy: "CodeDeploy blue/green is the standard for safe ECS deployments. Remember: supports both EC2 and Fargate."
                }
            },
            {
                id: 'dva_012',
                domain: "Domain 3: Deployment",
                difficulty: "easy",
                timeRecommendation: 90,
                scenario: "A Lambda function deployment failed and caused production issues. The team needs to quickly revert to the previous working version while investigating the problem.",
                question: "What is the FASTEST way to restore service?",
                options: [
                    "Update the Lambda alias to point to the previous version number",
                    "Redeploy the previous code package from S3",
                    "Use AWS Backup to restore the Lambda function",
                    "Create a new Lambda function with the old code"
                ],
                correct: 0,
                explanation: {
                    correct: "Lambda aliases can be instantly updated to point to any existing version, providing immediate rollback without redeployment.",
                    whyWrong: {
                        1: "Redeploying takes time and might introduce new issues",
                        2: "AWS Backup doesn't support Lambda functions",
                        3: "Creating new functions requires updating all integrations and permissions"
                    },
                    examStrategy: "Lambda aliases enable instant version switching. Always use aliases in production for quick rollbacks."
                }
            },
            {
                id: 'dva_013',
                domain: "Domain 3: Deployment",
                difficulty: "hard",
                timeRecommendation: 150,
                scenario: "A company needs to deploy a complex microservices application across multiple environments (dev, staging, prod) with different configurations. The infrastructure includes ECS, RDS, ElastiCache, and SQS. Deployments must be repeatable and configuration drift must be detected.",
                question: "Which approach provides the BEST deployment solution?",
                options: [
                    "Use AWS CDK with environment-specific stacks and CloudFormation drift detection",
                    "Create AMIs for each environment and use Systems Manager for configuration",
                    "Implement Terraform with workspaces and remote state in S3",
                    "Use Elastic Beanstalk with saved configurations for each environment"
                ],
                correct: 0,
                explanation: {
                    correct: "CDK provides type-safe infrastructure code with native CloudFormation integration, and drift detection ensures configuration compliance.",
                    whyWrong: {
                        1: "AMIs don't handle the full infrastructure stack (RDS, ElastiCache, SQS)",
                        2: "Terraform works but CDK provides better AWS service integration and drift detection",
                        3: "Elastic Beanstalk doesn't support the full complexity of the microservices architecture"
                    },
                    examStrategy: "CDK/CloudFormation is AWS's preferred IaC solution. Remember: CDK synthesizes to CloudFormation."
                }
            },
            {
                id: 'dva_014',
                domain: "Domain 3: Deployment",
                difficulty: "medium",
                timeRecommendation: 110,
                scenario: "A team needs to implement canary deployments for an API Gateway REST API backed by Lambda functions. They want to route 10% of traffic to the new version for 30 minutes before full deployment.",
                question: "How should this be configured?",
                options: [
                    "Configure API Gateway stage canary settings with 10% traffic and CodeDeploy for Lambda",
                    "Use Route 53 weighted routing with two API Gateway deployments",
                    "Implement custom routing logic in Lambda with random traffic distribution",
                    "Deploy to multiple Lambda aliases and use CloudFront for traffic splitting"
                ],
                correct: 0,
                explanation: {
                    correct: "API Gateway native canary deployment features provide simple percentage-based traffic splitting at the stage level, perfect for this use case.",
                    whyWrong: {
                        1: "Route 53 weighted routing requires multiple API endpoints and doesn't integrate with deployment",
                        2: "Custom Lambda logic adds unnecessary complexity for built-in functionality",
                        3: "CloudFront doesn't provide API-level canary features"
                    },
                    examStrategy: "API Gateway has built-in canary deployment support. Don't overcomplicate with external solutions."
                }
            },
            {
                id: 'dva_015',
                domain: "Domain 3: Deployment",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "A development team needs to ensure their Node.js application dependencies are consistent across development, testing, and production environments. The application is deployed to Elastic Beanstalk.",
                question: "What is the BEST practice for managing dependencies?",
                options: [
                    "Include package-lock.json in source control and use npm ci in deployment scripts",
                    "Vendor all node_modules in the deployment package",
                    "Use Lambda Layers to share dependencies across environments",
                    "Configure Elastic Beanstalk to cache dependencies between deployments"
                ],
                correct: 0,
                explanation: {
                    correct: "package-lock.json ensures exact dependency versions, and npm ci provides fast, reliable installation from lock files, ensuring consistency.",
                    whyWrong: {
                        1: "Vendoring node_modules increases package size and can cause platform-specific issues",
                        2: "Lambda Layers aren't applicable to Elastic Beanstalk deployments",
                        3: "Caching doesn't guarantee version consistency across environments"
                    },
                    examStrategy: "Lock files (package-lock.json, Pipfile.lock) are crucial for dependency management. Always use them."
                }
            },

            // Domain 4: Troubleshooting and Optimization (18%)
            {
                id: 'dva_016',
                domain: "Domain 4: Troubleshooting and Optimization",
                difficulty: "hard",
                timeRecommendation: 140,
                scenario: "A Lambda function processing Kinesis records experiences occasional timeout errors after 5 minutes. CloudWatch shows memory usage at 40% and the function successfully processes 95% of records. Timeouts occur during peak traffic periods.",
                question: "What is the MOST likely cause and solution?",
                options: [
                    "Database connection pool exhaustion; implement connection pooling with RDS Proxy",
                    "Insufficient memory allocation; increase Lambda memory to reduce CPU throttling",
                    "Cold start latency; enable Provisioned Concurrency for the function",
                    "Kinesis shard iterator expiration; reduce batch size and processing time"
                ],
                correct: 0,
                explanation: {
                    correct: "Connection pool exhaustion during peak traffic is a common cause of timeouts when memory usage is low. RDS Proxy manages connections efficiently.",
                    whyWrong: {
                        1: "Low memory usage indicates CPU isn't the bottleneck",
                        2: "Cold starts wouldn't cause timeouts after function is warm",
                        3: "Shard iterator issues would show different error patterns"
                    },
                    examStrategy: "Low memory usage + timeouts often = external resource bottleneck (DB connections, API limits)."
                }
            },
            {
                id: 'dva_017',
                domain: "Domain 4: Troubleshooting and Optimization",
                difficulty: "medium",
                timeRecommendation: 110,
                scenario: "An application's DynamoDB costs have increased 300% over the past month. CloudWatch metrics show consistent read/write traffic, but the table size has grown marginally. Application logs show no errors.",
                question: "What is the MOST likely cause of the cost increase?",
                options: [
                    "Hot partitions causing automatic scaling to increase provisioned capacity",
                    "Forgotten Global Secondary Index with high provisioned throughput",
                    "DynamoDB Streams enabled with no consumer causing data accumulation",
                    "Point-in-time recovery was enabled without notification"
                ],
                correct: 0,
                explanation: {
                    correct: "Hot partitions trigger auto-scaling even when overall traffic is stable, significantly increasing costs without visible errors.",
                    whyWrong: {
                        1: "GSI costs would be visible in billing breakdown and wouldn't suddenly appear",
                        2: "Streams don't accumulate costs based on unconsumed data",
                        3: "PITR adds about 20% cost, not 300%"
                    },
                    examStrategy: "Hot partitions are a common DynamoDB issue. Look for uneven key distribution when costs spike."
                }
            },
            {
                id: 'dva_018',
                domain: "Domain 4: Troubleshooting and Optimization",
                difficulty: "easy",
                timeRecommendation: 90,
                scenario: "Users report intermittent 504 Gateway Timeout errors from an API Gateway REST API. The backend Lambda function completes successfully according to CloudWatch Logs.",
                question: "What is the MOST likely cause?",
                options: [
                    "Lambda function execution time exceeds API Gateway's 29-second timeout limit",
                    "API Gateway throttling due to exceeding 10,000 requests per second",
                    "Lambda concurrent execution limit reached",
                    "VPC-enabled Lambda cannot reach RDS database"
                ],
                correct: 0,
                explanation: {
                    correct: "API Gateway has a hard 29-second timeout limit. If Lambda runs longer but succeeds, API Gateway returns 504.",
                    whyWrong: {
                        1: "Throttling returns 429 Too Many Requests, not 504",
                        2: "Concurrent execution limits cause Lambda throttling (429), not Gateway timeouts",
                        3: "VPC connectivity issues would cause Lambda failures, not success with Gateway timeout"
                    },
                    examStrategy: "Remember API Gateway's 29-second timeout. It's a common cause of 504 errors with long-running Lambdas."
                }
            },
            {
                id: 'dva_019',
                domain: "Domain 4: Troubleshooting and Optimization",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "An ECS Fargate service experiences memory allocation failures during container startup, despite tasks configured with 4GB memory. The application runs fine locally with 2GB RAM.",
                question: "What is the MOST likely issue?",
                options: [
                    "Container definition memory hard limit is set lower than memory reservation",
                    "Fargate platform version doesn't support the requested memory configuration",
                    "Application memory leak during initialization in production environment",
                    "ECS service auto-scaling is conflicting with task memory requirements"
                ],
                correct: 0,
                explanation: {
                    correct: "Mismatched memory settings between hard and soft limits in task definition is a common cause of allocation failures.",
                    whyWrong: {
                        1: "Fargate supports 4GB memory across all platform versions",
                        2: "Memory leaks would cause gradual failure, not immediate startup issues",
                        3: "Service auto-scaling doesn't affect individual task memory allocation"
                    },
                    examStrategy: "Check task definition memory/cpu compatibility matrix and hard/soft limit settings for ECS issues."
                }
            },
            {
                id: 'dva_020',
                domain: "Domain 4: Troubleshooting and Optimization",
                difficulty: "hard",
                timeRecommendation: 150,
                scenario: "X-Ray traces show a Lambda function making DynamoDB queries with 50ms latency in us-east-1, but the same queries from EC2 instances in the same region show 5ms latency. Both use the same SDK and IAM role.",
                question: "What explains this performance difference?",
                options: [
                    "Lambda functions run in a separate network with additional NAT traversal overhead",
                    "Lambda SDK includes additional retry logic not present in EC2",
                    "EC2 instances use placement groups for optimized network performance",
                    "DynamoDB throttles Lambda requests differently than EC2 requests"
                ],
                correct: 0,
                explanation: {
                    correct: "Lambda functions run in AWS-managed VPC with additional network hops and NAT traversal, adding latency compared to EC2.",
                    whyWrong: {
                        1: "SDK retry logic is identical regardless of compute platform",
                        2: "Placement groups don't affect DynamoDB query latency",
                        3: "DynamoDB doesn't discriminate throttling based on compute source"
                    },
                    examStrategy: "Lambda networking has inherent overhead. Consider this when comparing performance with EC2."
                }
            },

            // Additional questions to reach 500+ total
            // Domain 1: Development with AWS Services (continued)
            {
                id: 'dva_021',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "A real-time analytics application ingests 10,000 events per second from IoT devices. Events must be processed within 2 seconds, stored for 7 days for replay, and aggregated data must be available for dashboards.",
                question: "Which architecture provides the BEST solution?",
                options: [
                    "Kinesis Data Streams → Kinesis Analytics → Lambda → DynamoDB and S3",
                    "API Gateway → SQS FIFO → Lambda → RDS PostgreSQL",
                    "IoT Core → DynamoDB Streams → Lambda → ElasticSearch",
                    "EventBridge → Step Functions → ECS Tasks → Redshift"
                ],
                correct: 0,
                explanation: {
                    correct: "Kinesis Data Streams handles high-throughput ingestion with replay capability, Analytics provides real-time aggregation, and Lambda processes for storage.",
                    whyWrong: {
                        1: "SQS FIFO has 300 TPS limit, far below requirements",
                        2: "DynamoDB Streams doesn't support 7-day retention for replay",
                        3: "EventBridge has 10,000 events/second limit per region but lacks replay"
                    },
                    examStrategy: "Kinesis Data Streams is the go-to for high-throughput real-time data with replay needs."
                }
            },
            {
                id: 'dva_022',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "easy",
                timeRecommendation: 90,
                scenario: "A web application needs to send email notifications to users for account activities. The solution must handle bounces and complaints automatically.",
                question: "Which AWS service is MOST appropriate?",
                options: [
                    "Amazon SES with SNS notifications for bounce and complaint handling",
                    "Amazon SNS with email subscription endpoints",
                    "Lambda function using SMTP library to send emails",
                    "Amazon WorkMail with custom email rules"
                ],
                correct: 0,
                explanation: {
                    correct: "SES is designed for transactional emails with built-in bounce and complaint handling via SNS notifications.",
                    whyWrong: {
                        1: "SNS email subscriptions require user confirmation and don't handle bounces",
                        2: "SMTP libraries require managing email infrastructure and bounce handling",
                        3: "WorkMail is for business email, not transactional notifications"
                    },
                    examStrategy: "SES for transactional/marketing emails, SNS for pub/sub notifications, WorkMail for business email."
                }
            },
            {
                id: 'dva_023',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "hard",
                timeRecommendation: 140,
                scenario: "A video streaming platform needs to support 100,000 concurrent viewers with sub-3-second latency. Content is stored in S3 and needs adaptive bitrate streaming for various device types.",
                question: "Which solution provides the BEST viewing experience?",
                options: [
                    "CloudFront with S3 origin, MediaConvert for HLS preparation, and CloudFront signed cookies",
                    "EC2 Auto Scaling group with Nginx RTMP servers and ELB",
                    "API Gateway WebSocket connections with Lambda streaming functions",
                    "Kinesis Video Streams with real-time transcoding"
                ],
                correct: 0,
                explanation: {
                    correct: "CloudFront provides global distribution with low latency, MediaConvert creates HLS for adaptive streaming, and signed cookies enable access control.",
                    whyWrong: {
                        1: "EC2-based solution requires significant infrastructure management and scaling complexity",
                        2: "API Gateway WebSockets aren't designed for video streaming",
                        3: "Kinesis Video Streams is for ingestion, not distribution to viewers"
                    },
                    examStrategy: "CloudFront + MediaConvert is the standard for scalable video delivery. Remember HLS for adaptive streaming."
                }
            },
            {
                id: 'dva_024',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "medium",
                timeRecommendation: 110,
                scenario: "A mobile game needs to store player profiles, match history, and real-time leaderboards. The solution must support millions of players with millisecond response times.",
                question: "Which database strategy is MOST suitable?",
                options: [
                    "DynamoDB for profiles and history, ElastiCache Redis Sorted Sets for leaderboards",
                    "Aurora MySQL with read replicas for all data",
                    "Neptune graph database for player relationships and scoring",
                    "DocumentDB for JSON player data with aggregation pipelines"
                ],
                correct: 0,
                explanation: {
                    correct: "DynamoDB provides scalable storage for profiles, while Redis Sorted Sets are purpose-built for real-time leaderboards with millisecond latency.",
                    whyWrong: {
                        1: "Aurora read replicas still have replication lag for leaderboard updates",
                        2: "Neptune is for graph relationships, not optimized for leaderboards",
                        3: "DocumentDB aggregations aren't real-time enough for live leaderboards"
                    },
                    examStrategy: "Redis Sorted Sets are the standard for leaderboards. Combine services for optimal performance."
                }
            },
            {
                id: 'dva_025',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "An application needs to process large CSV files (100GB+) uploaded to S3, perform data transformations, and load results into Redshift for analytics.",
                question: "What is the MOST efficient processing approach?",
                options: [
                    "Use AWS Glue ETL jobs with Spark for distributed processing",
                    "Lambda function with S3 Select to query and transform data",
                    "ECS Fargate tasks with Python pandas for processing",
                    "EMR cluster with Hadoop MapReduce jobs"
                ],
                correct: 0,
                explanation: {
                    correct: "Glue ETL with Spark handles large-scale data processing efficiently with automatic scaling and native Redshift integration.",
                    whyWrong: {
                        1: "Lambda has 10GB memory limit, insufficient for 100GB files",
                        2: "Fargate with pandas would require chunking and complex orchestration",
                        3: "EMR requires more management overhead compared to serverless Glue"
                    },
                    examStrategy: "Glue is the serverless choice for ETL. Use EMR when you need specific Hadoop ecosystem tools."
                }
            },

            // Domain 2: Security (continued)
            {
                id: 'dva_026',
                domain: "Domain 2: Security",
                difficulty: "hard",
                timeRecommendation: 140,
                scenario: "A financial institution requires all S3 objects to be encrypted with customer-managed keys, automatic key rotation every 90 days, and detailed audit logs of all key usage. Encryption keys must never leave AWS.",
                question: "Which solution meets ALL requirements?",
                options: [
                    "S3 bucket encryption with KMS CMK, CloudTrail for key usage audit, and Lambda for 90-day rotation",
                    "S3 client-side encryption with AWS Encryption SDK and Parameter Store for key management",
                    "S3 SSE-C with keys stored in Secrets Manager with rotation",
                    "CloudHSM for key generation with S3 SSE-KMS integration"
                ],
                correct: 0,
                explanation: {
                    correct: "KMS CMK with S3 bucket encryption ensures all objects are encrypted, CloudTrail logs all key usage, and Lambda can enforce 90-day rotation.",
                    whyWrong: {
                        1: "Client-side encryption requires key distribution to clients",
                        2: "SSE-C requires passing keys with each request, violating the 'never leave AWS' requirement",
                        3: "CloudHSM is overkill and doesn't provide easier 90-day rotation"
                    },
                    examStrategy: "KMS CMKs provide the best balance of security and manageability for encryption requirements."
                }
            },
            {
                id: 'dva_027',
                domain: "Domain 2: Security",
                difficulty: "medium",
                timeRecommendation: 110,
                scenario: "A Lambda function needs to call an external HTTPS API that requires OAuth 2.0 authentication. The access token expires every hour and must be refreshed automatically.",
                question: "What is the BEST way to manage the OAuth tokens?",
                options: [
                    "Store refresh token in Secrets Manager and implement token refresh logic in Lambda",
                    "Use API Gateway as a proxy with Lambda authorizer for token management",
                    "Store tokens in DynamoDB with TTL and Lambda scheduled refresh",
                    "Implement token caching in ElastiCache with Lambda token refresh"
                ],
                correct: 0,
                explanation: {
                    correct: "Secrets Manager securely stores the refresh token, and Lambda can handle refresh logic when the access token expires.",
                    whyWrong: {
                        1: "API Gateway proxy adds unnecessary complexity for outbound OAuth",
                        2: "DynamoDB storage doesn't add value over Secrets Manager for tokens",
                        3: "ElastiCache for single token is overengineering"
                    },
                    examStrategy: "Secrets Manager for credentials, Parameter Store for configuration. Keep OAuth logic simple."
                }
            },
            {
                id: 'dva_028',
                domain: "Domain 2: Security",
                difficulty: "easy",
                timeRecommendation: 90,
                scenario: "A development team needs to ensure their Docker images stored in ECR are scanned for vulnerabilities before deployment to production ECS clusters.",
                question: "Which approach provides automated security scanning?",
                options: [
                    "Enable ECR image scanning on push and integrate with EventBridge for notifications",
                    "Use Lambda to trigger third-party scanning tools on image upload",
                    "Implement CodeBuild project to scan images during build process",
                    "Configure ECS task definition to validate images at runtime"
                ],
                correct: 0,
                explanation: {
                    correct: "ECR native image scanning on push automatically scans for CVEs with EventBridge integration for automated responses.",
                    whyWrong: {
                        1: "Third-party tools add complexity when ECR has native scanning",
                        2: "CodeBuild scanning doesn't catch vulnerabilities in base images",
                        3: "Runtime validation is too late in the deployment process"
                    },
                    examStrategy: "Use native AWS security features when available. ECR scanning is integrated and comprehensive."
                }
            },
            {
                id: 'dva_029',
                domain: "Domain 2: Security",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "An application stores PII data in DynamoDB. Compliance requires that specific fields (SSN, credit card) are encrypted with unique keys per customer and support key rotation.",
                question: "Which encryption approach meets these requirements?",
                options: [
                    "Use DynamoDB Encryption Client with KMS and separate data keys per customer",
                    "Enable DynamoDB table encryption with KMS CMK",
                    "Implement field-level encryption in application code with Secrets Manager",
                    "Use DynamoDB SSE with customer-managed CMKs per partition"
                ],
                correct: 0,
                explanation: {
                    correct: "DynamoDB Encryption Client enables field-level encryption with per-item data keys, perfect for customer-specific encryption.",
                    whyWrong: {
                        1: "Table encryption doesn't provide field-level or per-customer granularity",
                        2: "Application-level encryption lacks the key management of DynamoDB Encryption Client",
                        3: "SSE doesn't support per-partition CMKs or field-level encryption"
                    },
                    examStrategy: "DynamoDB Encryption Client for field-level encryption, table encryption for at-rest protection."
                }
            },
            {
                id: 'dva_030',
                domain: "Domain 2: Security",
                difficulty: "hard",
                timeRecommendation: 150,
                scenario: "A multi-region application needs to implement end-to-end encryption for data in transit between regions, including VPC peering connections, with the ability to inspect traffic for compliance.",
                question: "Which solution provides encryption AND inspection capabilities?",
                options: [
                    "Transit Gateway with AWS Network Firewall and Site-to-Site VPN for encryption",
                    "VPC Peering with Security Groups and NACLs for traffic control",
                    "PrivateLink endpoints with KMS encryption for data",
                    "Direct Connect with MACsec encryption and VPC Flow Logs"
                ],
                correct: 0,
                explanation: {
                    correct: "Transit Gateway centralizes connectivity, Network Firewall enables inspection, and Site-to-Site VPN provides encryption.",
                    whyWrong: {
                        1: "VPC Peering traffic is encrypted but doesn't support inspection",
                        2: "PrivateLink is for service endpoints, not inter-region VPC connectivity",
                        3: "Direct Connect MACsec only encrypts at Layer 2, not application traffic"
                    },
                    examStrategy: "Transit Gateway is the hub for complex multi-region networking with security requirements."
                }
            },

            // Domain 3: Deployment (continued)
            {
                id: 'dva_031',
                domain: "Domain 3: Deployment",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "A team needs to deploy a Python application that requires specific system libraries and binary dependencies. The application must run on Lambda with consistent behavior across environments.",
                question: "What is the BEST deployment approach?",
                options: [
                    "Create a Lambda container image with all dependencies and deploy from ECR",
                    "Use Lambda Layers for dependencies and ZIP file for application code",
                    "Compile dependencies on Amazon Linux 2 and include in deployment package",
                    "Use SAM CLI with requirements.txt for automatic dependency management"
                ],
                correct: 0,
                explanation: {
                    correct: "Container images allow full control over the runtime environment including system libraries, ensuring consistency.",
                    whyWrong: {
                        1: "Layers have size limits and don't handle system libraries well",
                        2: "Manual compilation is error-prone and hard to maintain",
                        3: "SAM CLI doesn't handle binary dependencies reliably"
                    },
                    examStrategy: "Container images for Lambda when you need system dependencies or consistency with existing containers."
                }
            },
            {
                id: 'dva_032',
                domain: "Domain 3: Deployment",
                difficulty: "hard",
                timeRecommendation: 140,
                scenario: "A company uses multiple AWS accounts for different environments. They need to deploy a CloudFormation stack that creates resources across all accounts simultaneously with rollback capability.",
                question: "Which deployment strategy is MOST appropriate?",
                options: [
                    "CloudFormation StackSets with organizational units targeting",
                    "CodePipeline with cross-account roles and parallel actions",
                    "Systems Manager Automation with multi-account execution",
                    "Lambda function with AssumeRole for each account"
                ],
                correct: 0,
                explanation: {
                    correct: "StackSets are designed for multi-account deployments with automatic rollback and organizational unit targeting.",
                    whyWrong: {
                        1: "CodePipeline parallel actions don't provide unified rollback",
                        2: "Systems Manager Automation lacks StackSets' deployment orchestration",
                        3: "Lambda AssumeRole is manual and lacks rollback coordination"
                    },
                    examStrategy: "StackSets = multi-account CloudFormation. Remember: requires trusted access with Organizations."
                }
            },
            {
                id: 'dva_033',
                domain: "Domain 3: Deployment",
                difficulty: "easy",
                timeRecommendation: 90,
                scenario: "A developer needs to test Lambda function changes locally before deploying to AWS. The function uses environment variables and connects to DynamoDB.",
                question: "Which tool provides the BEST local testing experience?",
                options: [
                    "SAM CLI with sam local start-lambda and DynamoDB Local",
                    "AWS CLI with --dry-run flag for validation",
                    "CloudFormation with change sets for preview",
                    "Lambda console test events with mocked responses"
                ],
                correct: 0,
                explanation: {
                    correct: "SAM CLI provides local Lambda runtime emulation with DynamoDB Local for complete local testing.",
                    whyWrong: {
                        1: "AWS CLI --dry-run only validates syntax, doesn't execute code",
                        2: "Change sets preview infrastructure changes, not function behavior",
                        3: "Console testing requires deployment and doesn't work offline"
                    },
                    examStrategy: "SAM CLI is the standard for local Lambda development. Remember: sam local for testing."
                }
            },
            {
                id: 'dva_034',
                domain: "Domain 3: Deployment",
                difficulty: "medium",
                timeRecommendation: 110,
                scenario: "An application deployed on Elastic Beanstalk needs configuration updates without downtime. The changes include environment variables and nginx configuration.",
                question: "What is the RECOMMENDED approach?",
                options: [
                    "Use .ebextensions configuration files with Rolling deployment policy",
                    "SSH into instances and manually update configuration files",
                    "Create new environment and use Blue/Green swap",
                    "Update configuration through Elastic Beanstalk console with All at Once"
                ],
                correct: 0,
                explanation: {
                    correct: ".ebextensions provides version-controlled configuration with Rolling deployment ensuring zero downtime.",
                    whyWrong: {
                        1: "Manual SSH changes are lost on instance replacement",
                        2: "Blue/Green is overkill for configuration changes",
                        3: "All at Once causes downtime during configuration updates"
                    },
                    examStrategy: ".ebextensions for Elastic Beanstalk configuration. Rolling or Rolling with batch for zero downtime."
                }
            },
            {
                id: 'dva_035',
                domain: "Domain 3: Deployment",
                difficulty: "hard",
                timeRecommendation: 150,
                scenario: "A CI/CD pipeline needs to deploy containerized microservices to EKS with automated testing, security scanning, and progressive rollout based on business metrics.",
                question: "Which tool combination provides the MOST comprehensive solution?",
                options: [
                    "CodePipeline → CodeBuild → ECR scanning → Flux/Argo CD → CloudWatch metrics",
                    "Jenkins → Docker Hub → kubectl rolling update → Prometheus",
                    "GitHub Actions → EKS Fargate → ALB weighted targets",
                    "CircleCI → Helm charts → Istio service mesh"
                ],
                correct: 0,
                explanation: {
                    correct: "CodePipeline orchestrates AWS-native CI/CD, ECR provides security scanning, and GitOps tools like Flux enable progressive delivery with CloudWatch metrics.",
                    whyWrong: {
                        1: "Jenkins setup lacks native AWS integration and security scanning",
                        2: "GitHub Actions with Fargate doesn't address Kubernetes deployment patterns",
                        3: "CircleCI requires external integration and Istio adds complexity"
                    },
                    examStrategy: "Combine AWS services with Kubernetes-native tools for EKS deployments. GitOps is becoming standard."
                }
            },

            // Domain 4: Troubleshooting and Optimization (continued)
            {
                id: 'dva_036',
                domain: "Domain 4: Troubleshooting and Optimization",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "CloudWatch Logs show Lambda function duration averaging 100ms, but X-Ray traces show total request latency of 2 seconds for API Gateway → Lambda → DynamoDB flow.",
                question: "What is the MOST likely cause of the latency gap?",
                options: [
                    "Lambda cold start time not included in function duration metrics",
                    "API Gateway request validation and transformation overhead",
                    "DynamoDB auto-scaling lag during traffic spikes",
                    "Network latency between availability zones"
                ],
                correct: 0,
                explanation: {
                    correct: "Cold starts add significant latency but aren't included in Lambda duration metrics, explaining the gap between function execution and total latency.",
                    whyWrong: {
                        1: "API Gateway overhead is typically milliseconds, not seconds",
                        2: "DynamoDB latency would appear in X-Ray traces",
                        3: "Inter-AZ latency is single-digit milliseconds"
                    },
                    examStrategy: "Lambda duration excludes cold start time. Use X-Ray for complete latency analysis."
                }
            },
            {
                id: 'dva_037',
                domain: "Domain 4: Troubleshooting and Optimization",
                difficulty: "hard",
                timeRecommendation: 140,
                scenario: "An ECS service using Fargate shows healthy tasks in the console, but ALB health checks fail intermittently. Tasks use awsvpc network mode with service discovery.",
                question: "What is the MOST likely root cause?",
                options: [
                    "Security group rules blocking health check traffic from ALB subnets",
                    "Fargate task CPU throttling during health checks",
                    "DNS resolution failures in Service Discovery",
                    "Container memory limits causing OOM kills"
                ],
                correct: 0,
                explanation: {
                    correct: "With awsvpc mode, security groups must explicitly allow health check traffic from ALB, a common misconfiguration.",
                    whyWrong: {
                        1: "CPU throttling would affect performance, not cause intermittent failures",
                        2: "Service Discovery issues would prevent all connections, not just health checks",
                        3: "OOM kills would show unhealthy tasks in console"
                    },
                    examStrategy: "awsvpc mode requires careful security group configuration. ALB needs explicit ingress rules."
                }
            },
            {
                id: 'dva_038',
                domain: "Domain 4: Troubleshooting and Optimization",
                difficulty: "easy",
                timeRecommendation: 90,
                scenario: "A DynamoDB table shows ConsumedReadCapacityUnits consistently at the provisioned limit, but UserErrors metrics show zero throttled requests.",
                question: "What explains this behavior?",
                options: [
                    "DynamoDB auto-scaling is increasing capacity before throttling occurs",
                    "Application is using eventually consistent reads counting as 0.5 units",
                    "Burst capacity is absorbing traffic spikes",
                    "CloudWatch metrics have a delay in showing throttling"
                ],
                correct: 0,
                explanation: {
                    correct: "Auto-scaling proactively increases capacity when utilization is high, preventing throttling while keeping consumption at limits.",
                    whyWrong: {
                        1: "Eventually consistent reads would show lower consumption, not at limit",
                        2: "Burst capacity would show consumption above provisioned capacity",
                        3: "CloudWatch metrics for throttling are near real-time"
                    },
                    examStrategy: "No throttling despite high utilization usually means auto-scaling is working correctly."
                }
            },
            {
                id: 'dva_039',
                domain: "Domain 4: Troubleshooting and Optimization",
                difficulty: "medium",
                timeRecommendation: 110,
                scenario: "Lambda functions in a VPC experience 10-second delays during first invocation, but subsequent invocations complete in 200ms. The function doesn't access VPC resources.",
                question: "What is the BEST optimization?",
                options: [
                    "Remove VPC configuration since the function doesn't need VPC resources",
                    "Enable Provisioned Concurrency to eliminate cold starts",
                    "Increase Lambda memory to speed up ENI attachment",
                    "Use VPC endpoints to reduce network latency"
                ],
                correct: 0,
                explanation: {
                    correct: "Removing unnecessary VPC configuration eliminates ENI attachment overhead, the primary cause of long cold starts.",
                    whyWrong: {
                        1: "Provisioned Concurrency adds cost for an unnecessary VPC configuration",
                        2: "Memory doesn't significantly affect ENI attachment time",
                        3: "VPC endpoints don't help with ENI attachment delays"
                    },
                    examStrategy: "Only use VPC configuration when Lambda needs VPC resources. It adds significant cold start time."
                }
            },
            {
                id: 'dva_040',
                domain: "Domain 4: Troubleshooting and Optimization",
                difficulty: "hard",
                timeRecommendation: 150,
                scenario: "An application using Aurora Serverless v1 experiences connection timeouts during scale-up events. The application uses connection pooling with 100 connections per instance.",
                question: "What is the BEST solution to prevent timeouts during scaling?",
                options: [
                    "Migrate to Aurora Serverless v2 which scales without connection interruption",
                    "Implement RDS Proxy to manage connections during scaling events",
                    "Reduce connection pool size to prevent overwhelming during scale-up",
                    "Enable Aurora Auto Scaling with predictive scaling policies"
                ],
                correct: 0,
                explanation: {
                    correct: "Aurora Serverless v2 scales incrementally without dropping connections, solving the v1 scaling interruption issue.",
                    whyWrong: {
                        1: "RDS Proxy helps but doesn't eliminate v1 scaling interruptions",
                        2: "Smaller connection pools don't solve the scaling interruption",
                        3: "Auto Scaling doesn't apply to Serverless Aurora"
                    },
                    examStrategy: "Aurora Serverless v2 solved v1's connection dropping during scaling. Key improvement to remember."
                }
            },

            // Continue adding more questions to reach 500+
            // Adding high-quality scenario-based questions for each domain

            {
                id: 'dva_041',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "hard",
                timeRecommendation: 150,
                scenario: "A social media platform needs to implement a 'trending topics' feature that identifies popular hashtags from millions of posts per hour. Results must update every minute with the top 100 trending tags globally and per region.",
                question: "Which architecture provides the MOST scalable and cost-effective solution?",
                options: [
                    "Kinesis Data Analytics with tumbling windows and SQL queries for aggregation",
                    "DynamoDB Streams → Lambda → ElasticSearch with aggregation queries",
                    "SQS → EC2 Auto Scaling group with Redis for counting",
                    "EventBridge → Step Functions → Batch for periodic processing"
                ],
                correct: 0,
                explanation: {
                    correct: "Kinesis Data Analytics is purpose-built for real-time streaming analytics with SQL-based tumbling windows perfect for trending calculations.",
                    whyWrong: {
                        1: "DynamoDB Streams to ElasticSearch adds complexity and cost for a streaming analytics problem",
                        2: "SQS with EC2 requires managing infrastructure and custom aggregation logic",
                        3: "EventBridge with Batch is for batch processing, not real-time streaming analytics"
                    },
                    examStrategy: "Kinesis Analytics excels at time-window aggregations. Think SQL queries on streams."
                }
            },
            {
                id: 'dva_042',
                domain: "Domain 2: Security",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "A healthcare application must ensure that all S3 objects containing patient data are encrypted and can only be accessed by specific IAM roles. Accidental public exposure must be prevented at the account level.",
                question: "Which combination of controls provides defense in depth?",
                options: [
                    "S3 Block Public Access, bucket policies with Deny for non-encrypted uploads, and SCPs",
                    "IAM policies with MFA requirements and CloudTrail logging",
                    "VPC endpoints for S3 with private DNS and NACLs",
                    "AWS Config rules with auto-remediation Lambda functions"
                ],
                correct: 0,
                explanation: {
                    correct: "Block Public Access prevents exposure, bucket policies enforce encryption, and SCPs provide organization-level controls.",
                    whyWrong: {
                        1: "MFA and logging provide audit but don't prevent public exposure",
                        2: "VPC endpoints control network access but don't enforce encryption",
                        3: "Config rules are detective controls, not preventive"
                    },
                    examStrategy: "Layer preventive controls: Block Public Access (account) → SCPs (organization) → Bucket policies (resource)."
                }
            },
            {
                id: 'dva_043',
                domain: "Domain 3: Deployment",
                difficulty: "medium",
                timeRecommendation: 110,
                scenario: "A team needs to deploy infrastructure changes that might affect production stability. They want to preview changes, test in isolation, and have automatic rollback capabilities.",
                question: "Which CloudFormation feature set BEST meets these requirements?",
                options: [
                    "Change Sets for preview, Stack Policy for protection, and Rollback Triggers with CloudWatch",
                    "Drift Detection, Stack Imports, and Manual Snapshots",
                    "Nested Stacks with Conditions and WaitConditions",
                    "Transform macros with Custom Resources and DeletionPolicy"
                ],
                correct: 0,
                explanation: {
                    correct: "Change Sets preview changes, Stack Policy prevents unwanted updates, and Rollback Triggers automate rollback on failures.",
                    whyWrong: {
                        1: "Drift Detection identifies changes but doesn't provide rollback",
                        2: "Nested Stacks organize templates but don't provide rollback automation",
                        3: "Transforms and Custom Resources extend functionality but don't address the requirements"
                    },
                    examStrategy: "Change Sets + Stack Policy + Rollback Triggers = safe production deployments."
                }
            },
            {
                id: 'dva_044',
                domain: "Domain 4: Troubleshooting and Optimization",
                difficulty: "hard",
                timeRecommendation: 140,
                scenario: "A Lambda function processing SQS messages shows increasing iterator age in CloudWatch, despite successful message processing. The function timeout is 5 minutes and batch size is 10.",
                question: "What is the MOST likely cause and solution?",
                options: [
                    "Function duration approaching timeout causes partial batch failures; implement batch item failures",
                    "Memory pressure causing garbage collection delays; increase memory allocation",
                    "Network latency to downstream services; implement connection pooling",
                    "SQS visibility timeout too short; increase to match Lambda timeout"
                ],
                correct: 0,
                explanation: {
                    correct: "When functions approach timeout with batch processing, partial failures cause reprocessing. Batch item failures allow reporting specific message failures.",
                    whyWrong: {
                        1: "Memory issues would show in metrics and cause errors, not iterator age",
                        2: "Network latency would increase duration but not specifically iterator age",
                        3: "Visibility timeout issues would cause duplicate processing, not iterator age"
                    },
                    examStrategy: "Iterator age indicates processing delays. With SQS+Lambda, consider batch item failures for partial success."
                }
            },
            {
                id: 'dva_045',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "An IoT application collects temperature readings from 50,000 sensors every 30 seconds. Data must be stored for 1 year, queryable by sensor ID and timestamp, with real-time alerting for anomalies.",
                question: "Which combination provides the BEST solution?",
                options: [
                    "IoT Core → IoT Analytics → S3 for storage, QuickSight for visualization, IoT Events for alerts",
                    "Kinesis Data Streams → Lambda → DynamoDB with TTL → SNS for alerts",
                    "API Gateway → SQS → RDS PostgreSQL → CloudWatch Alarms",
                    "Direct sensor → EC2 fleet → ElasticSearch → Kibana dashboards"
                ],
                correct: 0,
                explanation: {
                    correct: "IoT Core handles device connectivity, IoT Analytics provides time-series analysis, and IoT Events enables rule-based alerting - purpose-built for IoT.",
                    whyWrong: {
                        1: "Kinesis+DynamoDB works but IoT services are purpose-built for this use case",
                        2: "API Gateway+RDS doesn't scale well for high-frequency IoT data",
                        3: "Direct EC2 connections require managing infrastructure and security"
                    },
                    examStrategy: "Use IoT-specific services for IoT use cases. They're optimized for device management and time-series data."
                }
            },
            {
                id: 'dva_046',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "easy",
                timeRecommendation: 90,
                scenario: "A web application needs to display user profile images. Images are stored in S3 and should load quickly globally. URLs should not expose the S3 bucket structure.",
                question: "What is the MOST appropriate solution?",
                options: [
                    "CloudFront distribution with S3 origin and Origin Access Identity",
                    "S3 Static Website Hosting with Route 53 ALIAS records",
                    "API Gateway with Lambda function to retrieve and return images",
                    "S3 Transfer Acceleration with presigned URLs"
                ],
                correct: 0,
                explanation: {
                    correct: "CloudFront provides global caching and OAI ensures S3 remains private while CloudFront serves content with custom domains.",
                    whyWrong: {
                        1: "Static hosting requires public S3 access, exposing bucket structure",
                        2: "API Gateway+Lambda adds unnecessary latency and cost for static content",
                        3: "Transfer Acceleration is for uploads, presigned URLs still expose bucket structure"
                    },
                    examStrategy: "CloudFront + OAI is the standard pattern for serving S3 content globally and privately."
                }
            },
            {
                id: 'dva_047',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "hard",
                timeRecommendation: 150,
                scenario: "A machine learning pipeline processes 10TB of daily data through multiple stages: ingestion, cleaning, feature extraction, model training, and inference. Each stage has different compute requirements.",
                question: "Which orchestration approach provides the BEST flexibility and cost optimization?",
                options: [
                    "Step Functions with AWS Batch for compute-intensive stages and Lambda for lightweight tasks",
                    "Apache Airflow on EC2 with EMR clusters for all processing",
                    "SageMaker Pipelines with Processing Jobs and Training Jobs",
                    "Glue Workflows with Glue ETL jobs and SageMaker integration"
                ],
                correct: 0,
                explanation: {
                    correct: "Step Functions orchestrates different compute types optimally - Batch for heavy processing, Lambda for coordination, providing maximum flexibility.",
                    whyWrong: {
                        1: "Airflow+EMR requires infrastructure management and isn't cost-optimized for varying workloads",
                        2: "SageMaker Pipelines is ML-specific and less flexible for general data processing",
                        3: "Glue Workflows are ETL-focused and less suitable for ML training orchestration"
                    },
                    examStrategy: "Step Functions excels at orchestrating heterogeneous compute. Mix services for cost optimization."
                }
            },
            {
                id: 'dva_048',
                domain: "Domain 2: Security",
                difficulty: "medium",
                timeRecommendation: 110,
                scenario: "A mobile banking app needs to implement biometric authentication. Biometric data must never leave the device, but the backend must verify user identity for transactions.",
                question: "Which authentication flow is MOST secure?",
                options: [
                    "Device stores biometric hash, authenticates locally, sends signed JWT to Cognito for verification",
                    "Send biometric data to Lambda for comparison with stored templates in DynamoDB",
                    "Use Rekognition for facial comparison with stored user photos",
                    "Store encrypted biometrics in S3 and compare on device after download"
                ],
                correct: 0,
                explanation: {
                    correct: "Local biometric verification with JWT signing ensures biometric data never leaves device while maintaining secure authentication.",
                    whyWrong: {
                        1: "Sending biometric data to Lambda violates the security requirement",
                        2: "Rekognition requires sending facial data to AWS",
                        3: "Downloading biometrics to device creates security vulnerabilities"
                    },
                    examStrategy: "Biometric data should stay on-device. Use cryptographic proofs for verification."
                }
            },
            {
                id: 'dva_049',
                domain: "Domain 2: Security",
                difficulty: "easy",
                timeRecommendation: 90,
                scenario: "Developers need read-only access to production CloudWatch Logs for debugging, but must not see sensitive data like credit card numbers in log messages.",
                question: "What is the BEST way to provide this access?",
                options: [
                    "CloudWatch Logs data protection policies to mask sensitive data patterns",
                    "Lambda function to filter logs before developers access them",
                    "Separate log groups for sensitive and non-sensitive data",
                    "IAM policies with Deny statements for specific log streams"
                ],
                correct: 0,
                explanation: {
                    correct: "Data protection policies automatically mask sensitive patterns in CloudWatch Logs, providing transparent protection.",
                    whyWrong: {
                        1: "Lambda filtering adds complexity and latency",
                        2: "Separate log groups require application changes and may miss sensitive data",
                        3: "IAM policies control access, not content masking"
                    },
                    examStrategy: "CloudWatch Logs data protection policies are the newest feature for PII masking."
                }
            },
            {
                id: 'dva_050',
                domain: "Domain 3: Deployment",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "A company wants to standardize Lambda function deployments across 50+ development teams. Each team should use approved runtimes, layers, and security configurations.",
                question: "Which approach BEST enforces standards while maintaining team autonomy?",
                options: [
                    "AWS Service Catalog with SAM/CDK products and launch constraints",
                    "Shared CloudFormation templates in S3 with documentation",
                    "Lambda function policy requiring specific tags and configurations",
                    "CodeCommit repository with approved function templates"
                ],
                correct: 0,
                explanation: {
                    correct: "Service Catalog enforces standards through products while launch constraints ensure compliance, giving teams self-service deployment.",
                    whyWrong: {
                        1: "Shared templates rely on teams following standards voluntarily",
                        2: "Function policies don't enforce runtime or layer standards",
                        3: "Code repositories don't enforce deployment standards"
                    },
                    examStrategy: "Service Catalog is the tool for standardized self-service provisioning in enterprises."
                }
            },

            // Continue with more questions to reach 500+ total
            // Adding diverse scenarios across all domains

            {
                id: 'dva_051',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "A gaming company needs to implement a global matchmaking system that groups players based on skill level and geographic location. Matches must start within 30 seconds of request.",
                question: "Which architecture provides the BEST matchmaking performance?",
                options: [
                    "GameLift FlexMatch with SQS for match notifications and Lambda for game server allocation",
                    "DynamoDB Global Tables with Lambda for matching logic and SNS for notifications",
                    "ElastiCache Redis with Lua scripts for atomic matching operations",
                    "Neptune graph database for player relationships and matching algorithms"
                ],
                correct: 0,
                explanation: {
                    correct: "GameLift FlexMatch is purpose-built for game matchmaking with built-in rules engine and automatic server allocation.",
                    whyWrong: {
                        1: "DynamoDB Global Tables provide data distribution but lack matchmaking logic",
                        2: "Redis with Lua works but requires building matchmaking from scratch",
                        3: "Neptune is for graph relationships, not optimized for real-time matchmaking"
                    },
                    examStrategy: "Use purpose-built services when available. GameLift for games, not generic solutions."
                }
            },
            {
                id: 'dva_052',
                domain: "Domain 4: Troubleshooting and Optimization",
                difficulty: "hard",
                timeRecommendation: 140,
                scenario: "An application shows P99 latency spikes every day at 3 AM UTC. CloudWatch shows no correlation with traffic, CPU, or memory. The application uses DynamoDB with on-demand pricing.",
                question: "What is the MOST likely cause?",
                options: [
                    "DynamoDB point-in-time recovery backup process causing brief latency",
                    "Lambda container image garbage collection during low traffic",
                    "CloudWatch Logs agent batch upload causing resource contention",
                    "AWS internal maintenance window affecting availability zone"
                ],
                correct: 0,
                explanation: {
                    correct: "PITR continuous backups can cause brief latency spikes during snapshot consolidation, often scheduled during low-traffic periods.",
                    whyWrong: {
                        1: "Lambda container GC happens based on memory pressure, not time",
                        2: "CloudWatch Logs agent doesn't cause application latency",
                        3: "AWS maintenance is staggered and wouldn't happen daily at same time"
                    },
                    examStrategy: "Regular timing patterns often indicate scheduled AWS operations. Check backup and maintenance settings."
                }
            },
            {
                id: 'dva_053',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "easy",
                timeRecommendation: 90,
                scenario: "A blog platform needs to send email notifications to subscribers when new posts are published. Emails should be personalized with subscriber names and preferences.",
                question: "Which service combination is MOST cost-effective?",
                options: [
                    "SES with personalization templates and SNS for triggering sends",
                    "SES with Lambda for personalization and EventBridge for scheduling",
                    "WorkMail with distribution lists and Lambda for content generation",
                    "SNS with email protocol and Lambda for formatting"
                ],
                correct: 0,
                explanation: {
                    correct: "SES templates support personalization natively, and SNS can trigger bulk sends efficiently.",
                    whyWrong: {
                        1: "Lambda for personalization adds unnecessary compute cost",
                        2: "WorkMail is for business email, not bulk notifications",
                        3: "SNS email protocol doesn't support HTML or personalization"
                    },
                    examStrategy: "SES for bulk/marketing emails with formatting. SNS for simple notifications."
                }
            },
            {
                id: 'dva_054',
                domain: "Domain 2: Security",
                difficulty: "hard",
                timeRecommendation: 150,
                scenario: "A fintech application requires all database queries to be logged with the executing user's identity, query text, and results for compliance. The audit logs must be tamper-proof and retained for 7 years.",
                question: "Which solution provides the MOST comprehensive audit trail?",
                options: [
                    "RDS Aurora with Database Activity Streams to Kinesis, processed to S3 with Object Lock",
                    "CloudTrail with RDS Data API events and CloudWatch Logs retention",
                    "RDS Proxy with logging enabled and AWS Backup for retention",
                    "Custom Lambda triggers on database tables writing to DynamoDB"
                ],
                correct: 0,
                explanation: {
                    correct: "Database Activity Streams capture all database activity in real-time, and S3 Object Lock ensures tamper-proof storage.",
                    whyWrong: {
                        1: "CloudTrail only logs API calls, not query content or results",
                        2: "RDS Proxy logs connections, not query details",
                        3: "Lambda triggers miss direct database access and aren't tamper-proof"
                    },
                    examStrategy: "Database Activity Streams for comprehensive database auditing. Object Lock for compliance retention."
                }
            },
            {
                id: 'dva_055',
                domain: "Domain 3: Deployment",
                difficulty: "medium",
                timeRecommendation: 110,
                scenario: "A team needs to deploy a three-tier web application (frontend, API, database) where each tier may be updated independently. Rollback must be possible for any tier.",
                question: "Which deployment strategy provides the BEST independence?",
                options: [
                    "Separate CloudFormation stacks with exports/imports for cross-stack references",
                    "Single CloudFormation stack with nested stacks for each tier",
                    "Terraform modules with separate state files per tier",
                    "Elastic Beanstalk with multiple environments linked by configuration"
                ],
                correct: 0,
                explanation: {
                    correct: "Separate stacks allow independent updates and rollbacks while exports/imports maintain references.",
                    whyWrong: {
                        1: "Nested stacks still require parent stack updates affecting all tiers",
                        2: "Terraform works but CloudFormation provides better AWS integration",
                        3: "Elastic Beanstalk is primarily for application deployment, not infrastructure"
                    },
                    examStrategy: "Separate stacks for independent lifecycle management. Exports/imports for dependencies."
                }
            },
            {
                id: 'dva_056',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "hard",
                timeRecommendation: 150,
                scenario: "A news aggregator needs to fetch content from 1000+ RSS feeds every 5 minutes, extract articles, perform sentiment analysis, and update a searchable index. The system must handle feed failures gracefully.",
                question: "Which architecture provides the BEST scalability and fault tolerance?",
                options: [
                    "EventBridge Scheduler → SQS → Lambda (fetch) → Comprehend → OpenSearch",
                    "EC2 with cron jobs → RabbitMQ → Container tasks → ElasticSearch",
                    "Step Functions with Map state → Lambda → DynamoDB → CloudSearch",
                    "Fargate scheduled tasks → Kinesis → Lambda → DocumentDB"
                ],
                correct: 0,
                explanation: {
                    correct: "EventBridge Scheduler handles scheduling at scale, SQS provides fault tolerance, and OpenSearch offers powerful search capabilities.",
                    whyWrong: {
                        1: "EC2 cron jobs don't scale well for 1000+ feeds",
                        2: "Step Functions Map state has concurrency limits for 1000+ iterations",
                        3: "Fargate scheduled tasks are harder to manage at this scale"
                    },
                    examStrategy: "EventBridge Scheduler for large-scale scheduling. SQS for decoupling and fault tolerance."
                }
            },
            {
                id: 'dva_057',
                domain: "Domain 2: Security",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "An application needs to call multiple third-party APIs, each requiring different authentication methods (OAuth, API Key, Basic Auth). Credentials must be centrally managed and rotated.",
                question: "What is the BEST way to manage these diverse credentials?",
                options: [
                    "Secrets Manager with different secret types and rotation Lambda functions per API",
                    "Systems Manager Parameter Store with SecureString parameters",
                    "Cognito Identity Pools with custom authentication providers",
                    "IAM roles with external ID for each third-party service"
                ],
                correct: 0,
                explanation: {
                    correct: "Secrets Manager supports different secret types and custom rotation Lambda functions for any authentication method.",
                    whyWrong: {
                        1: "Parameter Store doesn't support automatic rotation",
                        2: "Cognito Identity Pools are for user authentication, not API credentials",
                        3: "IAM roles don't work for third-party API authentication"
                    },
                    examStrategy: "Secrets Manager for any rotating credential. Parameter Store for static configuration."
                }
            },
            {
                id: 'dva_058',
                domain: "Domain 4: Troubleshooting and Optimization",
                difficulty: "medium",
                timeRecommendation: 110,
                scenario: "A Lambda function consuming from Kinesis shows increasing iterator age despite processing records successfully. The function reserves 100 concurrent executions.",
                question: "What is the MOST likely issue?",
                options: [
                    "Number of Kinesis shards exceeds Lambda reserved concurrency causing throttling",
                    "Kinesis record size exceeding Lambda payload limit",
                    "Lambda function memory too low causing slow processing",
                    "Kinesis stream retention period set too high"
                ],
                correct: 0,
                explanation: {
                    correct: "Each Kinesis shard requires one Lambda concurrent execution. With more shards than reserved concurrency, some shards can't be processed.",
                    whyWrong: {
                        1: "Oversized records would cause errors, not successful processing with lag",
                        2: "Low memory would increase duration, but reserved concurrency is the bottleneck",
                        3: "Retention period doesn't affect iterator age"
                    },
                    examStrategy: "Lambda-Kinesis integration: one concurrent execution per shard. Plan concurrency accordingly."
                }
            },
            {
                id: 'dva_059',
                domain: "Domain 3: Deployment",
                difficulty: "easy",
                timeRecommendation: 90,
                scenario: "A development team needs to quickly create multiple identical development environments for testing. Each environment should be isolated and easy to terminate.",
                question: "What is the MOST efficient approach?",
                options: [
                    "CloudFormation template with parameters for environment names",
                    "Manually create resources and document the process",
                    "Use production environment with different IAM policies",
                    "EC2 AMIs with all software pre-installed"
                ],
                correct: 0,
                explanation: {
                    correct: "CloudFormation templates ensure consistency and can be parameterized for multiple identical environments with easy cleanup.",
                    whyWrong: {
                        1: "Manual creation is error-prone and time-consuming",
                        2: "Sharing production environment is dangerous and not isolated",
                        3: "AMIs only handle EC2, not entire environment infrastructure"
                    },
                    examStrategy: "Infrastructure as Code for repeatable environments. CloudFormation or CDK for consistency."
                }
            },
            {
                id: 'dva_060',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "A mobile app needs offline-first functionality with data synchronization when online. Users should see their own data immediately and shared data when synchronized.",
                question: "Which service provides the BEST offline-first experience?",
                options: [
                    "AWS AppSync with Amplify DataStore for offline persistence and sync",
                    "API Gateway with Lambda and client-side SQLite database",
                    "DynamoDB with Global Tables and client-side caching",
                    "S3 with Transfer Acceleration and local file storage"
                ],
                correct: 0,
                explanation: {
                    correct: "AppSync with DataStore provides automatic offline persistence, conflict resolution, and seamless synchronization.",
                    whyWrong: {
                        1: "API Gateway requires custom offline logic and sync mechanisms",
                        2: "DynamoDB Global Tables are for multi-region, not offline-first",
                        3: "S3 is for file storage, not structured data synchronization"
                    },
                    examStrategy: "AppSync + Amplify DataStore is the AWS solution for offline-first mobile apps."
                }
            },

            // Continue adding more comprehensive questions...
            // Domain 1: Advanced Development Patterns

            {
                id: 'dva_061',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "hard",
                timeRecommendation: 150,
                scenario: "A financial trading platform requires sub-millisecond latency for order matching. The system must handle 1 million orders per second during peak hours with strict ordering guarantees per trading symbol.",
                question: "Which architecture provides the REQUIRED performance?",
                options: [
                    "EC2 with placement groups, ElastiCache Redis with Lua scripts, and SR-IOV networking",
                    "Lambda with Provisioned Concurrency and DynamoDB with strong consistency",
                    "ECS on Fargate with Application Load Balancer and Aurora Serverless",
                    "API Gateway with caching and Kinesis Data Streams"
                ],
                correct: 0,
                explanation: {
                    correct: "Placement groups minimize network latency, Redis Lua scripts provide atomic operations, and SR-IOV offers highest network performance.",
                    whyWrong: {
                        1: "Lambda has inherent overhead incompatible with sub-millisecond requirements",
                        2: "Fargate and ALB add latency layers unsuitable for ultra-low latency",
                        3: "API Gateway has minimum latency overhead exceeding requirements"
                    },
                    examStrategy: "Sub-millisecond latency requires EC2 with placement groups and memory-based data stores."
                }
            },
            {
                id: 'dva_062',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "medium",
                timeRecommendation: 110,
                scenario: "An e-learning platform needs to transcode uploaded video courses into multiple formats and resolutions. Processing must be cost-effective and scale based on upload volume.",
                question: "Which solution provides the BEST cost-performance balance?",
                options: [
                    "S3 upload → Lambda trigger → MediaConvert jobs with queue priorities",
                    "S3 upload → EC2 Spot Fleet with FFmpeg → S3 output",
                    "Kinesis Video Streams → Lambda → Elastic Transcoder",
                    "ECS Fargate tasks with custom encoding containers"
                ],
                correct: 0,
                explanation: {
                    correct: "MediaConvert is serverless, scales automatically, and provides professional-grade encoding with pay-per-job pricing.",
                    whyWrong: {
                        1: "Spot Fleet requires management and may be interrupted during processing",
                        2: "Kinesis Video Streams is for live streaming, not file transcoding",
                        3: "Fargate tasks require custom encoding logic and container management"
                    },
                    examStrategy: "MediaConvert for file transcoding, Kinesis Video for streaming, Elastic Transcoder is legacy."
                }
            },
            {
                id: 'dva_063',
                domain: "Domain 2: Security",
                difficulty: "hard",
                timeRecommendation: 140,
                scenario: "A healthcare SaaS platform must implement field-level encryption for PII across multiple databases (RDS, DynamoDB, DocumentDB) with customer-managed keys per tenant.",
                question: "Which approach provides the MOST comprehensive encryption solution?",
                options: [
                    "AWS Encryption SDK with KMS multi-tenant key hierarchy and caching",
                    "Native database encryption with separate KMS keys per database",
                    "Client-side encryption libraries with keys in Secrets Manager",
                    "CloudHSM with custom key management application"
                ],
                correct: 0,
                explanation: {
                    correct: "Encryption SDK provides consistent field-level encryption across services with efficient key caching and multi-tenant support.",
                    whyWrong: {
                        1: "Native database encryption doesn't provide field-level granularity",
                        2: "Client-side libraries lack the key management sophistication of Encryption SDK",
                        3: "CloudHSM adds complexity without field-level encryption benefits"
                    },
                    examStrategy: "AWS Encryption SDK for application-layer encryption across multiple services."
                }
            },
            {
                id: 'dva_064',
                domain: "Domain 3: Deployment",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "A microservices application with 50+ services needs coordinated deployments where some services must deploy together while others can deploy independently.",
                question: "Which orchestration approach provides the REQUIRED flexibility?",
                options: [
                    "CodePipeline with parallel and sequential stages using stage transitions",
                    "Jenkins with Pipeline as Code and conditional stages",
                    "Step Functions with Choice states and parallel branches",
                    "GitLab CI/CD with dependency graphs"
                ],
                correct: 0,
                explanation: {
                    correct: "CodePipeline's stage transitions and approval actions enable complex orchestration with native AWS service integration.",
                    whyWrong: {
                        1: "Jenkins requires additional AWS integration setup",
                        2: "Step Functions is for workflow orchestration, not CI/CD pipelines",
                        3: "GitLab CI/CD requires external AWS integration"
                    },
                    examStrategy: "CodePipeline for AWS-native CI/CD orchestration with complex dependencies."
                }
            },
            {
                id: 'dva_065',
                domain: "Domain 4: Troubleshooting and Optimization",
                difficulty: "hard",
                timeRecommendation: 150,
                scenario: "An application experiences cascading failures when one microservice becomes slow. Response times increase across all services, eventually causing timeouts and service unavailability.",
                question: "Which pattern BEST prevents cascade failures?",
                options: [
                    "Circuit breaker pattern with exponential backoff and bulkhead isolation",
                    "Increase timeout values and add more service instances",
                    "Implement service mesh with automatic retries",
                    "Add caching layers between all service calls"
                ],
                correct: 0,
                explanation: {
                    correct: "Circuit breakers prevent cascading failures by failing fast, while bulkheads isolate failures to prevent spread.",
                    whyWrong: {
                        1: "Increasing timeouts worsens cascading failures by holding resources longer",
                        2: "Automatic retries can amplify problems during failures",
                        3: "Caching helps but doesn't address the root cause of cascade failures"
                    },
                    examStrategy: "Circuit breaker pattern is key for preventing cascading failures in microservices."
                }
            },
            {
                id: 'dva_066',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "A real estate platform needs to search properties by location (within radius), price range, amenities, and availability dates. The search must support complex filters and sorting.",
                question: "Which data store provides the BEST search capabilities?",
                options: [
                    "OpenSearch with geospatial queries and custom analyzers",
                    "DynamoDB with composite keys and filter expressions",
                    "RDS PostgreSQL with PostGIS extension",
                    "Neptune with property graph model"
                ],
                correct: 0,
                explanation: {
                    correct: "OpenSearch excels at complex searches with geospatial support, faceted search, and custom scoring.",
                    whyWrong: {
                        1: "DynamoDB filter expressions are limited and inefficient for complex queries",
                        2: "PostgreSQL PostGIS works but lacks the search features of OpenSearch",
                        3: "Neptune is for graph relationships, not property searches"
                    },
                    examStrategy: "OpenSearch (formerly Elasticsearch) for complex search requirements with geospatial needs."
                }
            },
            {
                id: 'dva_067',
                domain: "Domain 2: Security",
                difficulty: "medium",
                timeRecommendation: 110,
                scenario: "A mobile payment app needs to comply with PCI DSS. Credit card data must be tokenized before storage, with tokens usable for recurring payments.",
                question: "Which approach BEST meets PCI compliance requirements?",
                options: [
                    "Payment tokenization service with tokens in DynamoDB, never storing actual card data",
                    "KMS encryption for credit card data with restricted IAM access",
                    "Lambda function to hash credit cards with salt stored in Secrets Manager",
                    "API Gateway with request validation to reject credit card numbers"
                ],
                correct: 0,
                explanation: {
                    correct: "Tokenization replaces sensitive data with non-sensitive tokens, meeting PCI DSS requirements for data protection.",
                    whyWrong: {
                        1: "Encryption alone doesn't meet PCI tokenization requirements",
                        2: "Hashing doesn't allow for recurring payments and may not meet PCI standards",
                        3: "Request validation doesn't address storage requirements"
                    },
                    examStrategy: "Tokenization > Encryption for PCI compliance. Never store actual card data."
                }
            },
            {
                id: 'dva_068',
                domain: "Domain 3: Deployment",
                difficulty: "easy",
                timeRecommendation: 90,
                scenario: "A Lambda function deployment package exceeds the 50MB limit when zipped. The function requires large ML model files.",
                question: "What is the BEST solution?",
                options: [
                    "Package the function as a container image up to 10GB",
                    "Split the model into multiple Lambda Layers",
                    "Store model in S3 and download at runtime",
                    "Use EFS to mount model files"
                ],
                correct: 0,
                explanation: {
                    correct: "Container images support up to 10GB, perfect for large ML models with all dependencies included.",
                    whyWrong: {
                        1: "Layers have combined 250MB extracted size limit",
                        2: "S3 download adds cold start latency",
                        3: "EFS adds complexity and latency for model loading"
                    },
                    examStrategy: "Lambda container images for large deployments (up to 10GB). Layers for shared code under 250MB."
                }
            },
            {
                id: 'dva_069',
                domain: "Domain 4: Troubleshooting and Optimization",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "CloudWatch shows Lambda duration of 3 seconds, but X-Ray traces show 8-second end-to-end latency for the same request. The Lambda function makes multiple AWS service calls.",
                question: "What is the MOST likely cause of the latency difference?",
                options: [
                    "Lambda cold start and runtime initialization not included in duration metric",
                    "X-Ray tracing overhead adding significant latency",
                    "Network latency between AWS services",
                    "CloudWatch metrics aggregation delay"
                ],
                correct: 0,
                explanation: {
                    correct: "Lambda duration metrics exclude cold start and initialization time, which X-Ray captures in total latency.",
                    whyWrong: {
                        1: "X-Ray overhead is minimal, typically microseconds",
                        2: "Inter-service latency would be included in Lambda duration",
                        3: "Metrics show actual duration, not affected by aggregation"
                    },
                    examStrategy: "Lambda duration != total latency. Cold starts and init time are excluded from duration metrics."
                }
            },
            {
                id: 'dva_070',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "hard",
                timeRecommendation: 140,
                scenario: "A collaborative document editing application needs real-time synchronization across users with conflict resolution, offline support, and revision history.",
                question: "Which architecture provides the BEST real-time collaboration features?",
                options: [
                    "AppSync with subscriptions, DynamoDB versioning, and Amplify DataStore",
                    "API Gateway WebSockets with Lambda and ElasticSearch",
                    "IoT Core with MQTT topics per document and S3 versioning",
                    "Kinesis Data Streams with Lambda processors and RDS"
                ],
                correct: 0,
                explanation: {
                    correct: "AppSync subscriptions enable real-time updates, DynamoDB handles versioning, and DataStore provides offline sync with conflict resolution.",
                    whyWrong: {
                        1: "WebSockets require custom conflict resolution and offline logic",
                        2: "IoT Core is not designed for document collaboration",
                        3: "Kinesis is for streaming, not bidirectional collaboration"
                    },
                    examStrategy: "AppSync excels at real-time collaborative applications with built-in conflict resolution."
                }
            },

            // Adding final set of questions to reach 500+
            {
                id: 'dva_071',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "medium",
                timeRecommendation: 110,
                scenario: "A music streaming service needs to recommend songs based on listening history, with recommendations updating in real-time as users listen to new tracks.",
                question: "Which combination provides real-time personalized recommendations?",
                options: [
                    "Kinesis Data Streams → Kinesis Analytics → Lambda → Personalize → DynamoDB",
                    "SQS → Batch → SageMaker training → RDS PostgreSQL",
                    "EventBridge → Step Functions → Comprehend → ElasticSearch",
                    "API Gateway → Lambda → Rekognition → CloudSearch"
                ],
                correct: 0,
                explanation: {
                    correct: "Kinesis handles streaming data, Analytics processes in real-time, and Personalize provides ML-based recommendations.",
                    whyWrong: {
                        1: "Batch processing doesn't provide real-time updates",
                        2: "Comprehend is for text analysis, not recommendations",
                        3: "Rekognition is for image/video analysis, not music recommendations"
                    },
                    examStrategy: "Amazon Personalize is the managed service for recommendation engines."
                }
            },
            {
                id: 'dva_072',
                domain: "Domain 2: Security",
                difficulty: "easy",
                timeRecommendation: 90,
                scenario: "A Lambda function needs to access an RDS database in a private subnet. The function should have minimal network configuration overhead.",
                question: "What is the SIMPLEST secure solution?",
                options: [
                    "RDS Proxy with IAM authentication, Lambda without VPC configuration",
                    "Lambda in VPC with security groups and RDS password in Secrets Manager",
                    "RDS Data API with IAM authentication",
                    "VPC peering between Lambda VPC and RDS VPC"
                ],
                correct: 0,
                explanation: {
                    correct: "RDS Proxy allows Lambda to connect without VPC configuration while providing IAM authentication and connection pooling.",
                    whyWrong: {
                        1: "VPC configuration adds cold start latency",
                        2: "Data API only works with Aurora Serverless",
                        3: "VPC peering is unnecessary complexity"
                    },
                    examStrategy: "RDS Proxy eliminates VPC configuration needs for Lambda-RDS connections."
                }
            },
            {
                id: 'dva_073',
                domain: "Domain 3: Deployment",
                difficulty: "hard",
                timeRecommendation: 150,
                scenario: "A company needs to deploy applications across 100+ AWS accounts with centralized governance, automated compliance checks, and the ability to enforce specific configurations.",
                question: "Which solution provides the MOST comprehensive governance?",
                options: [
                    "Control Tower with Service Catalog, Config Rules, and CloudFormation StackSets",
                    "Organizations with SCPs and manual CloudFormation deployment",
                    "Landing Zone with CodePipeline and cross-account roles",
                    "Systems Manager with Automation documents and Parameter Store"
                ],
                correct: 0,
                explanation: {
                    correct: "Control Tower provides account vending, Service Catalog enforces standards, Config Rules ensure compliance, and StackSets enable multi-account deployment.",
                    whyWrong: {
                        1: "Manual deployment doesn't scale to 100+ accounts",
                        2: "Landing Zone is superseded by Control Tower",
                        3: "Systems Manager alone lacks account governance features"
                    },
                    examStrategy: "Control Tower is AWS's solution for multi-account governance and deployment."
                }
            },
            {
                id: 'dva_074',
                domain: "Domain 4: Troubleshooting and Optimization",
                difficulty: "medium",
                timeRecommendation: 120,
                scenario: "An API shows increasing response times during business hours. CloudWatch shows normal CPU and memory, but RDS shows connection count at maximum.",
                question: "What is the BEST immediate solution?",
                options: [
                    "Implement RDS Proxy for connection pooling and multiplexing",
                    "Increase RDS instance size for more connections",
                    "Add read replicas to distribute connection load",
                    "Implement caching to reduce database calls"
                ],
                correct: 0,
                explanation: {
                    correct: "RDS Proxy immediately solves connection pooling issues without application changes or downtime.",
                    whyWrong: {
                        1: "Larger instance requires downtime and may not solve pooling issues",
                        2: "Read replicas don't help with connection pool exhaustion",
                        3: "Caching helps but doesn't address immediate connection limit"
                    },
                    examStrategy: "RDS Proxy is the solution for connection pool exhaustion and management."
                }
            },
            {
                id: 'dva_075',
                domain: "Domain 1: Development with AWS Services",
                difficulty: "medium",
                timeRecommendation: 110,
                scenario: "A serverless application needs to execute workflows where each step may take 5-30 minutes, with complex branching logic based on results.",
                question: "Which service is MOST appropriate for this use case?",
                options: [
                    "Step Functions Standard Workflows with AWS Batch for long-running tasks",
                    "Step Functions Express Workflows with Lambda functions",
                    "SWF with EC2 workers for task execution",
                    "EventBridge with Lambda for workflow coordination"
                ],
                correct: 0,
                explanation: {
                    correct: "Standard Workflows support year-long executions and Batch handles long-running compute tasks efficiently.",
                    whyWrong: {
                        1: "Express Workflows have 5-minute maximum duration",
                        2: "SWF is legacy, replaced by Step Functions",
                        3: "EventBridge doesn't provide workflow state management"
                    },
                    examStrategy: "Step Functions Standard for long-running workflows, Express for high-volume short workflows."
                }
            },
            {
    id: 'dva_076',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "easy",
    timeRecommendation: 90,
    scenario: "A web application needs to store user session data that expires after 30 minutes of inactivity. The solution must be highly available and fast.",
    question: "Which storage solution is MOST appropriate?",
    options: [
        "ElastiCache Redis with TTL settings",
        "DynamoDB with TTL attribute",
        "S3 with lifecycle policies",
        "EFS with cron job cleanup"
    ],
    correct: 0,
    explanation: {
        correct: "ElastiCache Redis provides sub-millisecond latency for session retrieval and automatic expiration with TTL (Time To Live) settings. Perfect for session storage requiring fast access and precise timing.",
        whyWrong: {
            1: "DynamoDB TTL has up to 48-hour delay for deletion and adds unnecessary complexity for simple session storage",
            2: "S3 lifecycle policies work on daily granularity, not 30-minute precision, and S3 is not optimized for frequent session updates",
            3: "EFS is designed for file storage, not key-value session data, and cron job cleanup is unreliable and imprecise"
        },
        examStrategy: "For session storage: ElastiCache Redis with TTL for speed + automatic expiration. DynamoDB TTL is eventually consistent with delays."
    }
},

                    // BATCH 1: 50 Domain 1 Questions - Development with AWS Services (32%)
// Add these to your existing questionBank array

{
    id: 'dva_077',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A serverless e-commerce application uses API Gateway, Lambda, and DynamoDB. During Black Friday, the application experiences 500 errors when order volume peaks at 10,000 concurrent requests. CloudWatch shows Lambda concurrent executions hitting 1000 limit and DynamoDB throttling errors.",
    question: "What combination of solutions will BEST resolve both performance bottlenecks?",
    options: [
        "Increase Lambda reserved concurrency to 5000 and enable DynamoDB auto-scaling",
        "Implement SQS between API Gateway and Lambda, enable DynamoDB on-demand billing",
        "Use Lambda provisioned concurrency and DynamoDB Global Tables",
        "Add ElastiCache between Lambda and DynamoDB, implement exponential backoff"
    ],
    correct: 1,
    explanation: {
        correct: "SQS decouples the traffic spikes, allowing Lambda to process at its natural rate. DynamoDB on-demand automatically scales to handle traffic without pre-provisioning.",
        whyWrong: {
            0: "Reserved concurrency doesn't solve the fundamental traffic spike issue",
            2: "Provisioned concurrency is expensive for unpredictable traffic patterns",
            3: "ElastiCache doesn't solve write throttling in DynamoDB"
        },
        examStrategy: "For traffic spikes: Decouple with SQS. For DynamoDB: On-demand for unpredictable workloads."
    }
},
{
    id: 'dva_078',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function processes S3 events and updates DynamoDB. The function occasionally fails with 'Unable to import module' errors, but only for certain file types. The deployment package is 45MB and includes multiple libraries.",
    question: "What is the MOST likely cause and solution?",
    options: [
        "Lambda timeout - increase timeout from 3 seconds to 15 seconds",
        "Cold start issue - implement provisioned concurrency",
        "Deployment package too large - use Lambda Layers for shared libraries",
        "Memory allocation - increase from 128MB to 512MB"
    ],
    correct: 2,
    explanation: {
        correct: "45MB is close to Lambda's 50MB limit. Layers allow sharing libraries across functions and reduce package size.",
        whyWrong: {
            0: "Import errors are not timeout related",
            1: "Cold starts don't cause import module errors",
            3: "Memory doesn't affect module imports"
        },
        examStrategy: "Import errors + large package = Use Lambda Layers. 50MB limit for direct upload."
    }
},
{
    id: 'dva_079',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "A microservices application uses API Gateway with Lambda authorizers for authentication. Users report intermittent 'Access Denied' errors during peak hours, but the same requests work when retried. CloudWatch shows 200ms average authorizer response time with some spikes to 30 seconds.",
    question: "What is the BEST solution to improve authorizer reliability?",
    options: [
        "Increase Lambda authorizer memory and implement result caching",
        "Replace Lambda authorizer with Cognito User Pools",
        "Implement API Gateway caching with TTL of 300 seconds",
        "Use request-based authorizer instead of token-based"
    ],
    correct: 0,
    explanation: {
        correct: "Memory improves performance and reduces cold starts. Result caching prevents repeated authorizer calls for the same token.",
        whyWrong: {
            1: "Cognito might not fit complex authorization logic",
            2: "API Gateway caching doesn't cache authorizer results",
            3: "Request vs token type doesn't solve performance issues"
        },
        examStrategy: "Authorizer performance issues: Increase memory + enable result caching (5 min TTL)."
    }
},
{
    id: 'dva_080',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function reads messages from SQS, processes images from S3, and writes results to DynamoDB. The function processes 100 messages per invocation but occasionally fails with DynamoDB throttling errors.",
    question: "Which approach will BEST optimize the function's performance and reliability?",
    options: [
        "Reduce SQS batch size to 10 messages and enable DynamoDB auto-scaling",
        "Implement exponential backoff for DynamoDB writes with batch operations",
        "Use DynamoDB transactions for all write operations",
        "Process messages sequentially instead of in parallel"
    ],
    correct: 1,
    explanation: {
        correct: "Exponential backoff handles throttling gracefully. DynamoDB batch operations reduce API calls and improve throughput.",
        whyWrong: {
            0: "Smaller batch size reduces efficiency without solving throttling",
            2: "Transactions add overhead and have lower throughput limits",
            3: "Sequential processing reduces performance significantly"
        },
        examStrategy: "DynamoDB throttling: Exponential backoff + batch operations. Avoid unnecessary transactions."
    }
},
{
    id: 'dva_081',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A serverless API uses API Gateway, Lambda, and RDS Aurora Serverless. The application works well for low traffic but fails with database connection errors during traffic spikes. Each Lambda function creates a new database connection.",
    question: "What is the MOST effective solution to resolve the connection issues?",
    options: [
        "Increase Aurora Serverless max capacity and connection limits",
        "Implement RDS Proxy between Lambda and Aurora",
        "Use DynamoDB instead of Aurora for better Lambda integration",
        "Enable Lambda provisioned concurrency to maintain connections"
    ],
    correct: 1,
    explanation: {
        correct: "RDS Proxy pools and shares database connections, solving the Lambda connection scaling issue.",
        whyWrong: {
            0: "Increasing capacity doesn't solve the connection pooling problem",
            2: "DynamoDB might not fit the relational data requirements",
            3: "Provisioned concurrency doesn't solve database connection limits"
        },
        examStrategy: "Lambda + RDS connection issues = RDS Proxy. It handles connection pooling automatically."
    }
},
{
    id: 'dva_082',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function processes files uploaded to S3. The function must generate thumbnails in 3 different sizes and store them back to S3. Processing takes 45 seconds for large files, causing Lambda timeouts.",
    question: "What is the BEST approach to handle long-running image processing?",
    options: [
        "Increase Lambda timeout to 15 minutes and memory to 3GB",
        "Use Step Functions to orchestrate multiple Lambda functions for each thumbnail size",
        "Implement asynchronous processing with SQS and multiple Lambda functions",
        "Use EC2 instances with SQS for processing instead of Lambda"
    ],
    correct: 2,
    explanation: {
        correct: "SQS allows asynchronous processing. Multiple Lambda functions can process different thumbnail sizes in parallel efficiently.",
        whyWrong: {
            0: "Lambda has 15-minute limit, but this approach is expensive for long tasks",
            1: "Step Functions add complexity without solving the fundamental timeout issue",
            3: "EC2 introduces infrastructure management overhead"
        },
        examStrategy: "Long-running tasks in Lambda: Use SQS for async processing + parallel Lambda functions."
    }
},
{
    id: 'dva_083',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A real-time chat application uses API Gateway WebSocket API with Lambda functions. Users report that messages are sometimes delivered out of order and occasionally lost during high activity periods.",
    question: "What combination of services will BEST ensure message ordering and delivery?",
    options: [
        "Use SQS FIFO queues with Lambda and DynamoDB for message persistence",
        "Implement ElastiCache Redis with pub/sub for real-time messaging",
        "Use Kinesis Data Streams with Lambda consumers for ordered processing",
        "Add SQS standard queues between WebSocket API and Lambda"
    ],
    correct: 0,
    explanation: {
        correct: "SQS FIFO ensures message ordering. DynamoDB provides durable storage. Lambda processes messages reliably with dead letter queues.",
        whyWrong: {
            1: "Redis pub/sub doesn't guarantee delivery and lacks persistence",
            2: "Kinesis is overkill for chat and adds complexity",
            3: "Standard SQS doesn't guarantee order"
        },
        examStrategy: "Message ordering + delivery guarantee = SQS FIFO + DynamoDB persistence."
    }
},
{
    id: 'dva_084',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function retrieves user preferences from DynamoDB on every API call. The function has 300ms average response time, with 200ms spent on DynamoDB queries. User preferences change infrequently.",
    question: "Which caching strategy will provide the BEST performance improvement?",
    options: [
        "Enable DynamoDB DAX for microsecond latency",
        "Implement ElastiCache Redis with 1-hour TTL",
        "Use Lambda environment variables to cache data",
        "Enable API Gateway caching with 300-second TTL"
    ],
    correct: 1,
    explanation: {
        correct: "ElastiCache Redis provides sub-millisecond latency for frequently accessed data with appropriate TTL for infrequent changes.",
        whyWrong: {
            0: "DAX is expensive for simple read patterns",
            2: "Environment variables don't persist across invocations",
            3: "API Gateway caching caches responses, not data lookups"
        },
        examStrategy: "Infrequent changes + high read frequency = ElastiCache with appropriate TTL."
    }
},
{
    id: 'dva_085',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "A serverless application processes financial transactions using Lambda, DynamoDB, and SQS. The application must ensure exactly-once processing and maintain transaction consistency across multiple DynamoDB tables.",
    question: "What is the MOST appropriate approach to ensure data consistency?",
    options: [
        "Use DynamoDB transactions with SQS exactly-once processing",
        "Implement idempotency keys with conditional DynamoDB writes",
        "Use Step Functions with DynamoDB transaction support",
        "Implement saga pattern with compensating transactions"
    ],
    correct: 1,
    explanation: {
        correct: "Idempotency keys prevent duplicate processing. Conditional writes ensure atomic operations without the overhead of transactions.",
        whyWrong: {
            0: "SQS doesn't provide exactly-once processing natively",
            2: "Step Functions add complexity for simple consistency requirements",
            3: "Saga pattern is overkill for DynamoDB native transaction support"
        },
        examStrategy: "Exactly-once processing = Idempotency keys + conditional writes. Simpler than transactions."
    }
},
{
    id: 'dva_086',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function connects to an RDS MySQL database using environment variables for connection details. The application works in development but fails in production with 'Access Denied' errors.",
    question: "What is the MOST secure way to resolve this issue?",
    options: [
        "Store database credentials in Lambda environment variables with encryption",
        "Use AWS Secrets Manager to store and retrieve database credentials",
        "Hard-code credentials in the Lambda function code",
        "Use IAM database authentication instead of username/password"
    ],
    correct: 3,
    explanation: {
        correct: "IAM database authentication eliminates the need for stored credentials and uses temporary tokens with automatic rotation.",
        whyWrong: {
            0: "Environment variables are visible to anyone with Lambda access",
            1: "Secrets Manager is good but IAM auth is more secure",
            2: "Hard-coding credentials is a major security vulnerability"
        },
        examStrategy: "Database security: IAM database authentication > Secrets Manager > Environment variables."
    }
},
{
    id: 'dva_087',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A microservices application uses API Gateway with multiple Lambda functions. Each service needs to communicate with others, but direct Lambda-to-Lambda calls create tight coupling. The solution must support event-driven communication and handle failures gracefully.",
    question: "Which architectural pattern BEST addresses these requirements?",
    options: [
        "Use EventBridge for service communication with retry policies",
        "Implement REST API calls between services through API Gateway",
        "Use SQS with Lambda triggers for asynchronous communication",
        "Create a shared DynamoDB table for service coordination"
    ],
    correct: 0,
    explanation: {
        correct: "EventBridge provides loose coupling, content-based routing, automatic retries, and dead letter queues for failed events.",
        whyWrong: {
            1: "API Gateway calls maintain tight coupling",
            2: "SQS is good but lacks the routing capabilities of EventBridge",
            3: "Shared database creates data coupling"
        },
        examStrategy: "Service communication + failure handling = EventBridge with retry policies and DLQ."
    }
},
{
    id: 'dva_088',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function processes CSV files uploaded to S3. Files range from 10MB to 500MB. Small files process successfully, but large files cause Lambda to timeout or run out of memory.",
    question: "What is the MOST cost-effective solution for handling large files?",
    options: [
        "Increase Lambda memory to 3GB and timeout to 15 minutes",
        "Split large files into smaller chunks using S3 byte-range requests",
        "Use EC2 instances with Auto Scaling for large file processing",
        "Stream file processing using S3 Select and Lambda"
    ],
    correct: 1,
    explanation: {
        correct: "S3 byte-range requests allow processing files in chunks without loading entire file into memory. Cost-effective and scalable.",
        whyWrong: {
            0: "High memory Lambda is expensive for large files",
            2: "EC2 adds infrastructure overhead",
            3: "S3 Select is for querying, not general file processing"
        },
        examStrategy: "Large file processing in Lambda: Use S3 byte-range requests to process in chunks."
    }
},
{
    id: 'dva_089',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A serverless e-learning platform uses API Gateway, Lambda, and DynamoDB. Students can upload assignments to S3, which triggers Lambda for processing. During exam periods, upload volume increases 50x, causing DynamoDB throttling and Lambda timeout errors.",
    question: "Which combination of optimizations will BEST handle the traffic surge?",
    options: [
        "Enable DynamoDB auto-scaling and increase Lambda concurrency limits",
        "Implement SQS with batch processing and DynamoDB on-demand billing",
        "Use Step Functions to orchestrate the workflow with error handling",
        "Deploy additional API Gateway stages with weighted routing"
    ],
    correct: 1,
    explanation: {
        correct: "SQS buffers the traffic spikes and enables batch processing. DynamoDB on-demand automatically scales without pre-provisioning capacity.",
        whyWrong: {
            0: "Auto-scaling has delays and may not keep up with 50x spikes",
            2: "Step Functions don't solve the fundamental capacity issues",
            3: "Multiple stages don't solve backend capacity problems"
        },
        examStrategy: "Traffic spikes: SQS for buffering + DynamoDB on-demand for instant scaling."
    }
},
{
    id: 'dva_090',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function validates user input and stores data in DynamoDB. The function must prevent duplicate submissions based on a combination of user ID and timestamp within a 5-minute window.",
    question: "What is the MOST efficient approach to implement this requirement?",
    options: [
        "Use DynamoDB conditional writes with a composite key",
        "Query DynamoDB for existing records before each write",
        "Implement application-level locking with ElastiCache",
        "Use DynamoDB transactions to check and write atomically"
    ],
    correct: 0,
    explanation: {
        correct: "Conditional writes with composite key (userID#timestamp) provide atomic duplicate prevention without additional reads.",
        whyWrong: {
            1: "Separate query creates race conditions and consumes additional RCU",
            2: "Application locking is complex and error-prone",
            3: "Transactions are overkill and more expensive"
        },
        examStrategy: "Duplicate prevention: DynamoDB conditional writes with composite keys. Atomic and efficient."
    }
},
{
    id: 'dva_091',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A real-time analytics application ingests data through API Gateway to Lambda, which processes and stores results in DynamoDB. The application must handle 10,000 requests per second with sub-second latency requirements and cannot lose any data.",
    question: "Which architecture will BEST meet these requirements?",
    options: [
        "API Gateway → Lambda → DynamoDB with provisioned concurrency",
        "API Gateway → Kinesis Data Streams → Lambda → DynamoDB",
        "Application Load Balancer → Lambda → DynamoDB with DAX",
        "API Gateway → SQS → Lambda → DynamoDB with auto-scaling"
    ],
    correct: 1,
    explanation: {
        correct: "Kinesis Data Streams handles high throughput with guaranteed ordering and durability. Lambda consumers can process at scale with checkpointing.",
        whyWrong: {
            0: "API Gateway has 10,000 RPS limit and Lambda direct coupling creates bottlenecks",
            2: "ALB doesn't provide the durability guarantees needed",
            3: "SQS may not maintain order and has delivery delays"
        },
        examStrategy: "High throughput + no data loss + ordering = Kinesis Data Streams with Lambda consumers."
    }
},
{
    id: 'dva_092',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function processes messages from SQS and calls multiple external APIs. Some API calls fail intermittently, causing the entire message processing to fail and return to the queue.",
    question: "What is the BEST strategy to improve message processing reliability?",
    options: [
        "Increase SQS visibility timeout and implement retry logic in Lambda",
        "Use SQS dead letter queue and process each API call independently",
        "Implement exponential backoff for all API calls with circuit breaker",
        "Use Step Functions to handle API calls with automatic retries"
    ],
    correct: 2,
    explanation: {
        correct: "Exponential backoff handles transient failures gracefully. Circuit breaker prevents cascade failures when APIs are down.",
        whyWrong: {
            0: "Visibility timeout doesn't solve API reliability issues",
            1: "DLQ is for permanent failures, not transient API issues",
            3: "Step Functions add complexity for simple retry logic"
        },
        examStrategy: "API reliability: Exponential backoff + circuit breaker pattern for resilient external calls."
    }
},
{
    id: 'dva_093',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "A serverless image processing application must generate multiple image formats (JPEG, PNG, WebP) from uploaded images. Processing can take 2-10 minutes depending on image size. Users need real-time status updates during processing.",
    question: "Which architecture provides the BEST user experience and scalability?",
    options: [
        "Lambda with WebSocket API for real-time updates and EFS for temporary storage",
        "Step Functions Express workflows with EventBridge for status notifications",
        "SQS with Lambda processing and DynamoDB for status tracking with WebSocket API",
        "ECS Fargate tasks with EventBridge and WebSocket API for real-time updates"
    ],
    correct: 2,
    explanation: {
        correct: "SQS handles long-running tasks efficiently. DynamoDB tracks status. WebSocket API provides real-time updates to users.",
        whyWrong: {
            0: "Lambda 15-minute limit may not be sufficient, EFS adds latency",
            1: "Express workflows are for sub-minute tasks, not 2-10 minutes",
            3: "ECS adds infrastructure complexity compared to serverless approach"
        },
        examStrategy: "Long tasks + real-time updates = SQS + Lambda + DynamoDB status + WebSocket API."
    }
},
{
    id: 'dva_094',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function reads configuration data from Parameter Store on every invocation. The configuration changes weekly, but the function is called thousands of times per minute, resulting in high Parameter Store costs.",
    question: "What is the MOST cost-effective caching strategy?",
    options: [
        "Cache parameters in Lambda environment variables during deployment",
        "Use ElastiCache Redis to cache Parameter Store values",
        "Implement in-memory caching with TTL in Lambda global variables",
        "Store configuration in DynamoDB instead of Parameter Store"
    ],
    correct: 2,
    explanation: {
        correct: "Lambda global variables persist across invocations within the same execution context. TTL ensures freshness while minimizing API calls.",
        whyWrong: {
            0: "Environment variables require redeployment for configuration changes",
            1: "ElastiCache adds infrastructure cost and complexity",
            3: "DynamoDB doesn't solve the frequent reading cost issue"
        },
        examStrategy: "Parameter Store caching: Lambda global variables with TTL. Free and efficient."
    }
},
{
    id: 'dva_095',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A multi-tenant SaaS application uses a single Lambda function to serve all tenants. Each tenant has isolated data in separate DynamoDB tables. The function must enforce strict tenant isolation and cannot access another tenant's data.",
    question: "What is the MOST secure approach to implement tenant isolation?",
    options: [
        "Use different IAM roles for each tenant with assume role mechanism",
        "Implement row-level security with tenant ID in DynamoDB table names",
        "Use Lambda environment variables to store tenant-specific table names",
        "Implement application-level validation with tenant ID in all DynamoDB operations"
    ],
    correct: 0,
    explanation: {
        correct: "Different IAM roles provide the strongest isolation. AssumeRole ensures the function can only access resources for the specific tenant.",
        whyWrong: {
            1: "Table naming doesn't prevent access if credentials are compromised",
            2: "Environment variables are visible and don't provide security isolation",
            3: "Application-level validation can be bypassed and is error-prone"
        },
        examStrategy: "Multi-tenant security isolation: Different IAM roles per tenant with AssumeRole."
    }
},
{
    id: 'dva_096',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function processes IoT sensor data from Kinesis Data Streams. Data arrives at varying rates - sometimes 10 records per second, sometimes 10,000. The function occasionally falls behind during high-traffic periods.",
    question: "What configuration will BEST optimize processing performance?",
    options: [
        "Increase Lambda memory and enable enhanced fan-out for Kinesis",
        "Use multiple Lambda functions with different Kinesis shard assignments",
        "Implement parallel processing with increased batch size and parallelization factor",
        "Switch to Kinesis Data Firehose for batch processing"
    ],
    correct: 2,
    explanation: {
        correct: "Parallelization factor allows multiple Lambda instances per shard. Larger batch size reduces invocation overhead.",
        whyWrong: {
            0: "Enhanced fan-out doesn't solve processing capacity issues",
            1: "Manual shard assignment is complex and error-prone",
            3: "Firehose is for data loading, not real-time processing"
        },
        examStrategy: "Kinesis processing optimization: Increase parallelization factor + batch size for high throughput."
    }
},
{
    id: 'dva_097',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A serverless application processes financial transactions requiring strong consistency and audit trails. Transactions involve multiple DynamoDB tables and must be rolled back completely if any part fails.",
    question: "Which approach provides the STRONGEST consistency guarantees?",
    options: [
        "Use DynamoDB transactions with client request tokens for idempotency",
        "Implement two-phase commit pattern with application-level coordination",
        "Use Step Functions with DynamoDB transactions and error handling",
        "Implement saga pattern with compensating actions for each transaction"
    ],
    correct: 0,
    explanation: {
        correct: "DynamoDB transactions provide ACID properties across multiple tables. Client request tokens ensure idempotency for retries.",
        whyWrong: {
            1: "Two-phase commit is complex and can lead to inconsistent states",
            2: "Step Functions don't add value over native DynamoDB transactions",
            3: "Saga pattern is eventually consistent, not strongly consistent"
        },
        examStrategy: "Strong consistency across tables: DynamoDB transactions + idempotency tokens."
    }
},
{
    id: 'dva_098',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function downloads files from S3, processes them, and uploads results back to S3. Processing fails intermittently with 'No space left on device' errors for large files.",
    question: "What is the MOST effective solution to resolve this issue?",
    options: [
        "Increase Lambda memory allocation to get more temporary disk space",
        "Stream processing without downloading entire files to /tmp",
        "Use EFS mounted to Lambda for additional storage space",
        "Split large files into smaller chunks before processing"
    ],
    correct: 1,
    explanation: {
        correct: "Streaming processing eliminates the need for temporary storage and works with files of any size within Lambda memory limits.",
        whyWrong: {
            0: "Lambda /tmp space is limited to 512MB regardless of memory",
            2: "EFS adds latency and complexity for temporary processing",
            3: "File splitting doesn't solve the fundamental storage limitation"
        },
        examStrategy: "Lambda storage limits: Stream processing to avoid /tmp space limitations (512MB max)."
    }
},
{
    id: 'dva_099',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "A real-time gaming application uses API Gateway WebSocket API and Lambda for player communication. During peak hours with 10,000 concurrent connections, players experience message delivery delays of 3-5 seconds.",
    question: "Which optimization will MOST effectively reduce message latency?",
    options: [
        "Enable API Gateway caching and increase Lambda provisioned concurrency",
        "Use ElastiCache Redis pub/sub with Lambda and connection pooling",
        "Implement connection management with DynamoDB and batch message delivery",
        "Use Kinesis Data Streams for message routing with Lambda consumers"
    ],
    correct: 1,
    explanation: {
        correct: "Redis pub/sub provides sub-millisecond latency for real-time messaging. Connection pooling reduces Lambda cold starts.",
        whyWrong: {
            0: "API Gateway caching doesn't help with WebSocket real-time messages",
            2: "Batch delivery increases latency for real-time requirements",
            3: "Kinesis adds unnecessary overhead for simple message routing"
        },
        examStrategy: "Real-time gaming latency: ElastiCache Redis pub/sub for microsecond message delivery."
    }
},
{
    id: 'dva_100',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function processes form submissions and sends confirmation emails via SES. Email sending occasionally fails with throttling errors during marketing campaigns when submission volume increases 20x.",
    question: "What is the BEST approach to handle email sending reliably?",
    options: [
        "Increase SES sending quota and implement retry logic in Lambda",
        "Use SQS to queue email requests and process them asynchronously",
        "Implement exponential backoff with jitter for SES API calls",
        "Use multiple SES regions to distribute email sending load"
    ],
    correct: 1,
    explanation: {
        correct: "SQS decouples email sending from form processing, allowing emails to be processed at SES rate limits without blocking user responses.",
        whyWrong: {
            0: "Quota increases require approval and don't solve immediate throttling",
            2: "Backoff helps but doesn't solve the fundamental rate limiting issue",
            3: "Multiple regions add complexity and may have deliverability implications"
        },
        examStrategy: "Email throttling: SQS for async processing + respect SES rate limits with separate Lambda."
    }
},
{
    id: 'dva_101',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A document processing application uses Lambda to extract text from PDFs stored in S3. The application must support documents up to 100MB and provide progress updates to users. Processing can take 5-15 minutes depending on document complexity.",
    question: "Which architecture BEST supports these requirements?",
    options: [
        "Use Lambda with 15-minute timeout and WebSocket API for progress updates",
        "Use ECS Fargate tasks with EventBridge for progress notifications",
        "Implement Step Functions with Lambda and S3 for progress tracking",
        "Use SQS with Lambda and DynamoDB streams for real-time progress"
    ],
    correct: 1,
    explanation: {
        correct: "ECS Fargate handles long-running tasks without time limits and large memory requirements. EventBridge provides reliable progress notifications.",
        whyWrong: {
            0: "Lambda 15-minute limit may not be sufficient for complex documents",
            2: "Step Functions have complexity limits for long-running workflows",
            3: "Lambda timeout limitations still apply with SQS"
        },
        examStrategy: "Long-running document processing: ECS Fargate for unlimited runtime + EventBridge for progress."
    }
},
{
    id: 'dva_102',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function queries multiple DynamoDB tables to generate user reports. The function takes 8 seconds to complete due to sequential table queries, causing API Gateway timeout errors.",
    question: "What optimization will provide the GREATEST performance improvement?",
    options: [
        "Increase Lambda memory allocation from 512MB to 3GB",
        "Implement parallel queries using Promise.all() or async/await",
        "Use DynamoDB transactions to query all tables atomically",
        "Enable DynamoDB DAX for all tables to reduce query latency"
    ],
    correct: 1,
    explanation: {
        correct: "Parallel queries can run simultaneously, potentially reducing total time from 8s to the longest individual query time.",
        whyWrong: {
            0: "Memory increase won't significantly improve I/O bound operations",
            2: "Transactions are for consistency, not performance",
            3: "DAX improves individual query speed but doesn't address sequential processing"
        },
        examStrategy: "Sequential I/O operations: Implement parallel processing with Promise.all() for dramatic speedup."
    }
},
{
    id: 'dva_103',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A microservices application uses Lambda functions that need to share configuration data, user sessions, and temporary computation results. Data needs to be accessible across different Lambda functions with sub-millisecond latency requirements.",
    question: "Which combination of services provides the BEST solution for shared state management?",
    options: [
        "DynamoDB with DAX for all shared data with consistent reads",
        "ElastiCache Redis cluster with different data structures for each use case",
        "S3 with Transfer Acceleration for shared configuration and session data",
        "EFS mounted to all Lambda functions for shared file-based storage"
    ],
    correct: 1,
    explanation: {
        correct: "Redis supports multiple data structures (strings for config, hashes for sessions, lists for temp data) with sub-millisecond latency.",
        whyWrong: {
            0: "DAX is fast but doesn't provide the data structure flexibility needed",
            2: "S3 has seconds-level latency, not sub-millisecond",
            3: "EFS has higher latency and is designed for file storage, not data structures"
        },
        examStrategy: "Sub-millisecond shared state: ElastiCache Redis with appropriate data structures for each use case."
    }
},
{
    id: 'dva_104',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function processes webhook events from external services. Events arrive sporadically but sometimes in bursts of thousands within seconds. The function must process all events without loss.",
    question: "What is the MOST reliable architecture to handle event bursts?",
    options: [
        "Use API Gateway with Lambda proxy integration and increase concurrency",
        "Implement SQS between webhook and Lambda with dead letter queue",
        "Use Kinesis Data Streams to buffer events with Lambda consumers",
        "Deploy multiple Lambda functions behind an Application Load Balancer"
    ],
    correct: 1,
    explanation: {
        correct: "SQS provides reliable buffering for bursty traffic. DLQ ensures no events are lost even if processing fails.",
        whyWrong: {
            0: "API Gateway direct integration can still be overwhelmed during bursts",
            2: "Kinesis is overkill for simple webhook processing",
            3: "ALB doesn't integrate directly with Lambda for webhook processing"
        },
        examStrategy: "Bursty webhook processing: SQS buffering + DLQ for reliability + Lambda consumers."
    }
},
{
    id: 'dva_105',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A serverless data pipeline processes files uploaded to S3, transforms data, and stores results in multiple destinations (DynamoDB, RDS, and another S3 bucket). The pipeline must ensure data consistency across all destinations and handle partial failures gracefully.",
    question: "Which orchestration approach provides the BEST reliability and consistency?",
    options: [
        "Use Step Functions Standard workflow with error handling and retry logic",
        "Implement Lambda with DynamoDB transactions and manual rollback logic",
        "Use EventBridge with multiple targets and implement compensating actions",
        "Create multiple SQS queues for each destination with Lambda processors"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions provide built-in error handling, retries, and rollback capabilities for complex multi-step workflows.",
        whyWrong: {
            1: "DynamoDB transactions don't extend to RDS and S3",
            2: "EventBridge doesn't provide workflow orchestration for error handling",
            3: "Multiple SQS queues lack coordination for consistency"
        },
        examStrategy: "Multi-destination data consistency: Step Functions for workflow orchestration + error handling."
    }
},
{
    id: 'dva_106',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function analyzes log files from S3 and generates metrics stored in CloudWatch. Processing large log files (500MB+) causes memory errors even with 3GB Lambda memory allocation.",
    question: "What is the MOST efficient approach to process large log files?",
    options: [
        "Use S3 Select to pre-filter log data before Lambda processing",
        "Stream log files line-by-line using S3 getObject with streaming",
        "Split log files into smaller chunks using S3 multipart upload",
        "Use Athena to analyze logs and store results in S3, then trigger Lambda"
    ],
    correct: 1,
    explanation: {
        correct: "Streaming allows processing files of any size with constant memory usage, processing one line at a time.",
        whyWrong: {
            0: "S3 Select is for querying structured data, not general log processing",
            2: "File splitting doesn't solve the processing memory requirements",
            3: "Athena adds complexity and cost for simple log analysis"
        },
        examStrategy: "Large file processing in Lambda: Stream processing line-by-line to maintain constant memory usage."
    }
},
{
    id: 'dva_107',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "A real-time recommendation engine uses Lambda to process user behavior events and update user profiles in DynamoDB. The system must handle 50,000 events per second with updates to user profiles within 100ms of event receipt.",
    question: "Which architecture will BEST meet the latency and throughput requirements?",
    options: [
        "Kinesis Data Streams with Lambda consumers and DynamoDB with provisioned capacity",
        "API Gateway with Lambda and DynamoDB on-demand with DAX caching",
        "SQS with multiple Lambda functions and DynamoDB auto-scaling",
        "EventBridge with Lambda consumers and DynamoDB Global Tables"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis provides high throughput with ordering guarantees. Lambda consumers can process in parallel. Provisioned capacity ensures consistent performance.",
        whyWrong: {
            1: "API Gateway has throughput limits for 50,000 RPS",
            2: "SQS has delivery delays that may exceed 100ms requirement",
            3: "EventBridge routing adds latency for real-time requirements"
        },
        examStrategy: "High throughput + low latency: Kinesis Data Streams + Lambda + DynamoDB provisioned capacity."
    }
},
{
    id: 'dva_108',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function processes sensor data and must filter out duplicate readings based on sensor ID and timestamp within a 1-minute window. The function receives 1000 messages per second and must maintain high performance.",
    question: "What is the MOST performant duplicate detection strategy?",
    options: [
        "Use ElastiCache Redis with key expiration for recent sensor readings",
        "Query DynamoDB with GSI on sensor ID and timestamp for each message",
        "Maintain an in-memory set in Lambda global variables with periodic cleanup",
        "Use DynamoDB streams to track duplicate sensor readings"
    ],
    correct: 0,
    explanation: {
        correct: "Redis provides sub-millisecond lookups with automatic key expiration for the 1-minute window. Scales to handle 1000 messages/second.",
        whyWrong: {
            1: "DynamoDB queries add latency and consume read capacity",
            2: "Global variables are per-execution context and don't scale across invocations",
            3: "DynamoDB streams are for reacting to changes, not duplicate detection"
        },
        examStrategy: "High-performance duplicate detection: ElastiCache Redis with TTL for automatic cleanup."
    }
},
{
    id: 'dva_109',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A serverless application processes financial reports that must be generated exactly once per day at midnight UTC. If generation fails, it must retry with exponential backoff but not duplicate successful runs. The process involves multiple Lambda functions and can take 30-60 minutes to complete.",
    question: "Which architecture ensures reliable, exactly-once execution?",
    options: [
        "EventBridge scheduled rule with Step Functions Express workflow",
        "CloudWatch Events with Step Functions Standard workflow and DynamoDB execution tracking",
        "Lambda with CloudWatch Events and DynamoDB conditional writes for deduplication",
        "AWS Batch with EventBridge and S3 for execution state management"
    ],
    correct: 1,
    explanation: {
        correct: "Step Functions Standard supports long-running workflows with built-in retry logic. DynamoDB tracking ensures exactly-once execution across retries.",
        whyWrong: {
            0: "Express workflows have 5-minute limit, too short for 30-60 minute process",
            2: "Lambda 15-minute limit prevents 30-60 minute workflows",
            3: "AWS Batch adds infrastructure complexity for serverless requirements"
        },
        examStrategy: "Long-running exactly-once workflows: Step Functions Standard + DynamoDB execution tracking."
    }
},
{
    id: 'dva_110',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function processes user uploads and generates thumbnails of different sizes. The function must handle images up to 50MB and provide different thumbnail formats (JPEG, PNG, WebP) efficiently.",
    question: "What is the MOST efficient approach for thumbnail generation?",
    options: [
        "Process all thumbnail sizes sequentially in a single Lambda function",
        "Use separate Lambda functions for each thumbnail size triggered by S3 events",
        "Implement parallel processing within one Lambda function using child processes",
        "Use Lambda Layers for image processing libraries and process formats in parallel"
    ],
    correct: 3,
    explanation: {
        correct: "Lambda Layers reduce deployment size and cold start time. Parallel processing of different formats maximizes efficiency within Lambda timeout.",
        whyWrong: {
            0: "Sequential processing is slow and may hit timeout limits",
            1: "Multiple functions increase complexity and cold start overhead",
            2: "Child processes add complexity and resource overhead"
        },
        examStrategy: "Efficient image processing: Lambda Layers for libraries + parallel format processing within function."
    }
},
{
    id: 'dva_111',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A multi-tenant application serves different customers through a single API Gateway and Lambda function. Each tenant has different rate limits, authentication requirements, and data isolation needs. The solution must scale to 1000+ tenants efficiently.",
    question: "Which approach provides the BEST scalability and isolation?",
    options: [
        "Use API Gateway usage plans with different API keys for each tenant",
        "Implement tenant-specific Lambda functions with separate API Gateway stages",
        "Use Lambda environment variables for tenant configuration with request routing",
        "Implement dynamic tenant resolution with database-driven configuration"
    ],
    correct: 3,
    explanation: {
        correct: "Database-driven configuration scales to unlimited tenants without code changes. Dynamic resolution allows flexible tenant management.",
        whyWrong: {
            0: "API keys have management overhead and don't scale to 1000+ tenants",
            1: "Separate functions per tenant create management nightmare at scale",
            2: "Environment variables require deployment for each tenant change"
        },
        examStrategy: "Multi-tenant scaling: Database-driven tenant configuration for dynamic resolution + isolation."
    }
},
{
    id: 'dva_112',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function calls multiple third-party APIs to enrich user data. Some APIs are unreliable and may take up to 30 seconds to respond or fail intermittently. The function must complete within 60 seconds total.",
    question: "What strategy will BEST optimize function reliability and performance?",
    options: [
        "Implement parallel API calls with Promise.allSettled() and timeout handling",
        "Use Step Functions to orchestrate API calls with individual timeouts",
        "Implement sequential API calls with exponential backoff for failures",
        "Use SQS to queue API calls and process them asynchronously"
    ],
    correct: 0,
    explanation: {
        correct: "Promise.allSettled() allows parallel processing with individual timeouts. Function completes when fastest APIs finish, doesn't wait for slow ones.",
        whyWrong: {
            1: "Step Functions add complexity for simple API orchestration",
            2: "Sequential processing wastes time and may hit 60s limit",
            3: "SQS makes the process asynchronous, changing the user experience"
        },
        examStrategy: "Multiple API calls: Parallel processing with timeouts using Promise.allSettled() for resilience."
    }
},
{
    id: 'dva_113',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A serverless analytics platform processes user events in real-time and must maintain running aggregations (counts, sums, averages) over sliding time windows. Updates must be visible to dashboard users within 5 seconds of event ingestion.",
    question: "Which architecture provides REAL-TIME aggregation with 5-second latency?",
    options: [
        "Kinesis Data Analytics with Lambda output to DynamoDB and DAX",
        "Lambda with Kinesis triggers updating ElastiCache Redis with sliding windows",
        "Kinesis Data Firehose with Lambda transformation to S3 and Athena queries",
        "EventBridge with Lambda consumers updating DynamoDB with TTL-based windows"
    ],
    correct: 1,
    explanation: {
        correct: "Lambda with Kinesis provides real-time processing. Redis sliding windows enable efficient time-based aggregations with sub-second read latency.",
        whyWrong: {
            0: "Kinesis Analytics has complexity and latency overhead",
            2: "Firehose has minimum 60-second buffering, violating 5-second requirement",
            3: "EventBridge routing adds latency for real-time requirements"
        },
        examStrategy: "Real-time sliding windows: Lambda + Kinesis + Redis for sub-second aggregation updates."
    }
},
{
    id: 'dva_114',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function processes form data and must validate input against multiple business rules stored in different services (DynamoDB for user rules, RDS for business logic, Parameter Store for validation rules). Validation must complete within 2 seconds.",
    question: "What caching strategy will BEST optimize validation performance?",
    options: [
        "Cache all validation rules in Lambda environment variables at deployment",
        "Use ElastiCache Redis to cache frequently accessed validation rules",
        "Implement multi-level caching with Lambda memory and ElastiCache fallback",
        "Cache validation results in DynamoDB with TTL-based expiration"
    ],
    correct: 2,
    explanation: {
        correct: "Multi-level caching provides fastest access for frequently used rules (Lambda memory) with broader coverage (ElastiCache) for less frequent rules.",
        whyWrong: {
            0: "Environment variables can't be updated without redeployment",
            1: "Single-level caching may still have latency for frequently accessed rules",
            3: "Caching results instead of rules doesn't optimize rule lookup performance"
        },
        examStrategy: "Performance optimization: Multi-level caching (Lambda memory + ElastiCache) for maximum speed."
    }
},
{
    id: 'dva_115',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "A document collaboration platform uses Lambda and DynamoDB to handle real-time document editing. Multiple users can edit the same document simultaneously, requiring conflict resolution and operational transformation to maintain document consistency.",
    question: "Which approach BEST handles concurrent editing with conflict resolution?",
    options: [
        "Use DynamoDB optimistic locking with version numbers and retry logic",
        "Implement operational transformation with ElastiCache Redis and pub/sub",
        "Use DynamoDB streams with Lambda for conflict detection and resolution",
        "Implement event sourcing with DynamoDB and Lambda for state reconstruction"
    ],
    correct: 1,
    explanation: {
        correct: "Operational transformation is the standard for real-time collaborative editing. Redis pub/sub provides real-time synchronization between users.",
        whyWrong: {
            0: "Optimistic locking causes frequent conflicts with multiple simultaneous editors",
            2: "DynamoDB streams are eventually consistent, not suitable for real-time collaboration",
            3: "Event sourcing adds complexity without solving operational transformation requirements"
        },
        examStrategy: "Real-time collaborative editing: Operational transformation + Redis pub/sub for conflict-free editing."
    }
},
{
    id: 'dva_116',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function processes customer orders and must update inventory across multiple warehouse systems via REST APIs. Some warehouse APIs are slow (5-10 seconds) and the function must maintain data consistency if any update fails.",
    question: "What pattern provides the BEST consistency and performance?",
    options: [
        "Use DynamoDB transactions for inventory tracking with compensation logic",
        "Implement saga pattern with compensating transactions for each warehouse",
        "Use Step Functions with parallel API calls and error handling",
        "Process inventory updates asynchronously with SQS and eventual consistency"
    ],
    correct: 1,
    explanation: {
        correct: "Saga pattern handles distributed transactions across external systems with compensating actions for rollback if any warehouse update fails.",
        whyWrong: {
            0: "DynamoDB transactions don't extend to external REST APIs",
            2: "Step Functions don't provide transaction semantics for external APIs",
            3: "Eventual consistency doesn't meet the consistency requirements"
        },
        examStrategy: "Distributed transactions with external APIs: Saga pattern with compensating transactions."
    }
},
{
    id: 'dva_117',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A machine learning inference service uses Lambda to serve model predictions. Models are 200MB TensorFlow files stored in S3. Cold starts take 15 seconds to download and initialize models, causing user timeouts.",
    question: "Which optimization provides the FASTEST model loading and inference?",
    options: [
        "Use Lambda provisioned concurrency with models pre-loaded in memory",
        "Store models in EFS mounted to Lambda for faster access",
        "Use Lambda Container Images with models built into the container",
        "Implement model caching in ElastiCache Redis with lazy loading"
    ],
    correct: 2,
    explanation: {
        correct: "Container images include models in the image, eliminating download time. Lambda containers support up to 10GB, easily accommodating 200MB models.",
        whyWrong: {
            0: "Provisioned concurrency is expensive and still requires initial model loading",
            1: "EFS has higher latency than container image storage",
            3: "Redis can't store 200MB objects efficiently"
        },
        examStrategy: "Large model serving: Lambda Container Images with models built-in for fastest cold start."
    }
},
{
    id: 'dva_118',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A data processing pipeline uses Lambda to transform CSV files in S3. Files contain millions of rows and processing must be resumable if the function times out or fails partway through.",
    question: "What approach enables RESUMABLE processing of large CSV files?",
    options: [
        "Use S3 Select with pagination to process files in chunks with state tracking",
        "Split files into smaller pieces using S3 multipart upload before processing",
        "Use Lambda with checkpoint storage in DynamoDB for progress tracking",
        "Implement streaming with S3 byte-range requests and DynamoDB progress state"
    ],
    correct: 3,
    explanation: {
        correct: "Byte-range requests allow reading from specific file positions. DynamoDB tracks progress so processing can resume from the last checkpoint.",
        whyWrong: {
            0: "S3 Select is for querying, not general file processing with resumption",
            1: "File splitting changes the original file structure",
            2: "Checkpointing without byte-range access still requires reprocessing from start"
        },
        examStrategy: "Resumable file processing: S3 byte-range requests + DynamoDB progress tracking for checkpoints."
    }
},
{
    id: 'dva_119',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A real-time bidding platform must process bid requests, evaluate complex rules, and respond within 100ms. The system handles 100,000 requests per second with sub-millisecond latency requirements for rule evaluation.",
    question: "Which architecture achieves SUB-MILLISECOND rule evaluation at scale?",
    options: [
        "Lambda with DynamoDB DAX for rule storage and evaluation",
        "Lambda with ElastiCache Redis for rule caching and in-memory evaluation",
        "API Gateway with Lambda and Parameter Store for rule management",
        "ECS Fargate with Redis cluster for rule storage and parallel processing"
    ],
    correct: 1,
    explanation: {
        correct: "Redis provides sub-millisecond access for rule data. In-memory evaluation in Lambda eliminates database round trips for complex rule processing.",
        whyWrong: {
            0: "DAX provides microsecond latency but DynamoDB queries still add overhead",
            2: "Parameter Store has API rate limits and higher latency",
            3: "ECS adds container management overhead compared to serverless Lambda"
        },
        examStrategy: "Sub-millisecond performance: Lambda + Redis for data + in-memory evaluation for complex logic."
    }
},
{
    id: 'dva_120',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A serverless application processes user-generated content uploads (images, videos, documents) with different processing requirements for each file type. Processing workflows can take 1-30 minutes depending on file size and type.",
    question: "What architecture provides the MOST flexible content processing pipeline?",
    options: [
        "Use multiple Lambda functions with S3 event filtering by file extension",
        "Implement Step Functions workflow with conditional branching based on file type",
        "Use SQS with Lambda consumers and file type routing logic",
        "Deploy separate API Gateway endpoints for each content type"
    ],
    correct: 1,
    explanation: {
        correct: "Step Functions provide workflow orchestration with conditional logic, error handling, and support for long-running processes beyond Lambda limits.",
        whyWrong: {
            0: "Multiple Lambda functions lack orchestration for complex workflows",
            2: "SQS routing doesn't handle workflow orchestration or long-running tasks",
            3: "Separate endpoints create management overhead without workflow benefits"
        },
        examStrategy: "Complex content processing: Step Functions for workflow orchestration + conditional branching by file type."
    }
},
{
    id: 'dva_121',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "A financial trading platform uses Lambda to process market data feeds and must calculate complex derivatives pricing in real-time. Calculations require access to historical data (100GB+) and must complete within 50ms per calculation.",
    question: "Which data access pattern provides the FASTEST calculation performance?",
    options: [
        "Use EFS with Lambda for shared access to historical data files",
        "Pre-load frequently accessed data in Lambda memory with ElastiCache fallback",
        "Use DynamoDB with partition key design optimized for time-series queries",
        "Implement data tiering with Redis for hot data and S3 for historical data"
    ],
    correct: 1,
    explanation: {
        correct: "Lambda memory provides fastest access (nanoseconds). ElastiCache fallback ensures broader data coverage while maintaining sub-millisecond access.",
        whyWrong: {
            0: "EFS has millisecond latency, too slow for 50ms calculation requirement",
            2: "DynamoDB queries add latency even with optimized partition design",
            3: "S3 has seconds-level latency for historical data access"
        },
        examStrategy: "Ultra-fast data access: Lambda memory for hot data + ElastiCache for broader historical data."
    }
},
{
    id: 'dva_122',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A notification service uses Lambda to send push notifications via multiple providers (SNS, third-party APIs). The service must handle provider failures gracefully and ensure notification delivery through alternative providers.",
    question: "What pattern provides the MOST reliable notification delivery?",
    options: [
        "Implement circuit breaker pattern with provider failover logic",
        "Use multiple SQS queues with different Lambda functions for each provider",
        "Deploy notifications to all providers simultaneously for redundancy",
        "Use Step Functions with retry logic and provider fallback sequence"
    ],
    correct: 0,
    explanation: {
        correct: "Circuit breaker prevents cascading failures and enables automatic failover to healthy providers while monitoring provider health.",
        whyWrong: {
            1: "Multiple queues lack coordination for failover scenarios",
            2: "Simultaneous delivery creates duplicate notifications",
            3: "Step Functions add complexity for simple failover logic"
        },
        examStrategy: "Provider failover reliability: Circuit breaker pattern with automatic provider health monitoring."
    }
},
{
    id: 'dva_123',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A content recommendation engine analyzes user behavior in real-time to update recommendation models. The system processes 1 million events per minute and must update user profiles and global models with minimal latency impact on user experience.",
    question: "Which architecture provides REAL-TIME processing with minimal user impact?",
    options: [
        "Kinesis Data Streams with Lambda for real-time processing and batch model updates",
        "API Gateway with Lambda for real-time updates to DynamoDB and background model training",
        "EventBridge with multiple Lambda consumers for parallel profile and model updates",
        "SQS with Lambda for buffered processing and scheduled model retraining"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis handles high throughput with ordering. Lambda processes events in real-time for profiles while batch processing optimizes model updates.",
        whyWrong: {
            1: "API Gateway has throughput limits for 1M events per minute",
            2: "EventBridge routing adds latency for real-time requirements",
            3: "SQS buffering introduces delays that impact recommendation freshness"
        },
        examStrategy: "High-throughput real-time processing: Kinesis + Lambda for event processing + batch model updates."
    }
},
{
    id: 'dva_124',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A serverless application generates PDF reports from user data stored in multiple DynamoDB tables. Report generation takes 30-45 seconds and users need to be notified when reports are ready for download.",
    question: "What approach provides the BEST user experience for report generation?",
    options: [
        "Use Lambda with 15-minute timeout and WebSocket API for progress updates",
        "Implement asynchronous processing with SQS, Lambda, and SNS notifications",
        "Use Step Functions with Lambda and email notifications when complete",
        "Process reports synchronously with API Gateway and long polling"
    ],
    correct: 1,
    explanation: {
        correct: "SQS enables asynchronous processing without blocking user requests. SNS provides flexible notification options (email, SMS, push).",
        whyWrong: {
            0: "45-second processing in Lambda works but WebSocket adds complexity",
            2: "Step Functions add overhead for simple linear workflow",
            3: "Synchronous processing blocks users for 30-45 seconds"
        },
        examStrategy: "Long-running user-initiated tasks: Async processing with SQS + SNS notifications for completion."
    }
},
{
    id: 'dva_125',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A multiplayer game backend uses Lambda and DynamoDB to handle player actions. The game requires strict ordering of player actions and must prevent race conditions when multiple players interact with the same game objects simultaneously.",
    question: "Which approach ensures STRICT action ordering and prevents race conditions?",
    options: [
        "Use DynamoDB transactions with conditional writes and retry logic",
        "Implement single-threaded processing with SQS FIFO and Lambda consumers",
        "Use optimistic locking with version numbers and conflict resolution",
        "Process actions through Kinesis Data Streams with partition keys by game object"
    ],
    correct: 3,
    explanation: {
        correct: "Kinesis partition keys ensure all actions for the same game object are processed in order by the same Lambda instance, preventing race conditions.",
        whyWrong: {
            0: "Transactions help with consistency but don't guarantee action ordering",
            1: "Single-threaded processing doesn't scale for multiple game objects",
            2: "Optimistic locking creates conflicts with high concurrency"
        },
        examStrategy: "Action ordering + race condition prevention: Kinesis partitioning by game object for ordered processing."
    }
},
{
    id: 'dva_126',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function processes sensor data from IoT devices and must detect anomalies using machine learning models. The function receives data from 10,000 sensors every 30 seconds and must identify anomalies within 5 minutes of data receipt.",
    question: "What architecture optimizes BOTH processing speed and anomaly detection accuracy?",
    options: [
        "Use Lambda with pre-loaded ML models and parallel processing by sensor type",
        "Implement real-time processing with Kinesis Analytics ML and Lambda outputs",
        "Use SageMaker endpoints with Lambda for real-time inference and caching",
        "Process data in batches with Lambda and store models in S3 for loading"
    ],
    correct: 0,
    explanation: {
        correct: "Pre-loaded models eliminate initialization time. Parallel processing by sensor type enables specialized anomaly detection for different device characteristics.",
        whyWrong: {
            1: "Kinesis Analytics ML has limited model flexibility",
            2: "SageMaker endpoints add latency and cost for simple anomaly detection",
            3: "S3 model loading adds latency for each invocation"
        },
        examStrategy: "Real-time ML inference: Pre-load models in Lambda + parallel processing by data characteristics."
    }
},
   // BATCH 2: 40 Domain 2 Questions - Security (24%)
// Add these to your existing questionBank array

{
    id: 'dva_127',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A multi-tenant SaaS application uses Lambda functions to process data for different customers. Each customer's data must be strictly isolated, and Lambda functions should only access resources for the specific customer making the request. The solution must scale to 500+ customers without managing separate functions.",
    question: "Which approach provides the STRONGEST security isolation while maintaining scalability?",
    options: [
        "Use Lambda environment variables with customer-specific IAM role ARNs and AssumeRole",
        "Implement customer ID validation in application code with shared IAM permissions",
        "Create separate Lambda functions for each customer with dedicated IAM roles",
        "Use AWS Organizations with separate accounts for each customer"
    ],
    correct: 0,
    explanation: {
        correct: "Environment variables store customer-specific role ARNs. AssumeRole provides temporary credentials with precise permissions for each customer, ensuring complete isolation.",
        whyWrong: {
            1: "Application-level validation can be bypassed and doesn't provide IAM-level isolation",
            2: "Separate functions per customer don't scale to 500+ customers efficiently",
            3: "Separate accounts add massive operational overhead and complexity"
        },
        examStrategy: "Multi-tenant isolation: Environment variables + AssumeRole for customer-specific permissions. Scales efficiently."
    }
},
{
    id: 'dva_128',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function processes sensitive customer data and stores results in S3. The function currently uses environment variables to store database credentials and S3 bucket names. A security audit identified this as a vulnerability.",
    question: "What is the MOST secure way to manage these sensitive configurations?",
    options: [
        "Use AWS Secrets Manager for credentials and Parameter Store for non-sensitive configs",
        "Encrypt environment variables using Lambda's built-in KMS encryption",
        "Use IAM roles for database access and Parameter Store SecureString for bucket names",
        "Store all configurations in a private S3 bucket with bucket policies"
    ],
    correct: 2,
    explanation: {
        correct: "IAM roles eliminate stored credentials entirely. Parameter Store SecureString encrypts configuration data at rest and provides audit logging.",
        whyWrong: {
            0: "Secrets Manager is good but IAM roles are more secure than any stored credential",
            1: "Encrypted environment variables are still visible to anyone with Lambda access",
            3: "S3 bucket storage still requires access credentials and adds complexity"
        },
        examStrategy: "Security hierarchy: IAM roles (no credentials) > Secrets Manager > Parameter Store SecureString > Environment variables."
    }
},
{
    id: 'dva_129',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "An API Gateway receives requests from web and mobile clients. The API must authenticate users, authorize access to specific resources based on user roles, and log all security events. The solution must support both JWT tokens from external identity providers and API keys for system-to-system communication.",
    question: "Which combination provides COMPREHENSIVE authentication and authorization?",
    options: [
        "Use Cognito User Pools with custom authorizer for API keys and CloudTrail for logging",
        "Implement Lambda authorizer with JWT validation, API key validation, and EventBridge for security events",
        "Use API Gateway built-in authentication with Cognito and usage plans for API keys",
        "Deploy multiple API Gateway instances with different authentication methods"
    ],
    correct: 1,
    explanation: {
        correct: "Lambda authorizer handles both JWT and API key validation with custom logic. EventBridge enables real-time security event processing and alerting.",
        whyWrong: {
            0: "Cognito User Pools don't handle API key authentication natively",
            2: "Built-in authentication lacks flexibility for mixed authentication requirements",
            3: "Multiple instances create management overhead without security benefits"
        },
        examStrategy: "Mixed authentication needs: Lambda authorizer for flexibility + EventBridge for security event processing."
    }
},
{
    id: 'dva_130',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A serverless application stores encrypted sensitive documents in S3. Different user groups need different levels of access: admins can read/write all documents, managers can read documents from their department, and employees can only read their own documents.",
    question: "What S3 security approach provides FINE-GRAINED access control?",
    options: [
        "Use S3 bucket policies with conditional statements based on user attributes",
        "Implement object-level permissions using S3 ACLs and IAM groups",
        "Use S3 Access Points with different policies for each user group",
        "Create separate S3 buckets for each department with IAM role access"
    ],
    correct: 0,
    explanation: {
        correct: "Bucket policies with conditions can evaluate user attributes (department, role) and object prefixes to provide precise access control at scale.",
        whyWrong: {
            1: "S3 ACLs are deprecated and don't provide the granularity needed",
            2: "Access Points are for different access patterns, not fine-grained permissions",
            3: "Separate buckets don't solve the cross-department access requirements"
        },
        examStrategy: "S3 fine-grained access: Bucket policies with conditional statements using user attributes and object prefixes."
    }
},
{
    id: 'dva_131',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A financial application processes transactions using Lambda and DynamoDB. All data must be encrypted at rest and in transit. The application must use different encryption keys for different data types (PII, financial data, audit logs) and support key rotation without downtime.",
    question: "Which encryption strategy provides MAXIMUM security with operational efficiency?",
    options: [
        "Use KMS with separate Customer Managed Keys (CMK) for each data type and automatic rotation",
        "Implement client-side encryption with application-managed keys stored in Secrets Manager",
        "Use DynamoDB encryption with AWS managed keys and Lambda environment variables for key IDs",
        "Deploy Hardware Security Modules (HSM) with CloudHSM for all encryption operations"
    ],
    correct: 0,
    explanation: {
        correct: "KMS CMKs provide separate keys per data type with automatic rotation, audit logging, and fine-grained access controls. No downtime during rotation.",
        whyWrong: {
            1: "Application-managed keys create key management complexity and security risks",
            2: "AWS managed keys don't allow separate keys per data type or custom rotation schedules",
            3: "CloudHSM is overkill and expensive for most applications, adds operational complexity"
        },
        examStrategy: "Multi-key encryption: KMS Customer Managed Keys per data type + automatic rotation for zero downtime."
    }
},
{
    id: 'dva_132',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function needs to access an RDS database in a private subnet. The function currently fails with connection timeout errors. The security team requires that database traffic never traverse the public internet.",
    question: "What configuration enables secure database connectivity?",
    options: [
        "Place Lambda in the same VPC and subnet as RDS with security group rules",
        "Use VPC endpoints for RDS and configure Lambda with VPC access",
        "Create a NAT Gateway for Lambda to access RDS through private routing",
        "Use AWS PrivateLink to establish secure connection between Lambda and RDS"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda in VPC can directly communicate with RDS in private subnets. Security groups control access precisely without internet routing.",
        whyWrong: {
            1: "VPC endpoints don't exist for RDS; they're for AWS API services",
            2: "NAT Gateway enables internet access, not direct VPC communication with RDS",
            3: "PrivateLink isn't needed for Lambda-to-RDS communication within the same VPC"
        },
        examStrategy: "Lambda-to-RDS private communication: Lambda in VPC + security group rules. No internet routing needed."
    }
},
{
    id: 'dva_133',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A containerized application runs on ECS and needs to access multiple AWS services (S3, DynamoDB, SQS, Parameter Store). Each container should have minimal required permissions, and credentials should never be stored in container images or environment variables.",
    question: "Which approach provides LEAST PRIVILEGE access with secure credential management?",
    options: [
        "Use ECS Task Roles with service-specific policies and IAM conditions for resource access",
        "Create a single IAM role with all required permissions and attach to ECS tasks",
        "Use AWS Secrets Manager to store IAM access keys and retrieve them at container startup",
        "Implement instance profiles on EC2 hosts with shared permissions for all containers"
    ],
    correct: 0,
    explanation: {
        correct: "ECS Task Roles provide temporary credentials per task. Service-specific policies with conditions ensure least privilege access without stored credentials.",
        whyWrong: {
            1: "Single role violates least privilege principle by granting excessive permissions",
            2: "Stored access keys create security risks and management overhead",
            3: "Instance profiles provide shared permissions, violating least privilege for individual containers"
        },
        examStrategy: "ECS security: Task Roles with service-specific policies + IAM conditions for least privilege per container."
    }
},
{
    id: 'dva_134',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A web application uses Cognito User Pools for authentication. Users report they can access other users' data by changing user IDs in API requests. The Lambda function validates JWT tokens but doesn't verify user authorization for specific resources.",
    question: "What is the MOST effective way to prevent unauthorized data access?",
    options: [
        "Implement resource-level authorization in Lambda using JWT claims and request validation",
        "Use Cognito Groups to manage user permissions and validate group membership",
        "Create separate API Gateway endpoints for each user with custom authorizers",
        "Store user permissions in DynamoDB and validate on every API call"
    ],
    correct: 0,
    explanation: {
        correct: "Resource-level authorization validates that the authenticated user (from JWT) can access the specific resource requested, preventing horizontal privilege escalation.",
        whyWrong: {
            1: "Cognito Groups help with coarse-grained permissions but don't solve resource-level authorization",
            2: "Separate endpoints per user don't scale and add complexity",
            3: "External permission storage adds latency and doesn't leverage JWT claims"
        },
        examStrategy: "Prevent data access across users: Resource-level authorization using JWT claims + request validation."
    }
},
{
    id: 'dva_135',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "A microservices architecture uses API Gateway and Lambda functions. Services need to communicate securely with each other, and each service should authenticate requests from other services. The solution must prevent replay attacks and ensure message integrity.",
    question: "Which inter-service authentication approach provides the STRONGEST security?",
    options: [
        "Use mutual TLS (mTLS) with service-specific certificates for authentication and encryption",
        "Implement JWT tokens with service identities and request signing using HMAC",
        "Use API Gateway API keys for service authentication with IP allowlisting",
        "Create VPC endpoints with security groups to restrict service-to-service communication"
    ],
    correct: 0,
    explanation: {
        correct: "mTLS provides mutual authentication, prevents man-in-the-middle attacks, ensures message integrity, and prevents replay attacks through certificate validation.",
        whyWrong: {
            1: "JWT with HMAC is good but doesn't prevent replay attacks without additional nonce handling",
            2: "API keys with IP allowlisting don't provide message integrity or replay protection",
            3: "VPC endpoints control network access but don't provide authentication or message integrity"
        },
        examStrategy: "Secure service-to-service communication: mTLS for mutual authentication + message integrity + replay protection."
    }
},
{
    id: 'dva_136',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function processes payment data and must comply with PCI DSS requirements. The function needs to log all processing activities for audit purposes while ensuring that sensitive payment information is never logged.",
    question: "What logging strategy ensures COMPLIANCE while maintaining audit requirements?",
    options: [
        "Use structured logging with data masking for sensitive fields and CloudWatch for audit trails",
        "Log all data to CloudWatch with encryption and restrict access using IAM policies",
        "Implement separate logging streams for sensitive and non-sensitive data",
        "Use CloudTrail for all logging and enable log file validation"
    ],
    correct: 0,
    explanation: {
        correct: "Structured logging allows precise control over what gets logged. Data masking ensures PCI compliance while maintaining audit trails for non-sensitive operations.",
        whyWrong: {
            1: "Logging sensitive payment data violates PCI DSS even if encrypted",
            2: "Separate streams still risk logging sensitive data and add complexity",
            3: "CloudTrail logs API calls, not application-level payment processing activities"
        },
        examStrategy: "PCI compliance logging: Structured logging + data masking for sensitive fields. Never log payment data."
    }
},
{
    id: 'dva_137',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A serverless application handles healthcare data (PHI) and must meet HIPAA compliance. Data is stored in DynamoDB and S3, processed by Lambda, and accessed through API Gateway. The solution must ensure end-to-end encryption, access logging, and data residency in specific regions.",
    question: "Which combination ensures FULL HIPAA compliance for the serverless architecture?",
    options: [
        "Enable encryption at rest/transit for all services, implement CloudTrail + VPC Flow Logs, use region-specific deployments",
        "Use KMS encryption with customer-managed keys, CloudWatch Logs, and S3 Cross-Region Replication",
        "Implement client-side encryption, API Gateway access logging, and multi-region deployment for availability",
        "Enable AWS Config for compliance monitoring, GuardDuty for threat detection, and CloudFormation for infrastructure"
    ],
    correct: 0,
    explanation: {
        correct: "Encryption at rest/transit protects data. CloudTrail logs all API access. VPC Flow Logs track network traffic. Region-specific deployment ensures data residency compliance.",
        whyWrong: {
            1: "Cross-Region Replication may violate data residency requirements for HIPAA",
            2: "Multi-region deployment conflicts with data residency requirements",
            3: "Config and GuardDuty help with monitoring but don't address core encryption and logging requirements"
        },
        examStrategy: "HIPAA serverless compliance: All services encrypted + CloudTrail + VPC Flow Logs + single-region deployment."
    }
},
{
    id: 'dva_138',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function connects to a third-party API that requires client certificates for authentication. The certificates contain private keys and must be rotated monthly. The security team prohibits storing certificates in Lambda deployment packages.",
    question: "What is the MOST secure approach for certificate management?",
    options: [
        "Store certificates in AWS Secrets Manager with automatic rotation and retrieve in Lambda",
        "Use Parameter Store SecureString to store certificates encrypted with KMS",
        "Store certificates in a private S3 bucket with server-side encryption",
        "Use Lambda environment variables with KMS encryption for certificate storage"
    ],
    correct: 0,
    explanation: {
        correct: "Secrets Manager is designed for certificate management with automatic rotation capabilities, versioning, and secure retrieval without hardcoding in deployment packages.",
        whyWrong: {
            1: "Parameter Store lacks built-in certificate rotation capabilities",
            2: "S3 storage requires additional logic for rotation and retrieval",
            3: "Environment variables expose certificates to anyone with Lambda access, even if encrypted"
        },
        examStrategy: "Certificate management: Secrets Manager for automatic rotation + secure retrieval. Built for certificates."
    }
},
{
    id: 'dva_139',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A financial trading application processes real-time market data using Kinesis Data Streams and Lambda. All data must be encrypted in transit and at rest. The application must detect and prevent unauthorized access attempts and provide real-time security alerting.",
    question: "Which security architecture provides COMPREHENSIVE protection with real-time threat detection?",
    options: [
        "Enable Kinesis server-side encryption, VPC endpoints, GuardDuty, and EventBridge for security events",
        "Use TLS for data streams, Lambda in VPC, CloudTrail logging, and SNS for alerting",
        "Implement client-side encryption, WAF for API protection, and CloudWatch alarms for monitoring",
        "Enable KMS encryption for all services, Security Hub for compliance, and Lambda authorizers"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis SSE encrypts data at rest. VPC endpoints encrypt transit. GuardDuty provides ML-based threat detection. EventBridge enables real-time security event processing.",
        whyWrong: {
            1: "CloudTrail logging is good but lacks real-time threat detection capabilities",
            2: "WAF doesn't apply to Kinesis streams, client-side encryption adds complexity",
            3: "Security Hub is for compliance dashboards, not real-time threat detection"
        },
        examStrategy: "Real-time security: Kinesis SSE + VPC endpoints + GuardDuty threat detection + EventBridge alerting."
    }
},
{
    id: 'dva_140',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A web application uses API Gateway with Lambda authorizer for authentication. During peak traffic, the authorizer experiences high latency due to database lookups for user validation. Caching is needed to improve performance while maintaining security.",
    question: "What caching strategy provides OPTIMAL performance without compromising security?",
    options: [
        "Enable API Gateway authorizer result caching with 5-minute TTL and cache key including all request context",
        "Use ElastiCache Redis to cache user validation results with 1-hour TTL",
        "Implement Lambda global variables to cache authorization decisions within execution context",
        "Cache authorization tokens in Lambda environment variables during deployment"
    ],
    correct: 0,
    explanation: {
        correct: "API Gateway authorizer caching is built for this purpose. 5-minute TTL balances performance and security. Cache key with request context prevents cross-user access.",
        whyWrong: {
            1: "1-hour TTL is too long for security-sensitive authorization decisions",
            2: "Global variables only work within single execution context, not across invocations",
            3: "Environment variables cache tokens across all users, creating major security vulnerability"
        },
        examStrategy: "Authorizer performance: API Gateway result caching with short TTL + request context in cache key."
    }
},
{
    id: 'dva_141',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "A multi-region application stores user data in DynamoDB Global Tables. Different regions have different data protection regulations (GDPR, CCPA, local laws). Users must have their data deleted from all regions upon request, and the application must prove compliance.",
    question: "Which approach ensures GLOBAL data deletion compliance with audit proof?",
    options: [
        "Implement coordinated deletion across all regions with DynamoDB Streams and compliance logging",
        "Use Step Functions to orchestrate deletion workflow across regions with execution history",
        "Create region-specific deletion Lambda functions with CloudTrail for audit logging",
        "Use DynamoDB Global Tables built-in deletion with AWS Config for compliance monitoring"
    ],
    correct: 1,
    explanation: {
        correct: "Step Functions provide orchestration across regions with built-in execution history, retry logic, and failure handling. Execution logs provide audit proof of deletion completion.",
        whyWrong: {
            0: "DynamoDB Streams approach lacks coordination guarantees and error handling across regions",
            2: "Region-specific functions lack coordination and may miss regions during failure scenarios",
            3: "Global Tables don't have built-in deletion workflows, and Config monitors resources, not data operations"
        },
        examStrategy: "Multi-region data deletion: Step Functions orchestration + execution history for audit proof + retry handling."
    }
},
{
    id: 'dva_142',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A serverless API processes user uploads to S3. The API must generate presigned URLs for uploads but restrict file types to images only and limit file size to 10MB. Users should not be able to upload executable files or oversized content.",
    question: "What combination provides SECURE upload restrictions with presigned URLs?",
    options: [
        "Generate presigned URLs with conditions for content-type and content-length limits",
        "Use Lambda to validate file headers after upload and delete invalid files",
        "Implement S3 bucket policies to restrict uploads based on file extensions",
        "Create separate S3 buckets for each allowed file type with different presigned URLs"
    ],
    correct: 0,
    explanation: {
        correct: "Presigned URL conditions enforce restrictions at upload time, preventing invalid uploads entirely. More secure and efficient than post-upload validation.",
        whyWrong: {
            1: "Post-upload validation allows temporary storage of malicious files",
            2: "File extensions can be spoofed; content-type validation is more reliable",
            3: "Separate buckets add complexity without security benefits over conditions"
        },
        examStrategy: "Secure S3 uploads: Presigned URLs with conditions for content-type + size limits. Prevent vs remediate."
    }
},
{
    id: 'dva_143',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A DevOps pipeline uses Lambda functions to deploy applications across multiple AWS accounts. Each deployment function needs temporary access to target accounts with specific permissions for that deployment only. The solution must prevent privilege escalation and ensure audit trails.",
    question: "Which cross-account access pattern provides MAXIMUM security for deployment automation?",
    options: [
        "Use cross-account IAM roles with external ID and assume role with deployment-specific policies",
        "Create IAM users in each target account with programmatic access and rotate keys after deployment",
        "Use AWS Organizations SCPs to control cross-account access with shared deployment roles",
        "Implement temporary credentials using STS GetSessionToken with MFA requirements"
    ],
    correct: 0,
    explanation: {
        correct: "Cross-account roles with external ID prevent confused deputy attacks. Deployment-specific policies ensure least privilege. AssumeRole provides temporary credentials with audit trails.",
        whyWrong: {
            1: "IAM users with access keys create permanent credentials and key management overhead",
            2: "SCPs control permissions but don't provide temporary credential mechanisms",
            3: "GetSessionToken doesn't work cross-account; designed for same-account temporary elevation"
        },
        examStrategy: "Cross-account deployment: AssumeRole with external ID + deployment-specific policies + audit trails."
    }
},
{
    id: 'dva_144',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function processes sensitive financial data and must ensure that all execution logs are encrypted and only accessible to security and compliance teams. Application logs contain transaction IDs that are not sensitive but execution context may contain sensitive data.",
    question: "What logging configuration provides APPROPRIATE data protection and access control?",
    options: [
        "Use CloudWatch Logs with KMS encryption and resource-based policies for team access",
        "Send all logs to S3 with server-side encryption and bucket policies for access control",
        "Implement custom logging with encrypted storage in DynamoDB and Lambda access control",
        "Use CloudTrail with log file encryption and separate logging for application vs system events"
    ],
    correct: 0,
    explanation: {
        correct: "CloudWatch Logs KMS encryption protects sensitive execution context. Resource-based policies provide fine-grained access control to security and compliance teams only.",
        whyWrong: {
            1: "S3 logging requires custom implementation and loses CloudWatch integration benefits",
            2: "Custom logging in DynamoDB adds complexity and operational overhead",
            3: "CloudTrail logs API calls, not Lambda application logs with sensitive execution context"
        },
        examStrategy: "Sensitive log protection: CloudWatch Logs + KMS encryption + resource-based policies for team access."
    }
},
{
    id: 'dva_145',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A real-time chat application uses API Gateway WebSocket API with Lambda for message processing. Messages must be encrypted end-to-end, users authenticated via JWT, and message history stored securely. The application must prevent message tampering and ensure message authenticity.",
    question: "Which architecture ensures END-TO-END security with message integrity?",
    options: [
        "Use TLS for transport encryption, JWT validation in Lambda authorizer, and message signing with user keys",
        "Implement client-side encryption with WebSocket TLS, Cognito authentication, and DynamoDB encryption",
        "Use API Gateway with mTLS, Lambda JWT validation, and S3 encryption for message storage",
        "Enable WebSocket encryption, custom authorizer with database validation, and encrypted ElastiCache storage"
    ],
    correct: 0,
    explanation: {
        correct: "TLS protects transport. JWT validation ensures authentication. Message signing with user keys provides end-to-end encryption and prevents tampering with authenticity verification.",
        whyWrong: {
            1: "Client-side encryption alone doesn't provide message signing for authenticity verification",
            2: "mTLS adds complexity for web clients and doesn't address end-to-end encryption requirements",
            3: "ElastiCache is in-memory and doesn't provide persistent secure storage for message history"
        },
        examStrategy: "End-to-end chat security: TLS transport + JWT auth + message signing for authenticity + integrity."
    }
},
{
    id: 'dva_146',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A mobile application uses Cognito User Pools for authentication and stores user preferences in DynamoDB. The app must ensure that users can only access and modify their own data. The DynamoDB table uses email as the partition key.",
    question: "What access control mechanism provides USER-LEVEL data isolation in DynamoDB?",
    options: [
        "Use IAM policy conditions with cognito-identity.amazonaws.com:sub to match the partition key",
        "Implement application-level validation in Lambda to check user email against request data",
        "Create separate DynamoDB tables for each user with user-specific IAM roles",
        "Use DynamoDB fine-grained access control with user-specific resource policies"
    ],
    correct: 0,
    explanation: {
        correct: "IAM policy conditions with Cognito identity can restrict DynamoDB access to items where the partition key matches the authenticated user's email, providing database-level isolation.",
        whyWrong: {
            1: "Application validation can be bypassed and doesn't provide database-level security",
            2: "Separate tables per user don't scale and create massive operational overhead",
            3: "DynamoDB doesn't support user-specific resource policies at the item level"
        },
        examStrategy: "DynamoDB user isolation: IAM policy conditions with Cognito identity matching partition key for database-level security."
    }
},
{
    id: 'dva_147',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "A financial services application must implement defense in depth across all layers. The application uses CloudFront, API Gateway, Lambda, and RDS. Each layer must have appropriate security controls, and the system must detect and respond to threats automatically.",
    question: "Which combination provides COMPREHENSIVE defense in depth with automated threat response?",
    options: [
        "CloudFront with WAF, API Gateway with authorizers, Lambda in VPC, RDS encryption, and GuardDuty with EventBridge automation",
        "CloudFront caching, API Gateway throttling, Lambda resource policies, RDS backup encryption, and CloudWatch alarms",
        "CloudFront with Shield, API Gateway logging, Lambda IAM roles, RDS Multi-AZ, and AWS Config compliance monitoring",
        "CloudFront with custom headers, API Gateway CORS, Lambda VPC endpoints, RDS security groups, and CloudTrail logging"
    ],
    correct: 0,
    explanation: {
        correct: "Each layer has appropriate security: WAF (application layer), authorizers (API), VPC (network), encryption (data), GuardDuty (threat detection), EventBridge (automated response).",
        whyWrong: {
            1: "Focuses on performance and backup rather than security controls at each layer",
            2: "Shield and Config provide monitoring but lack comprehensive security controls",
            3: "Custom headers and CORS don't provide meaningful security controls for financial services"
        },
        examStrategy: "Defense in depth: Layer-appropriate security controls + threat detection + automated response automation."
    }
},
{
    id: 'dva_148',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function processes webhook events from external partners. Each partner has different authentication requirements: some use HMAC signatures, others use bearer tokens, and some use mutual TLS certificates. The function must validate requests securely.",
    question: "What validation strategy handles MULTIPLE authentication methods securely?",
    options: [
        "Implement partner detection logic with method-specific validation and secure credential storage in Secrets Manager",
        "Use API Gateway with multiple custom authorizers for different partners",
        "Create separate Lambda functions for each partner with dedicated authentication logic",
        "Implement a single authentication method and require all partners to conform"
    ],
    correct: 0,
    explanation: {
        correct: "Single function with partner detection reduces operational overhead. Method-specific validation ensures security. Secrets Manager provides secure credential storage for all methods.",
        whyWrong: {
            1: "Multiple authorizers add complexity and latency for webhook processing",
            2: "Separate functions create management overhead and code duplication",
            3: "Forcing conformity may not be possible with external partners and reduces flexibility"
        },
        examStrategy: "Multi-partner webhook security: Partner detection + method-specific validation + Secrets Manager credentials."
    }
},
{
    id: 'dva_149',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A healthcare application processes patient data using Step Functions to orchestrate multiple Lambda functions. The workflow includes data validation, analysis, and storage steps. Each step must be audited, and patient data must never be logged in plain text. The solution must provide complete audit trails for compliance.",
    question: "Which approach ensures HIPAA-compliant auditing with complete workflow visibility?",
    options: [
        "Use Step Functions with CloudWatch Logs, implement data masking in all Lambda functions, and enable AWS CloudTrail",
        "Enable Step Functions Express workflows with X-Ray tracing and encrypted CloudWatch Logs",
        "Use Step Functions Standard with execution history, implement structured logging with PII masking, and enable VPC Flow Logs",
        "Create custom audit logging in DynamoDB with encrypted fields and Step Functions event logging"
    ],
    correct: 2,
    explanation: {
        correct: "Step Functions Standard provides detailed execution history. Structured logging with PII masking ensures compliance. VPC Flow Logs track all network activity for complete audit trails.",
        whyWrong: {
            0: "CloudWatch Logs from Step Functions may contain patient data in execution history",
            1: "Express workflows have limited execution history and X-Ray may expose sensitive data",
            3: "Custom audit logging adds complexity and may miss built-in AWS audit capabilities"
        },
        examStrategy: "HIPAA workflow auditing: Step Functions Standard execution history + structured logging + PII masking + VPC Flow Logs."
    }
},
{
    id: 'dva_150',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A serverless application uses S3 for file storage and Lambda for processing. Files contain sensitive customer data and must be encrypted. The encryption keys must be rotated quarterly, and different customer data must use different encryption keys for isolation.",
    question: "What key management strategy provides CUSTOMER-LEVEL encryption isolation with rotation?",
    options: [
        "Use KMS Customer Managed Keys with customer ID in key policy and automatic rotation",
        "Create separate KMS keys for each customer with manual quarterly rotation",
        "Use S3 server-side encryption with customer-provided keys (SSE-C) stored in Secrets Manager",
        "Implement client-side encryption with customer-specific keys managed in Parameter Store"
    ],
    correct: 0,
    explanation: {
        correct: "KMS CMKs allow customer ID in key policies for access control. Automatic rotation eliminates operational overhead while maintaining quarterly schedule.",
        whyWrong: {
            1: "Manual rotation creates operational overhead and risk of missed rotations",
            2: "SSE-C requires application to manage and provide keys for every operation",
            3: "Client-side encryption adds complexity and Parameter Store lacks key rotation capabilities"
        },
        examStrategy: "Customer encryption isolation: KMS CMKs with customer ID in policies + automatic quarterly rotation."
    }
},
{
    id: 'dva_151',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A distributed application spans multiple AWS accounts and uses EventBridge for cross-account communication. Events contain sensitive business data and must be encrypted in transit and at rest. The solution must prevent unauthorized cross-account access and provide audit trails.",
    question: "Which EventBridge security configuration provides MAXIMUM protection for cross-account events?",
    options: [
        "Use EventBridge with KMS encryption, cross-account resource policies with conditions, and CloudTrail for audit logging",
        "Implement custom encryption in event payloads with EventBridge custom bus and IAM cross-account roles",
        "Use EventBridge with SSL/TLS encryption and API Gateway for cross-account event routing",
        "Enable EventBridge schema registry with validation and S3 for event archival with encryption"
    ],
    correct: 0,
    explanation: {
        correct: "KMS encryption protects data at rest. Resource policies with conditions control cross-account access precisely. CloudTrail provides complete audit trails of all event operations.",
        whyWrong: {
            1: "Custom encryption adds complexity and EventBridge already supports native KMS encryption",
            2: "API Gateway routing adds unnecessary complexity and latency for event-driven architecture",
            3: "Schema registry helps with validation but doesn't address encryption and access control requirements"
        },
        examStrategy: "Cross-account EventBridge security: KMS encryption + resource policies with conditions + CloudTrail auditing."
    }
},
{
    id: 'dva_152',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function processes user authentication requests and must detect suspicious login patterns (multiple failed attempts, unusual locations, impossible travel). The function needs to make real-time decisions to block or allow logins while maintaining user experience.",
    question: "What approach provides EFFECTIVE fraud detection with minimal false positives?",
    options: [
        "Implement rule-based detection with IP geolocation and time-based analysis in Lambda with DynamoDB tracking",
        "Use Amazon Fraud Detector with custom models and real-time scoring integrated with Lambda",
        "Create custom machine learning models in SageMaker with batch inference for login analysis",
        "Use GuardDuty findings with EventBridge to trigger Lambda functions for login blocking"
    ],
    correct: 1,
    explanation: {
        correct: "Amazon Fraud Detector is purpose-built for fraud detection with machine learning models that reduce false positives compared to rule-based systems. Real-time scoring fits authentication use case.",
        whyWrong: {
            0: "Rule-based systems have high false positive rates and don't adapt to new attack patterns",
            2: "Batch inference doesn't support real-time authentication decisions",
            3: "GuardDuty detects infrastructure threats, not application-level authentication fraud"
        },
        examStrategy: "Authentication fraud detection: Amazon Fraud Detector for ML-based real-time scoring with low false positives."
    }
},
{
    id: 'dva_153',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "A compliance-critical application must ensure that all API requests are authenticated, authorized, logged, and immutable. The application handles financial transactions and must provide cryptographic proof that logs haven't been tampered with for regulatory audits.",
    question: "Which logging architecture provides TAMPER-PROOF audit trails with cryptographic integrity?",
    options: [
        "Use CloudTrail with log file validation, S3 Object Lock, and CloudWatch Logs with KMS encryption",
        "Implement custom logging with blockchain-based integrity verification and encrypted storage",
        "Use AWS CloudHSM to sign all log entries with hardware-based cryptographic verification",
        "Enable GuardDuty with findings export to S3 and implement hash-based integrity checking"
    ],
    correct: 0,
    explanation: {
        correct: "CloudTrail log file validation provides cryptographic integrity verification. S3 Object Lock prevents deletion/modification. KMS encryption protects against unauthorized access.",
        whyWrong: {
            1: "Custom blockchain implementation adds enormous complexity and operational overhead",
            2: "CloudHSM for every log entry is expensive and operationally complex",
            3: "GuardDuty is for threat detection, not comprehensive application audit logging"
        },
        examStrategy: "Tamper-proof audit logs: CloudTrail log file validation + S3 Object Lock + KMS encryption for integrity."
    }
},
{
    id: 'dva_154',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A mobile application uploads user photos to S3 through presigned URLs. The application must ensure uploaded images are safe (no malware), appropriately sized, and don't contain inappropriate content before making them available to other users.",
    question: "What validation approach provides COMPREHENSIVE upload security scanning?",
    options: [
        "Use S3 Event Notifications to trigger Lambda for virus scanning, image validation, and content moderation with Rekognition",
        "Implement client-side validation with server-side verification using Lambda and CloudWatch monitoring",
        "Use S3 Access Points with Lambda validation and manual content review processes",
        "Enable S3 server access logging with automated scanning using third-party security tools"
    ],
    correct: 0,
    explanation: {
        correct: "S3 events trigger immediate processing. Lambda can perform virus scanning, validate image properties, and use Rekognition for content moderation automatically.",
        whyWrong: {
            1: "Client-side validation can be bypassed and doesn't provide server-side security",
            2: "Access Points don't provide content scanning capabilities and manual review doesn't scale",
            3: "Server access logs track access patterns, not file content validation"
        },
        examStrategy: "Upload security validation: S3 events → Lambda virus scanning + image validation + Rekognition content moderation."
    }
},
{
    id: 'dva_155',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A multi-tenant application serves enterprise customers who require complete data isolation and separate encryption keys. Each tenant's data must be processed in isolated execution environments with no possibility of cross-tenant data access. The solution must scale to 1000+ tenants efficiently.",
    question: "Which architecture provides COMPLETE tenant isolation with scalable key management?",
    options: [
        "Use separate AWS accounts per tenant with account-level isolation and tenant-specific KMS keys",
        "Implement tenant-specific Lambda execution roles with KMS key conditions and DynamoDB encryption context",
        "Deploy separate VPCs per tenant with dedicated Lambda functions and tenant-specific encryption",
        "Use AWS Organizations with SCPs for tenant isolation and centralized KMS key management"
    ],
    correct: 1,
    explanation: {
        correct: "Lambda execution roles with KMS conditions provide compute isolation. DynamoDB encryption context ensures data-level isolation. Scales efficiently without separate accounts.",
        whyWrong: {
            0: "Separate accounts provide maximum isolation but don't scale operationally to 1000+ tenants",
            2: "Separate VPCs and functions create massive operational overhead without additional security benefits",
            3: "SCPs control permissions but don't provide the execution isolation required for complete tenant separation"
        },
        examStrategy: "Scalable tenant isolation: Lambda execution roles + KMS key conditions + DynamoDB encryption context for 1000+ tenants."
    }
},
{
    id: 'dva_156',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A serverless application processes payment webhooks from multiple payment providers. Each provider sends webhooks with different signature methods for verification. The application must verify webhook authenticity and prevent replay attacks.",
    question: "What webhook verification strategy ensures AUTHENTICITY and prevents replay attacks?",
    options: [
        "Implement provider-specific signature validation with timestamp checking and nonce tracking in DynamoDB",
        "Use API Gateway with custom authorizers for each payment provider and IP allowlisting",
        "Validate webhook signatures in Lambda with Redis caching for performance optimization",
        "Create separate API endpoints for each provider with shared signature validation logic"
    ],
    correct: 0,
    explanation: {
        correct: "Provider-specific validation handles different signature methods. Timestamp checking prevents old webhook replay. Nonce tracking in DynamoDB prevents duplicate webhook processing.",
        whyWrong: {
            1: "IP allowlisting doesn't verify message authenticity and custom authorizers add latency",
            2: "Redis caching alone doesn't prevent replay attacks without timestamp and nonce validation",
            3: "Separate endpoints don't address replay attack prevention or signature validation requirements"
        },
        examStrategy: "Webhook security: Provider-specific signature validation + timestamp checking + nonce tracking for replay prevention."
    }
},
{
    id: 'dva_157',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A financial application must implement zero-trust security principles across all components. The application uses microservices architecture with API Gateway, Lambda, DynamoDB, and external service integrations. Every service interaction must be authenticated and authorized.",
    question: "Which zero-trust implementation provides COMPREHENSIVE security with service-level authentication?",
    options: [
        "Use service mesh with mTLS, API Gateway with JWT validation, IAM roles for every service, and request signing for external APIs",
        "Implement VPC security groups, Lambda authorizers, database encryption, and API key authentication",
        "Use AWS WAF for application protection, Cognito for user authentication, and encrypted service communication",
        "Deploy services in private subnets with NAT gateways, enable CloudTrail logging, and use KMS encryption"
    ],
    correct: 0,
    explanation: {
        correct: "Service mesh mTLS provides service-to-service authentication. JWT validation for API access. IAM roles for AWS service access. Request signing for external services. Complete zero-trust coverage.",
        whyWrong: {
            1: "Security groups provide network-level control but don't implement zero-trust service authentication",
            2: "WAF and Cognito focus on perimeter security, not internal service-to-service zero-trust",
            3: "Network isolation doesn't implement zero-trust principles of service-level authentication"
        },
        examStrategy: "Zero-trust architecture: Service mesh mTLS + JWT API validation + IAM roles + external API signing for complete coverage."
    }
},
{
    id: 'dva_158',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function stores sensitive configuration data including database credentials, API keys, and encryption keys. The function is deployed across multiple environments (dev, staging, production) with different security requirements for each environment.",
    question: "What configuration management approach provides ENVIRONMENT-SPECIFIC security with operational efficiency?",
    options: [
        "Use Parameter Store hierarchical parameters with environment prefixes and IAM policy conditions for access control",
        "Create separate Secrets Manager entries for each environment with environment-specific IAM roles",
        "Implement environment variables with KMS encryption using different keys per environment",
        "Store configurations in S3 with environment-specific buckets and Lambda execution role access"
    ],
    correct: 0,
    explanation: {
        correct: "Parameter Store hierarchy (/dev/, /staging/, /prod/) enables organized configuration. IAM conditions can restrict access based on environment. Cost-effective and operationally efficient.",
        whyWrong: {
            1: "Secrets Manager is more expensive and complex for simple environment-based configuration management",
            2: "Environment variables expose sensitive data to anyone with Lambda access regardless of encryption",
            3: "S3 configuration storage adds complexity and latency compared to Parameter Store"
        },
        examStrategy: "Environment-specific config security: Parameter Store hierarchy + IAM policy conditions for environment access control."
    }
},
{
    id: 'dva_159',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "A real-time analytics application processes sensitive customer behavior data using Kinesis Data Streams. The data must be encrypted in transit and at rest, with different encryption keys for different customer segments. The solution must support real-time processing while maintaining encryption key isolation.",
    question: "Which encryption strategy provides CUSTOMER-SEGMENT isolation with real-time processing capability?",
    options: [
        "Use Kinesis server-side encryption with KMS, customer segment routing to different shards, and Lambda with segment-specific IAM roles",
        "Implement client-side encryption with segment-specific keys before sending to Kinesis streams",
        "Use separate Kinesis streams for each customer segment with different KMS keys and shared Lambda processing",
        "Enable Kinesis encryption with customer-managed keys and implement segment detection in Lambda consumers"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis SSE with KMS provides encryption at rest. Customer segment routing enables key isolation per shard. Lambda roles with segment-specific permissions ensure access control.",
        whyWrong: {
            1: "Client-side encryption adds complexity and doesn't leverage Kinesis native encryption capabilities",
            2: "Separate streams create operational overhead and don't scale efficiently for multiple segments",
            3: "Single KMS key doesn't provide the isolation required between customer segments"
        },
        examStrategy: "Kinesis segment encryption: SSE with KMS + segment routing to shards + Lambda roles for key isolation."
    }
},
{
    id: 'dva_160',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A serverless application handles user registration and login. The application must implement account lockout after failed login attempts, send security notifications for suspicious activities, and allow users to reset their security status. All security events must be logged for compliance.",
    question: "What security event handling approach provides COMPREHENSIVE user account protection?",
    options: [
        "Use Cognito with advanced security features, Lambda triggers for custom logic, and EventBridge for security event routing",
        "Implement custom authentication with DynamoDB user tracking and SNS for security notifications",
        "Use API Gateway with rate limiting and CloudWatch alarms for failed authentication monitoring",
        "Deploy custom Lambda authorizers with Redis for session tracking and SQS for security event processing"
    ],
    correct: 0,
    explanation: {
        correct: "Cognito advanced security provides built-in account lockout and risk detection. Lambda triggers enable custom security logic. EventBridge routes security events for comprehensive handling.",
        whyWrong: {
            1: "Custom authentication implementation increases security risk and operational complexity",
            2: "Rate limiting doesn't provide account-specific lockout and lacks user-level security features",
            3: "Custom authorizers with Redis require significant security implementation and maintenance"
        },
        examStrategy: "User account security: Cognito advanced security + Lambda triggers + EventBridge for comprehensive protection."
    }
},
{
    id: 'dva_161',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A healthcare application processes medical images using Lambda and stores them in S3. Images must be encrypted with patient-specific keys, access must be logged and audited, and the application must support emergency access procedures for life-threatening situations while maintaining HIPAA compliance.",
    question: "Which architecture balances EMERGENCY access requirements with strict HIPAA compliance?",
    options: [
        "Use S3 with KMS patient-specific keys, break-glass IAM roles for emergencies, comprehensive CloudTrail logging, and automated compliance monitoring",
        "Implement client-side encryption with patient keys stored in HSM and emergency key escrow procedures",
        "Use S3 server-side encryption with emergency access through root account and detailed access logging",
        "Deploy encrypted EFS with patient directories and emergency access through temporary credential elevation"
    ],
    correct: 0,
    explanation: {
        correct: "Patient-specific KMS keys ensure data isolation. Break-glass roles provide controlled emergency access with full audit trails. CloudTrail logging ensures HIPAA compliance with automated monitoring.",
        whyWrong: {
            1: "HSM and key escrow add enormous complexity and operational overhead for emergency scenarios",
            2: "Root account access violates least privilege principles and creates security risks",
            3: "EFS doesn't provide the granular encryption and access control needed for patient-specific requirements"
        },
        examStrategy: "HIPAA emergency access: Patient-specific KMS keys + break-glass IAM roles + CloudTrail auditing + compliance monitoring."
    }
},
{
    id: 'dva_162',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function integrates with multiple third-party APIs that require different authentication methods: OAuth 2.0, API keys, and HMAC signatures. The function must securely store and manage these credentials while supporting credential rotation.",
    question: "What credential management strategy handles MULTIPLE authentication methods with rotation support?",
    options: [
        "Use Secrets Manager with JSON secrets containing method-specific credentials and automatic rotation where supported",
        "Store credentials in Parameter Store SecureString with manual rotation procedures for each method",
        "Implement credential storage in encrypted S3 with Lambda-based rotation logic",
        "Use environment variables with KMS encryption and deployment-based credential updates"
    ],
    correct: 0,
    explanation: {
        correct: "Secrets Manager JSON format can store multiple credential types per secret. Automatic rotation for OAuth tokens. Manual rotation support for API keys and HMAC secrets. Built for multi-method credential management.",
        whyWrong: {
            1: "Parameter Store lacks automatic rotation capabilities for OAuth tokens",
            2: "S3 credential storage adds complexity and doesn't provide built-in rotation features",
            3: "Environment variables expose credentials to Lambda console access and require redeployment for rotation"
        },
        examStrategy: "Multi-method credential management: Secrets Manager JSON secrets + automatic/manual rotation support per method."
    }
},
{
    id: 'dva_163',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A financial trading platform requires end-to-end encryption for all trading data, real-time threat detection, automated response to security incidents, and regulatory compliance reporting. The platform handles millions of transactions per day across multiple regions.",
    question: "Which comprehensive security architecture meets ALL financial trading security requirements?",
    options: [
        "Implement KMS encryption, GuardDuty with EventBridge automation, Security Hub for compliance, and CloudTrail across all regions",
        "Use CloudHSM for encryption, custom threat detection with Lambda, manual incident response, and quarterly compliance reports",
        "Deploy WAF protection, Cognito authentication, encrypted data storage, and monthly security reviews",
        "Enable VPC security, IAM roles, data encryption, and annual compliance audits"
    ],
    correct: 0,
    explanation: {
        correct: "KMS provides scalable encryption for millions of transactions. GuardDuty ML threat detection with EventBridge automation. Security Hub centralizes compliance. Multi-region CloudTrail ensures complete audit coverage.",
        whyWrong: {
            1: "CloudHSM is expensive at scale, custom detection lacks ML capabilities, manual response doesn't scale",
            2: "WAF and Cognito don't address end-to-end encryption and threat detection requirements",
            3: "Basic security controls don't meet comprehensive financial trading security requirements"
        },
        examStrategy: "Financial trading security: KMS encryption + GuardDuty ML detection + EventBridge automation + Security Hub compliance."
    }
},
{
    id: 'dva_164',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A serverless application uses Step Functions to orchestrate data processing workflows. The workflows handle sensitive customer data and must ensure that data is encrypted at each step, access is logged, and failed executions don't expose sensitive data in error messages.",
    question: "What Step Functions security configuration protects SENSITIVE data throughout workflow execution?",
    options: [
        "Enable Step Functions encryption at rest, use Lambda functions with KMS encryption, implement error handling with data masking, and enable CloudWatch Logs encryption",
        "Use Step Functions Express workflows with X-Ray tracing and encrypted CloudWatch Logs",
        "Implement client-side encryption in workflow input with custom error handling and S3 logging",
        "Enable VPC endpoints for Step Functions with security groups and encrypted EBS volumes"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions encryption protects workflow state. Lambda KMS encryption protects processing. Error handling with data masking prevents exposure. CloudWatch encryption protects logs.",
        whyWrong: {
            1: "Express workflows have limited logging and X-Ray may expose sensitive data in traces",
            2: "Client-side encryption in workflow input doesn't protect intermediate processing steps",
            3: "VPC endpoints and EBS don't address Step Functions workflow data protection requirements"
        },
        examStrategy: "Step Functions data protection: Workflow encryption + Lambda KMS + error masking + encrypted CloudWatch Logs."
    }
},
{
    id: 'dva_165',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "A global application uses CloudFront for content delivery and must implement security controls for different geographic regions due to varying data protection laws. Some regions require data to never leave specific geographic boundaries, while others allow global distribution with encryption.",
    question: "Which CloudFront security configuration provides REGION-SPECIFIC data protection compliance?",
    options: [
        "Use CloudFront with geographic restrictions, Lambda@Edge for region-based logic, and different origin configurations per region",
        "Implement multiple CloudFront distributions with region-specific behaviors and separate S3 origins",
        "Use CloudFront with WAF geo-blocking and single global origin with encrypted content",
        "Deploy regional CloudFront distributions with VPC origins and region-specific security groups"
    ],
    correct: 0,
    explanation: {
        correct: "Geographic restrictions control access by region. Lambda@Edge enables region-specific processing logic. Different origins ensure data stays in compliant regions.",
        whyWrong: {
            1: "Multiple distributions create management overhead without providing region-specific processing logic",
            2: "WAF geo-blocking prevents access but doesn't ensure data residency compliance",
            3: "CloudFront doesn't support VPC origins directly, and security groups don't address geographic compliance"
        },
        examStrategy: "Regional data compliance: CloudFront geo-restrictions + Lambda@Edge region logic + region-specific origins."
    }
},
{
    id: 'dva_166',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function processes user uploads and must scan for malware, validate file types, and check content for inappropriate material. The function must handle various file formats (images, documents, videos) and provide detailed security reports.",
    question: "What comprehensive file scanning approach provides MULTI-FORMAT security validation?",
    options: [
        "Use Lambda with ClamAV for malware scanning, file type validation libraries, and Rekognition for image/video content moderation",
        "Implement S3 server-side virus scanning with custom Lambda validation and manual content review",
        "Use third-party security APIs for all scanning with Lambda orchestration and result aggregation",
        "Deploy EC2 instances with comprehensive security software and Lambda triggers for processing"
    ],
    correct: 0,
    explanation: {
        correct: "ClamAV provides open-source malware scanning in Lambda. File type libraries validate formats. Rekognition handles image/video content moderation. Cost-effective comprehensive solution.",
        whyWrong: {
            1: "S3 doesn't provide built-in virus scanning and manual review doesn't scale",
            2: "Third-party APIs for all scanning create vendor dependency and increase costs",
            3: "EC2 instances add infrastructure overhead compared to serverless Lambda approach"
        },
        examStrategy: "Comprehensive file scanning: Lambda + ClamAV malware scanning + file validation + Rekognition content moderation."
    }
},

    // BATCH 3: 38 Questions - Domain 3: Deployment (23%) & Domain 4: Troubleshooting (21%)
// Add these to your existing questionBank array

// DOMAIN 3: DEPLOYMENT (20 questions)

{
    id: 'dva_167',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A serverless application uses CodePipeline for CI/CD with multiple environments (dev, staging, prod). The pipeline must deploy Lambda functions with different configurations per environment, run automated tests, and support rollback capabilities. Deployments to production require manual approval.",
    question: "Which CodePipeline configuration provides COMPREHENSIVE deployment automation with environment-specific controls?",
    options: [
        "Use CodeBuild for multi-environment builds, Lambda deployment with environment variables, CodeDeploy for blue/green deployments, and manual approval actions",
        "Implement separate pipelines per environment with environment-specific CodeBuild projects and automated testing",
        "Use single pipeline with environment parameters, conditional deployments, and CloudFormation for infrastructure",
        "Create environment-specific repositories with branch-based triggering and separate deployment processes"
    ],
    correct: 0,
    explanation: {
        correct: "CodeBuild handles environment-specific builds and testing. Lambda environment variables provide configuration flexibility. CodeDeploy blue/green enables safe deployments with rollback. Manual approval ensures production control.",
        whyWrong: {
            1: "Separate pipelines create management overhead and don't share common build artifacts",
            2: "Single pipeline with parameters lacks the safety controls needed for production deployments",
            3: "Separate repositories duplicate code and create maintenance overhead"
        },
        examStrategy: "Multi-environment CI/CD: CodeBuild + Lambda configs + CodeDeploy blue/green + manual approval for production."
    }
},
{
    id: 'dva_168',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function deployment occasionally fails with 'CodeStorageLimitExceededException' errors. The deployment package is 45MB and includes multiple dependencies. The function needs to be deployed across multiple regions for disaster recovery.",
    question: "What deployment strategy resolves storage limitations while enabling multi-region deployment?",
    options: [
        "Use Lambda Layers to share dependencies and S3 cross-region replication for deployment packages",
        "Compress deployment packages more aggressively and use CloudFormation StackSets for multi-region deployment",
        "Split the function into smaller microservices and deploy each independently",
        "Use container images instead of ZIP packages and Amazon ECR for multi-region image distribution"
    ],
    correct: 3,
    explanation: {
        correct: "Container images support up to 10GB vs 250MB limit for ZIP. ECR provides native multi-region replication for container images, solving both size and distribution challenges.",
        whyWrong: {
            0: "Layers help but still have size limits; S3 replication doesn't solve the fundamental storage limit",
            1: "Compression has limits and doesn't address the fundamental 250MB ZIP package restriction",
            2: "Splitting functions changes architecture unnecessarily when container images solve the problem"
        },
        examStrategy: "Large Lambda deployments: Container images (10GB limit) + ECR multi-region replication vs ZIP (250MB limit)."
    }
},
{
    id: 'dva_169',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "A microservices application uses ECS Fargate with Application Load Balancer. The deployment process must support zero-downtime deployments, automatic rollback on health check failures, and gradual traffic shifting to new versions. The application serves critical financial transactions.",
    question: "Which ECS deployment strategy provides ZERO-DOWNTIME deployments with automatic rollback for critical applications?",
    options: [
        "Use ECS rolling deployments with health checks and CloudWatch alarms for automatic rollback",
        "Implement blue/green deployments with CodeDeploy, target group switching, and automated rollback triggers",
        "Use ECS service auto-scaling with gradual capacity increases and manual validation",
        "Deploy new task definitions with immediate replacement and load balancer drain connections"
    ],
    correct: 1,
    explanation: {
        correct: "CodeDeploy blue/green creates separate target groups for new version. Traffic shifts gradually with health monitoring. Automatic rollback on failed health checks ensures reliability for critical applications.",
        whyWrong: {
            0: "Rolling deployments may cause service disruption during task replacement",
            2: "Auto-scaling doesn't provide deployment safety controls and manual validation doesn't scale",
            3: "Immediate replacement risks downtime and lacks gradual traffic shifting safety"
        },
        examStrategy: "Critical ECS deployments: CodeDeploy blue/green + target group switching + automated rollback on health failures."
    }
},
{
    id: 'dva_170',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A CloudFormation stack manages Lambda functions, API Gateway, and DynamoDB tables. Updates to Lambda function code should deploy quickly without affecting infrastructure, but infrastructure changes need careful validation and approval.",
    question: "What CloudFormation strategy separates APPLICATION deployments from INFRASTRUCTURE changes?",
    options: [
        "Use nested stacks to separate application code from infrastructure with independent update processes",
        "Implement separate CloudFormation stacks for infrastructure and application components",
        "Use CloudFormation transforms to handle Lambda code updates differently from infrastructure",
        "Create parameterized templates with conditional logic for application vs infrastructure updates"
    ],
    correct: 1,
    explanation: {
        correct: "Separate stacks allow independent deployment cycles. Infrastructure stack changes infrequently with approval processes. Application stack deploys Lambda code frequently without infrastructure risk.",
        whyWrong: {
            0: "Nested stacks still couple infrastructure and application deployment timing",
            2: "Transforms don't provide separate deployment lifecycle management",
            3: "Conditional logic in single template doesn't achieve deployment separation"
        },
        examStrategy: "CloudFormation separation: Separate stacks for infrastructure vs application for independent deployment cycles."
    }
},
{
    id: 'dva_171',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A containerized application runs on EKS and uses Helm charts for deployment. The application must support canary deployments with traffic splitting, automated performance testing, and rollback based on custom metrics (error rate, latency). Different services have different rollout requirements.",
    question: "Which Kubernetes deployment approach provides ADVANCED canary capabilities with custom metrics?",
    options: [
        "Use Kubernetes native deployments with Ingress controllers for traffic splitting and custom HPA metrics",
        "Implement Flagger with Istio service mesh for automated canary analysis and custom metric evaluation",
        "Use AWS Load Balancer Controller with target group weighting and CloudWatch custom metrics",
        "Deploy with ArgoCD rollouts and NGINX ingress for traffic management with Prometheus metrics"
    ],
    correct: 1,
    explanation: {
        correct: "Flagger automates canary deployments with sophisticated traffic splitting. Istio service mesh provides advanced traffic management. Custom metric evaluation enables business-specific rollback criteria.",
        whyWrong: {
            0: "Native Kubernetes deployments lack advanced canary automation and custom metric integration",
            2: "AWS Load Balancer Controller is limited compared to service mesh capabilities for canary deployments",
            3: "ArgoCD rollouts are good but lack the automated analysis capabilities of Flagger with Istio"
        },
        examStrategy: "Advanced Kubernetes canary: Flagger + Istio service mesh + custom metrics for automated canary analysis."
    }
},
{
    id: 'dva_172',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function uses environment variables for configuration that changes frequently. Updating environment variables requires function redeployment, causing downtime. The solution must enable configuration updates without redeployment.",
    question: "What configuration approach enables DYNAMIC updates without Lambda redeployment?",
    options: [
        "Use AWS Parameter Store with Lambda function code that retrieves parameters at runtime",
        "Implement S3-based configuration files with Lambda polling for changes",
        "Use DynamoDB configuration table with Lambda reading values on each invocation",
        "Store configuration in Lambda Layers and update layers independently"
    ],
    correct: 0,
    explanation: {
        correct: "Parameter Store allows runtime configuration retrieval without redeployment. Parameters can be updated independently. Lambda code fetches current values during execution.",
        whyWrong: {
            1: "S3 polling adds complexity and latency compared to Parameter Store integration",
            2: "DynamoDB configuration adds unnecessary cost and latency for simple configuration values",
            3: "Lambda Layers still require layer updates and function configuration changes"
        },
        examStrategy: "Dynamic Lambda config: Parameter Store runtime retrieval vs environment variables (requires redeployment)."
    }
},
{
    id: 'dva_173',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A multi-account AWS organization deploys applications using CloudFormation across dev, staging, and production accounts. Each account has different compliance requirements, and deployments must be coordinated across accounts with approval workflows.",
    question: "Which multi-account deployment strategy provides COORDINATED deployments with account-specific compliance?",
    options: [
        "Use CloudFormation StackSets with organizational units and service-managed permissions for cross-account deployment",
        "Implement CodePipeline with cross-account roles and account-specific CloudFormation templates",
        "Use AWS Control Tower with Account Factory and customized deployment pipelines per account",
        "Deploy through centralized DevOps account with assume role access to target accounts"
    ],
    correct: 1,
    explanation: {
        correct: "CodePipeline orchestrates cross-account deployments with approval workflows. Cross-account roles provide secure access. Account-specific templates handle compliance differences.",
        whyWrong: {
            0: "StackSets are good for identical deployments but lack account-specific customization and approval workflows",
            2: "Control Tower focuses on account setup, not application deployment orchestration",
            3: "Centralized deployment lacks the account-specific compliance controls and approval workflows needed"
        },
        examStrategy: "Multi-account deployments: CodePipeline + cross-account roles + account-specific templates + approval workflows."
    }
},
{
    id: 'dva_174',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A serverless application deployment includes Lambda functions, API Gateway, DynamoDB tables, and IAM roles. The deployment process occasionally fails due to IAM role creation timing issues and DynamoDB table dependencies.",
    question: "What CloudFormation approach resolves DEPENDENCY and TIMING issues in serverless deployments?",
    options: [
        "Use CloudFormation DependsOn attributes and custom resources for deployment ordering",
        "Implement AWS SAM with automatic dependency resolution and deployment optimization",
        "Use CloudFormation conditions to handle optional resources and reduce dependencies",
        "Deploy components in separate CloudFormation stacks with manual dependency management"
    ],
    correct: 1,
    explanation: {
        correct: "AWS SAM automatically handles serverless resource dependencies and deployment ordering. SAM transforms resolve IAM role and resource timing issues common in serverless deployments.",
        whyWrong: {
            0: "Manual DependsOn attributes are error-prone and SAM handles this automatically",
            2: "Conditions don't solve timing and dependency issues between required resources",
            3: "Separate stacks create management overhead without solving the dependency timing issues"
        },
        examStrategy: "Serverless deployment dependencies: AWS SAM automatic resolution vs manual CloudFormation DependsOn."
    }
},
{
    id: 'dva_175',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "A Python application uses CodeBuild for CI/CD with multiple dependencies and testing requirements. Builds are slow (15+ minutes) and inconsistent due to dependency installation. The solution must improve build speed and reliability.",
    question: "Which CodeBuild optimization provides FASTEST and most RELIABLE builds?",
    options: [
        "Use custom Docker images with pre-installed dependencies and CodeBuild build caching",
        "Implement parallel build phases with dependency caching and optimized buildspec configuration",
        "Use CodeBuild batch builds with multiple concurrent build processes",
        "Pre-package dependencies in S3 and download during build process"
    ],
    correct: 0,
    explanation: {
        correct: "Custom Docker images eliminate dependency installation time. CodeBuild caching stores build artifacts. Combination provides fastest, most reliable builds with consistent environments.",
        whyWrong: {
            1: "Parallel phases help but don't eliminate dependency installation overhead",
            2: "Batch builds provide parallel execution but don't solve dependency installation time",
            3: "S3 dependency packages still require download and installation time"
        },
        examStrategy: "CodeBuild optimization: Custom Docker images + build caching for speed + reliability + consistent environments."
    }
},
{
    id: 'dva_176',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A microservices application uses AWS CDK for infrastructure deployment. Different teams manage different services but share common infrastructure components (VPC, security groups, databases). Changes must be coordinated without blocking individual team deployments.",
    question: "What CDK architecture enables SHARED infrastructure with independent team deployments?",
    options: [
        "Use CDK constructs library with shared infrastructure stack and application-specific stacks",
        "Implement CDK pipelines with cross-stack references and automated dependency management",
        "Create monolithic CDK application with team-specific constructs and conditional deployment",
        "Use separate CDK applications per team with manual infrastructure coordination"
    ],
    correct: 0,
    explanation: {
        correct: "Shared infrastructure stack provides common components. CDK constructs library enables reusable patterns. Application stacks reference shared infrastructure independently.",
        whyWrong: {
            1: "Cross-stack references create tight coupling that can block deployments",
            2: "Monolithic application doesn't provide team independence and creates deployment bottlenecks",
            3: "Separate applications with manual coordination defeats the purpose of infrastructure as code"
        },
        examStrategy: "CDK team architecture: Shared infrastructure stack + constructs library + independent application stacks."
    }
},
{
    id: 'dva_177',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A global application deploys to multiple regions with region-specific compliance requirements. Some regions require data residency, others need specific encryption standards. The deployment process must handle region-specific configurations while maintaining consistency.",
    question: "Which deployment strategy handles REGION-SPECIFIC requirements with global consistency?",
    options: [
        "Use CloudFormation StackSets with region-specific parameters and compliance-aware deployment policies",
        "Implement region-specific CI/CD pipelines with customized CloudFormation templates per region",
        "Use AWS Config Rules to enforce compliance post-deployment with automated remediation",
        "Deploy through AWS Control Tower with region-specific Organizational Units and guardrails"
    ],
    correct: 0,
    explanation: {
        correct: "StackSets provide consistent deployment patterns across regions. Region-specific parameters handle compliance differences. Deployment policies ensure region-appropriate configurations.",
        whyWrong: {
            1: "Region-specific pipelines create management overhead and reduce consistency",
            2: "Config Rules are reactive compliance monitoring, not proactive deployment configuration",
            3: "Control Tower focuses on account governance, not application deployment with region-specific requirements"
        },
        examStrategy: "Global compliance deployments: CloudFormation StackSets + region parameters + compliance policies."
    }
},
{
    id: 'dva_178',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A serverless API uses API Gateway with Lambda functions. Deployments must support versioning, traffic splitting between versions, and automated rollback if error rates exceed thresholds. The solution must minimize cold start impact.",
    question: "What API Gateway deployment approach provides VERSION management with automated rollback?",
    options: [
        "Use API Gateway stages with Lambda aliases and weighted routing for traffic splitting",
        "Implement separate API Gateway deployments with DNS switching for version management",
        "Use API Gateway canary deployments with CloudWatch alarms for automated rollback",
        "Deploy multiple API Gateway instances with load balancer traffic distribution"
    ],
    correct: 2,
    explanation: {
        correct: "API Gateway canary deployments provide built-in traffic splitting and version management. CloudWatch alarms enable automated rollback on error thresholds. Integrated solution for serverless APIs.",
        whyWrong: {
            0: "Lambda aliases with weighted routing require manual management and don't integrate with API Gateway stages",
            1: "Separate deployments with DNS switching adds complexity and doesn't provide gradual traffic shifting",
            3: "Multiple API Gateway instances create management overhead without integrated rollback capabilities"
        },
        examStrategy: "API Gateway versioning: Canary deployments + CloudWatch alarm rollback for automated version management."
    }
},
{
    id: 'dva_179',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A containerized application uses ECS with CodeDeploy for blue/green deployments. The application has complex health checks including database connectivity, external API availability, and custom business logic validation. Standard health checks are insufficient.",
    question: "Which health check strategy provides COMPREHENSIVE validation for complex application deployments?",
    options: [
        "Use ECS service health checks with custom Lambda functions for extended validation and CodeDeploy lifecycle hooks",
        "Implement Application Load Balancer health checks with multiple target groups and custom health endpoints",
        "Use CodeDeploy with custom deployment scripts and external monitoring system integration",
        "Deploy health check containers alongside application containers with shared validation logic"
    ],
    correct: 0,
    explanation: {
        correct: "ECS service health checks provide basic validation. Lambda functions handle complex business logic validation. CodeDeploy lifecycle hooks integrate custom validation into deployment process.",
        whyWrong: {
            1: "ALB health checks are limited to HTTP responses and can't validate complex business logic",
            2: "External monitoring integration adds complexity and may not integrate well with CodeDeploy automation",
            3: "Sidecar health check containers add resource overhead and complexity"
        },
        examStrategy: "Complex ECS health checks: Service health checks + Lambda validation + CodeDeploy lifecycle hooks."
    }
},
{
    id: 'dva_180',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A development team wants to implement feature flags in a Lambda-based serverless application. Feature flags should be toggleable without deployment, support gradual rollouts, and integrate with monitoring for automatic rollback.",
    question: "What feature flag implementation provides DYNAMIC control with automated rollback capabilities?",
    options: [
        "Use AWS AppConfig for feature flag management with Lambda extension for low-latency retrieval",
        "Implement feature flags in DynamoDB with Lambda function polling and caching",
        "Use Parameter Store for feature flags with Lambda environment variable updates",
        "Store feature flags in S3 with Lambda polling and local caching mechanisms"
    ],
    correct: 0,
    explanation: {
        correct: "AWS AppConfig is purpose-built for feature flags with validation, gradual rollouts, and automatic rollback. Lambda extension provides sub-millisecond retrieval with caching.",
        whyWrong: {
            1: "DynamoDB approach requires custom implementation of validation, rollouts, and rollback logic",
            2: "Parameter Store lacks the advanced feature flag capabilities like gradual rollouts and automatic rollback",
            3: "S3 polling adds latency and doesn't provide the sophisticated feature flag management needed"
        },
        examStrategy: "Feature flags: AWS AppConfig + Lambda extension for purpose-built feature flag management with rollback."
    }
},
{
    id: 'dva_181',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "A microservices architecture uses CodeCommit, CodeBuild, and CodeDeploy with multiple service repositories. Services have dependencies on each other, and the deployment order matters. The solution must handle dependency-aware deployments across multiple repositories.",
    question: "Which CI/CD approach manages INTER-SERVICE dependencies with coordinated deployments?",
    options: [
        "Use CodePipeline with multiple source stages and AWS Step Functions for deployment orchestration",
        "Implement mono-repository approach with single CodePipeline and dependency-aware build stages",
        "Use separate CodePipelines per service with EventBridge for cross-pipeline coordination",
        "Deploy services independently with runtime dependency checks and graceful degradation"
    ],
    correct: 0,
    explanation: {
        correct: "Multiple CodePipeline source stages handle different repositories. Step Functions orchestrate deployment order based on service dependencies. Maintains repository separation with coordinated deployment.",
        whyWrong: {
            1: "Mono-repository approach forces all teams into single codebase, reducing team autonomy",
            2: "EventBridge coordination adds complexity and doesn't provide deployment ordering guarantees",
            3: "Independent deployment with runtime checks doesn't solve deployment ordering for dependent services"
        },
        examStrategy: "Service dependency deployment: CodePipeline multi-source + Step Functions orchestration for dependency order."
    }
},
{
    id: 'dva_182',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function deployment package includes large ML models that rarely change. The deployment process is slow due to package size, and developers want to update application code without redeploying the models.",
    question: "What Lambda packaging strategy separates FREQUENT code updates from INFREQUENT model updates?",
    options: [
        "Use Lambda Layers for ML models and main deployment package for application code",
        "Store ML models in S3 and download them during Lambda cold start initialization",
        "Use Lambda container images with multi-stage builds for models and application code",
        "Implement EFS mounting to Lambda functions for shared model storage"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda Layers are designed for shared, infrequently changing components like ML models. Application code can be updated independently without redeploying models in layers.",
        whyWrong: {
            1: "S3 model downloads during cold start add significant latency to function execution",
            2: "Container images still bundle everything together, not providing separation of update cycles",
            3: "EFS adds latency and complexity compared to Lambda Layers for this use case"
        },
        examStrategy: "Lambda code/model separation: Layers for infrequent models + main package for frequent code updates."
    }
},
{
    id: 'dva_183',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A global e-commerce platform uses CloudFormation for infrastructure deployment across 12 regions. Each region has different instance types available, compliance requirements, and networking configurations. The deployment must handle regional differences while maintaining template consistency.",
    question: "Which CloudFormation approach handles REGIONAL variations with template consistency and compliance?",
    options: [
        "Use CloudFormation StackSets with region-specific mapping sections and conditional resource creation",
        "Implement separate CloudFormation templates per region with shared parameter files",
        "Use CloudFormation macros to transform templates dynamically based on target region",
        "Deploy through AWS Service Catalog with region-specific product versions"
    ],
    correct: 0,
    explanation: {
        correct: "StackSets provide consistent deployment across regions. Mapping sections handle region-specific values (instance types, AMIs). Conditions enable region-specific resource creation for compliance.",
        whyWrong: {
            1: "Separate templates create maintenance overhead and reduce consistency",
            2: "CloudFormation macros add complexity and are harder to debug than mappings and conditions",
            3: "Service Catalog adds abstraction layer that doesn't solve regional template variation challenges"
        },
        examStrategy: "Regional CloudFormation: StackSets + mappings for region values + conditions for compliance resources."
    }
},
{
    id: 'dva_184',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A development team deploys serverless applications using AWS SAM. The team wants to implement local testing, automated testing in the pipeline, and integration testing against real AWS services before production deployment.",
    question: "What SAM testing strategy provides COMPREHENSIVE validation from local development to production?",
    options: [
        "Use SAM local for development testing, SAM pipeline with CodeBuild for automated tests, and separate test environment for integration testing",
        "Implement unit tests with mocked AWS services and deploy directly to production with monitoring",
        "Use SAM local exclusively for all testing with simulated AWS service responses",
        "Deploy to shared development environment for all testing phases with manual validation"
    ],
    correct: 0,
    explanation: {
        correct: "SAM local enables local development testing. CodeBuild in SAM pipeline provides automated testing. Separate test environment allows integration testing against real AWS services before production.",
        whyWrong: {
            1: "Mocked services don't catch integration issues and direct production deployment is risky",
            2: "SAM local alone doesn't provide integration testing against real AWS services",
            3: "Shared development environment creates conflicts and doesn't provide production-like testing"
        },
        examStrategy: "SAM testing pipeline: Local development + automated pipeline testing + real AWS integration testing."
    }
},
{
    id: 'dva_185',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A mission-critical application requires zero-downtime deployments with immediate rollback capability if any issues are detected. The application uses ALB, ECS Fargate, and RDS. The deployment process must validate application health, database connectivity, and business logic before completing.",
    question: "Which deployment architecture provides MISSION-CRITICAL zero-downtime with comprehensive validation?",
    options: [
        "Use CodeDeploy blue/green with ECS, RDS read replicas for testing, custom lifecycle hooks for validation, and automated rollback triggers",
        "Implement rolling deployments with ECS service updates and health check monitoring",
        "Use separate environments with traffic switching via Route 53 weighted routing",
        "Deploy with ECS capacity providers and gradual task replacement with manual validation"
    ],
    correct: 0,
    explanation: {
        correct: "CodeDeploy blue/green creates complete environment isolation. RDS read replicas enable database connectivity testing. Lifecycle hooks provide custom validation. Automated rollback ensures immediate recovery.",
        whyWrong: {
            1: "Rolling deployments can cause partial service disruption and don't provide complete environment isolation",
            2: "Route 53 traffic switching is slower than ALB target group switching and adds DNS propagation delays",
            3: "Manual validation doesn't meet mission-critical requirements for immediate automated rollback"
        },
        examStrategy: "Mission-critical deployment: CodeDeploy blue/green + read replica testing + lifecycle hooks + automated rollback."
    }
},
{
    id: 'dva_186',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A team uses Git-based deployment workflows with CodeCommit repositories. Different branches should automatically deploy to different environments (feature branches to dev, main branch to staging, tags to production).",
    question: "What Git workflow automation provides BRANCH-BASED deployment with appropriate environment targeting?",
    options: [
        "Use CodePipeline with branch-based source triggers and conditional deployments based on branch patterns",
        "Implement separate CodePipeline instances for each branch with manual triggering",
        "Use CodeBuild with webhook triggers and environment detection based on Git metadata",
        "Deploy through GitHub Actions with AWS credentials and branch-specific workflows"
    ],
    correct: 0,
    explanation: {
        correct: "CodePipeline supports branch-based source triggers. Conditional deployments can route to appropriate environments based on branch patterns. Native AWS integration with proper IAM controls.",
        whyWrong: {
            1: "Separate pipelines create management overhead and don't scale to feature branch workflows",
            2: "CodeBuild alone lacks the deployment orchestration needed for multi-environment targeting",
            3: "GitHub Actions require external credential management and don't integrate as well with AWS services"
        },
        examStrategy: "Git-based deployment: CodePipeline branch triggers + conditional deployments for environment targeting."
    }
},

// DOMAIN 4: TROUBLESHOOTING (18 questions)

{
    id: 'dva_187',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A Lambda function intermittently times out after 3 minutes when processing large datasets from S3. CloudWatch logs show the function starts processing but doesn't complete. Memory usage appears normal, but CPU utilization metrics are inconsistent.",
    question: "What troubleshooting approach will MOST effectively identify the timeout root cause?",
    options: [
        "Enable X-Ray tracing to analyze execution timeline and identify bottlenecks in the processing pipeline",
        "Increase Lambda memory allocation to 3GB and timeout to 15 minutes to eliminate resource constraints",
        "Implement custom CloudWatch metrics to track processing stages and identify where delays occur",
        "Add detailed logging statements throughout the function code to trace execution flow"
    ],
    correct: 0,
    explanation: {
        correct: "X-Ray tracing provides detailed execution timeline showing exactly where time is spent. Identifies bottlenecks in S3 operations, processing logic, or external service calls that cause timeouts.",
        whyWrong: {
            1: "Increasing resources without identifying the bottleneck may not solve the underlying issue",
            2: "Custom metrics help but don't provide the detailed execution timeline needed for timeout analysis",
            3: "Detailed logging adds overhead and may not capture timing relationships between operations"
        },
        examStrategy: "Lambda timeout troubleshooting: X-Ray tracing for execution timeline analysis to identify specific bottlenecks."
    }
},
{
    id: 'dva_188',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "API Gateway returns 502 errors intermittently for a Lambda-backed REST API. The Lambda function logs show successful execution, but API Gateway access logs indicate backend errors. The issue occurs more frequently during high traffic periods.",
    question: "What is the MOST likely cause and troubleshooting approach for these 502 errors?",
    options: [
        "Lambda function timeout - increase timeout settings and check for long-running operations",
        "Lambda concurrent execution limits - monitor concurrency metrics and increase reserved concurrency",
        "API Gateway response format issues - validate Lambda function response structure and headers",
        "VPC configuration problems - check security groups and subnet configurations for Lambda"
    ],
    correct: 2,
    explanation: {
        correct: "502 errors with successful Lambda execution typically indicate response format issues. Lambda must return proper JSON structure with statusCode, headers, and body for API Gateway integration.",
        whyWrong: {
            0: "Lambda timeouts would show in CloudWatch logs, but logs show successful execution",
            1: "Concurrency limits would cause 429 errors, not 502 errors with successful Lambda execution",
            3: "VPC issues would prevent Lambda execution entirely, not cause response format problems"
        },
        examStrategy: "API Gateway 502 + successful Lambda: Check Lambda response format (statusCode, headers, body) structure."
    }
},
{
    id: 'dva_189',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "DynamoDB operations are experiencing throttling errors despite having sufficient provisioned capacity. The table has a hot partition issue, and queries are failing with ProvisionedThroughputExceededException. The table design uses timestamp as the sort key.",
    question: "Which troubleshooting approach will BEST resolve the hot partition and throttling issues?",
    options: [
        "Analyze access patterns with CloudWatch Contributor Insights and implement partition key diversification strategies",
        "Enable DynamoDB auto-scaling and increase read/write capacity units across all partitions",
        "Use DynamoDB Accelerator (DAX) to cache frequently accessed items and reduce direct table load",
        "Implement exponential backoff in application code and use eventually consistent reads"
    ],
    correct: 0,
    explanation: {
        correct: "Contributor Insights identifies hot partition keys causing throttling. Partition key diversification (adding prefixes, using composite keys) distributes load evenly across partitions.",
        whyWrong: {
            1: "Auto-scaling won't help hot partitions since capacity is unevenly distributed among partitions",
            2: "DAX caching helps read performance but doesn't solve the underlying hot partition problem",
            3: "Exponential backoff and eventually consistent reads don't address the root cause of uneven partition access"
        },
        examStrategy: "DynamoDB hot partition: Contributor Insights analysis + partition key diversification for even load distribution."
    }
},
{
    id: 'dva_190',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function triggered by S3 events fails to process some uploaded files. CloudWatch logs show 'Unable to import module' errors for certain file types. The function processes JPEG files successfully but fails on PNG and PDF files.",
    question: "What troubleshooting approach will identify why the function fails for specific file types?",
    options: [
        "Check if the Lambda function has sufficient memory allocation for different file sizes",
        "Verify that required Python libraries for PNG and PDF processing are included in the deployment package",
        "Increase Lambda timeout to allow more processing time for complex file formats",
        "Examine S3 event structure to ensure file type information is correctly passed to Lambda"
    ],
    correct: 1,
    explanation: {
        correct: "'Unable to import module' errors indicate missing dependencies. PNG and PDF processing likely require specific libraries (PIL, PyPDF2) not included in the deployment package.",
        whyWrong: {
            0: "Memory allocation doesn't affect module import capabilities",
            2: "Timeout doesn't solve import errors, which occur before function execution begins",
            3: "S3 event structure is consistent regardless of file type; the issue is in function dependencies"
        },
        examStrategy: "Lambda 'Unable to import module': Check deployment package for required libraries/dependencies."
    }
},
{
    id: 'dva_191',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A microservices application experiences slow response times during peak traffic. The architecture uses API Gateway, Lambda, and DynamoDB. X-Ray traces show high latency in Lambda function execution, but individual Lambda metrics appear normal.",
    question: "Which X-Ray analysis approach will MOST effectively identify the performance bottleneck?",
    options: [
        "Analyze X-Ray service map to identify the slowest service components and their dependencies",
        "Use X-Ray trace timeline to examine Lambda cold start frequency and initialization time",
        "Implement X-Ray custom segments to measure performance of specific code blocks within Lambda functions",
        "Compare X-Ray traces between peak and normal traffic periods to identify load-related bottlenecks"
    ],
    correct: 3,
    explanation: {
        correct: "Comparing traces between peak and normal traffic reveals load-related issues like database connection exhaustion, external API throttling, or resource contention that only appear under high load.",
        whyWrong: {
            0: "Service map shows high-level latency but doesn't identify load-specific bottlenecks",
            1: "Cold starts would show in individual Lambda metrics, but the problem description states these appear normal",
            2: "Custom segments help with detailed analysis but comparing traffic patterns is more effective for load-related issues"
        },
        examStrategy: "Peak traffic performance issues: Compare X-Ray traces between peak vs normal traffic for load-specific bottlenecks."
    }
},
{
    id: 'dva_192',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "CloudWatch logs for a Lambda function show 'Task timed out after X seconds' errors, but the function timeout is set to 15 minutes. The function processes data from SQS and makes multiple API calls to external services.",
    question: "What could cause Lambda timeout errors when the configured timeout should be sufficient?",
    options: [
        "External API calls are hanging without proper timeout configuration in the HTTP client",
        "SQS visibility timeout is shorter than Lambda timeout, causing message reprocessing",
        "Lambda function is running out of memory, causing the runtime to terminate early",
        "The function is hitting the maximum execution duration limit for the Lambda runtime"
    ],
    correct: 0,
    explanation: {
        correct: "HTTP clients without proper timeout configuration can hang indefinitely waiting for responses, causing Lambda to hit its timeout limit even when external services are unavailable.",
        whyWrong: {
            1: "SQS visibility timeout issues cause duplicate processing, not Lambda timeout errors",
            2: "Memory exhaustion would show different error messages and memory metrics",
            3: "15-minute timeout is the maximum Lambda limit, so this wouldn't be exceeded"
        },
        examStrategy: "Lambda timeout with external APIs: Check HTTP client timeout configuration to prevent hanging connections."
    }
},
{
    id: 'dva_193',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A serverless application randomly returns stale data to users despite recent updates to DynamoDB. The application uses Lambda with DynamoDB reads, and the issue occurs across different regions. Some users see updated data while others see old data.",
    question: "What troubleshooting approach will identify the cause of inconsistent data reads?",
    options: [
        "Check DynamoDB consistency settings and implement strongly consistent reads for critical operations",
        "Analyze Lambda function execution environments to identify connection pooling issues with DynamoDB",
        "Examine DynamoDB Global Tables replication lag between regions and implement read preferences",
        "Implement application-level caching validation to ensure cache invalidation is working properly"
    ],
    correct: 0,
    explanation: {
        correct: "DynamoDB eventually consistent reads can return stale data. Random stale data across regions suggests eventually consistent reads during periods of write activity. Strongly consistent reads solve this.",
        whyWrong: {
            1: "Connection pooling doesn't cause stale data; it affects performance and connection management",
            2: "Global Tables replication would affect region-specific consistency, not random stale data within regions",
            3: "The problem description doesn't mention application-level caching, focusing on DynamoDB reads"
        },
        examStrategy: "DynamoDB stale data troubleshooting: Check consistency settings - implement strongly consistent reads."
    }
},
{
    id: 'dva_194',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function triggered by Kinesis streams processes events successfully in development but fails in production with 'Record processing failed' errors. The error rate increases during high-traffic periods.",
    question: "What is the MOST likely cause of Kinesis processing failures in production?",
    options: [
        "Lambda concurrent execution limits preventing adequate scaling for Kinesis shard processing",
        "Kinesis shard iterator expires due to slow processing of individual records in the batch",
        "Lambda function timeout is insufficient for processing large Kinesis record batches",
        "Kinesis stream retention period is too short, causing records to expire before processing"
    ],
    correct: 1,
    explanation: {
        correct: "Kinesis shard iterators have a 5-minute expiration. Slow record processing in production (due to higher load, external API delays) can cause iterator expiration and processing failures.",
        whyWrong: {
            0: "Concurrency limits would show throttling errors, not 'Record processing failed' errors",
            2: "Timeout errors would show timeout-specific error messages, not record processing failures",
            3: "Default retention is 24 hours; records wouldn't expire that quickly under normal circumstances"
        },
        examStrategy: "Kinesis 'Record processing failed': Check for slow processing causing shard iterator expiration (5-minute limit)."
    }
},
{
    id: 'dva_195',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "An application experiences memory leaks in Lambda functions that process images from S3. Memory usage increases over time within the same execution context, eventually causing out-of-memory errors. The function uses global variables for image processing libraries.",
    question: "Which debugging approach will MOST effectively identify and resolve the memory leak?",
    options: [
        "Use CloudWatch Lambda Insights to monitor memory usage patterns and identify memory growth over time",
        "Implement memory profiling within the Lambda function code to track object allocation and deallocation",
        "Increase Lambda memory allocation and implement execution context reuse limits",
        "Use X-Ray memory tracking to identify memory-intensive operations and optimize processing logic"
    ],
    correct: 1,
    explanation: {
        correct: "Memory profiling within the function code can track object creation/destruction, identify global variable memory accumulation, and pinpoint specific operations causing leaks.",
        whyWrong: {
            0: "Lambda Insights shows memory usage but doesn't provide detailed allocation tracking needed for leak analysis",
            2: "Increasing memory masks the problem; execution context limits help but don't identify the leak source",
            3: "X-Ray doesn't provide detailed memory allocation tracking needed for memory leak debugging"
        },
        examStrategy: "Lambda memory leak debugging: In-function memory profiling to track object allocation/deallocation patterns."
    }
},
{
    id: 'dva_196',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A DynamoDB table experiences frequent throttling despite using on-demand billing mode. The table handles e-commerce traffic with sudden spikes during sales events. Throttling occurs even when traffic is within expected limits.",
    question: "What could cause throttling in DynamoDB on-demand mode during traffic spikes?",
    options: [
        "DynamoDB on-demand mode has a maximum request rate that is being exceeded during sales events",
        "On-demand scaling takes time to adapt, and sudden traffic spikes exceed the current allocated capacity",
        "Hot partition keys are causing uneven load distribution despite on-demand capacity",
        "Table GSI (Global Secondary Index) limits are causing throttling in the main table"
    ],
    correct: 1,
    explanation: {
        correct: "DynamoDB on-demand mode scales automatically but takes time to adapt to traffic increases. Sudden spikes (2x or more) can temporarily exceed current capacity before scaling occurs.",
        whyWrong: {
            0: "On-demand mode doesn't have hard request rate limits like provisioned mode",
            2: "Hot partitions can cause throttling, but on-demand mode handles this better than provisioned mode",
            3: "GSI throttling would show specific errors and wouldn't affect main table unless GSI is being queried"
        },
        examStrategy: "DynamoDB on-demand throttling: Sudden traffic spikes exceed scaling adaptation time - implement gradual scaling or pre-warming."
    }
},
{
    id: 'dva_197',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A Lambda function that processes financial transactions intermittently fails with 'Unable to import module' errors, but only for specific execution contexts. The function works correctly most of the time and uses the same deployment package across all invocations.",
    question: "What advanced troubleshooting technique will identify why module imports fail intermittently?",
    options: [
        "Implement Lambda execution environment logging to track container reuse and identify problematic environments",
        "Use AWS X-Ray to trace module import operations and identify timing-related import failures",
        "Enable detailed CloudWatch Logs and implement custom error handling for import operations",
        "Deploy the function with reserved concurrency to ensure consistent execution environments"
    ],
    correct: 0,
    explanation: {
        correct: "Intermittent import failures suggest specific execution environments have corrupted state or missing dependencies. Tracking container reuse identifies problematic environments.",
        whyWrong: {
            1: "X-Ray doesn't trace module imports since they occur before function execution begins",
            2: "CloudWatch Logs help but don't identify the relationship between import failures and execution environments",
            3: "Reserved concurrency doesn't solve environment-specific import issues"
        },
        examStrategy: "Intermittent Lambda import failures: Track execution environment/container reuse to identify problematic environments."
    }
},
{
    id: 'dva_198',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "API Gateway REST API returns 504 Gateway Timeout errors during peak traffic. Backend Lambda functions show normal execution times (2-3 seconds) in CloudWatch metrics. The API has custom authorizers and uses proxy integration.",
    question: "What is the MOST likely cause of 504 timeout errors with normal Lambda execution times?",
    options: [
        "Lambda authorizer function is timing out or taking longer than the configured timeout",
        "API Gateway integration timeout is set lower than the Lambda function execution time",
        "VPC configuration for Lambda functions is causing network delays",
        "DynamoDB throttling in the backend is causing Lambda function delays"
    ],
    correct: 0,
    explanation: {
        correct: "Custom authorizers have their own timeout limits. During peak traffic, authorizer functions may experience cold starts or resource constraints causing timeouts before the main Lambda function is even invoked.",
        whyWrong: {
            1: "API Gateway integration timeout is 29 seconds by default, much longer than 2-3 second Lambda execution",
            2: "VPC delays would show in Lambda execution time metrics, but these are normal",
            3: "DynamoDB throttling would appear in Lambda execution time, but the metrics show normal times"
        },
        examStrategy: "API Gateway 504 + normal Lambda times: Check custom authorizer timeout/performance during peak traffic."
    }
},
{
    id: 'dva_199',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A Step Functions workflow intermittently fails with 'States.TaskFailed' errors during the third step of a five-step process. The failure rate increases during high-concurrency periods, but individual Lambda functions in the workflow execute successfully when tested independently.",
    question: "Which Step Functions troubleshooting approach will identify the cause of intermittent workflow failures?",
    options: [
        "Analyze Step Functions execution history to identify patterns in failure timing and resource constraints",
        "Implement Step Functions error handling with retry policies and exponential backoff for the failing step",
        "Use X-Ray integration with Step Functions to trace execution flow and identify bottlenecks",
        "Enable detailed logging for all Lambda functions in the workflow to capture error details"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions execution history shows detailed failure information, timing patterns, and resource utilization that help identify concurrency-related issues or resource constraints specific to workflow orchestration.",
        whyWrong: {
            1: "Retry policies help with resilience but don't identify the root cause of failures",
            2: "X-Ray helps with performance analysis but execution history provides more specific workflow failure details",
            3: "Lambda function logging won't show Step Functions orchestration issues if individual functions work correctly"
        },
        examStrategy: "Step Functions intermittent failures: Analyze execution history for failure patterns and resource constraints."
    }
},
{
    id: 'dva_200',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A serverless application experiences high latency during database operations. The application uses Lambda functions with RDS Aurora Serverless. Latency spikes occur unpredictably and don't correlate with traffic patterns.",
    question: "What database troubleshooting approach will identify the cause of unpredictable latency spikes?",
    options: [
        "Enable RDS Performance Insights to analyze database performance and identify slow queries during latency spikes",
        "Monitor Aurora Serverless scaling events and correlate them with latency spike timing",
        "Implement Lambda database connection pooling to reduce connection establishment overhead",
        "Use CloudWatch database metrics to identify resource utilization patterns during high latency periods"
    ],
    correct: 1,
    explanation: {
        correct: "Aurora Serverless automatically scales compute capacity. Scaling events (especially scale-up operations) can cause temporary latency spikes that don't correlate with traffic patterns.",
        whyWrong: {
            0: "Performance Insights helps with query optimization but doesn't identify Aurora Serverless scaling-related latency",
            2: "Connection pooling helps overall performance but doesn't solve Aurora Serverless scaling latency",
            3: "Basic CloudWatch metrics don't show Aurora Serverless scaling events that cause unpredictable latency"
        },
        examStrategy: "Aurora Serverless unpredictable latency: Monitor scaling events correlation with latency spikes."
    }
},
{
    id: 'dva_201',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 140,
    scenario: "A Lambda function processes messages from SQS and occasionally fails to delete messages after successful processing, causing duplicate processing. The function logs show successful execution and proper delete attempts, but messages remain visible in the queue.",
    question: "What SQS troubleshooting approach will identify why message deletion fails despite successful processing?",
    options: [
        "Check SQS visibility timeout settings and ensure they're longer than Lambda function execution time plus deletion time",
        "Verify that the SQS delete message operation uses the correct receipt handle from the original receive operation",
        "Monitor SQS CloudWatch metrics for delete errors and implement retry logic for failed deletions",
        "Implement SQS batch operations to improve deletion reliability and reduce API call overhead"
    ],
    correct: 1,
    explanation: {
        correct: "SQS message deletion requires the exact receipt handle from the receive operation. If the receipt handle is modified, stored incorrectly, or from a different receive operation, deletion will fail silently.",
        whyWrong: {
            0: "Visibility timeout affects message visibility, not deletion success after processing",
            2: "Monitoring helps but doesn't solve the root cause of incorrect receipt handle usage",
            3: "Batch operations improve efficiency but don't solve receipt handle correctness issues"
        },
        examStrategy: "SQS message deletion failure: Verify correct receipt handle usage from original receive operation."
    }
},
{
    id: 'dva_202',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "CloudWatch custom metrics from Lambda functions are missing or delayed during high-traffic periods. The Lambda functions execute successfully and logging shows metric publication attempts, but metrics don't appear in CloudWatch dashboards.",
    question: "What could cause CloudWatch custom metrics to be missing during high-traffic periods?",
    options: [
        "CloudWatch API throttling limits are being exceeded during metric publication bursts",
        "Lambda function execution time increases during high traffic, causing metric publication timeouts",
        "CloudWatch custom metrics have a delay during high load and will eventually appear",
        "Lambda concurrent execution limits prevent metric publication threads from executing"
    ],
    correct: 0,
    explanation: {
        correct: "CloudWatch PutMetricData API has request rate limits. During high traffic, simultaneous metric publications from multiple Lambda invocations can exceed these limits, causing throttling.",
        whyWrong: {
            1: "Lambda execution time doesn't affect CloudWatch API timeouts directly",
            2: "While CloudWatch has some delays, missing metrics during high traffic suggests throttling rather than normal delay",
            3: "Concurrency limits affect Lambda invocations but don't prevent metric publication within executing functions"
        },
        examStrategy: "Missing CloudWatch custom metrics during high traffic: CloudWatch API throttling - implement batching or reduce publication frequency."
    }
},
{
    id: 'dva_203',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A complex serverless application with multiple Lambda functions, API Gateway, DynamoDB, and S3 experiences performance degradation. Response times vary significantly for similar requests, and the issue affects different components unpredictably.",
    question: "Which comprehensive troubleshooting strategy will MOST effectively identify performance bottlenecks across the distributed system?",
    options: [
        "Implement distributed tracing with X-Ray across all components and analyze end-to-end request flows",
        "Use CloudWatch synthetic monitoring to simulate user journeys and identify performance patterns",
        "Enable detailed logging across all components and correlate timestamps to identify bottlenecks",
        "Implement custom performance metrics for each service component and create unified dashboards"
    ],
    correct: 0,
    explanation: {
        correct: "X-Ray distributed tracing provides end-to-end visibility across Lambda, API Gateway, DynamoDB, and S3. Shows request flow, timing breakdown, and identifies bottlenecks in complex distributed architectures.",
        whyWrong: {
            1: "Synthetic monitoring helps with user experience but doesn't provide detailed bottleneck analysis",
            2: "Manual log correlation is complex and error-prone for distributed systems with timing variations",
            3: "Custom metrics help but don't provide the request flow context needed for distributed troubleshooting"
        },
        examStrategy: "Distributed system performance troubleshooting: X-Ray tracing for end-to-end visibility and bottleneck identification."
    }
},
{
    id: 'dva_204',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function that processes images from S3 works correctly for small files but fails with 'No space left on device' errors for large files. The function is allocated 3GB of memory but still encounters storage limitations.",
    question: "What is the cause of storage limitations despite sufficient memory allocation?",
    options: [
        "Lambda /tmp directory has a 512MB limit regardless of memory allocation",
        "S3 objects larger than Lambda memory allocation cannot be processed",
        "Lambda function timeout is preventing complete file processing",
        "Lambda deployment package size limits are being exceeded during execution"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda /tmp directory has a fixed 512MB limit that doesn't increase with memory allocation. Large image processing that writes temporary files will hit this limit.",
        whyWrong: {
            1: "S3 object size isn't directly limited by Lambda memory allocation",
            2: "Timeout would show timeout errors, not storage space errors",
            3: "Deployment package limits affect deployment, not runtime storage space"
        },
        examStrategy: "Lambda 'No space left on device': /tmp directory 512MB limit - use streaming or external storage for large files."
    }
},

// BATCH 4: 60 Advanced Domain 1 Questions - Development with AWS Services (32%)
// Expert-level scenarios and complex integrations

{
    id: 'dva_205',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A real-time bidding platform processes 1 million bid requests per second with sub-50ms latency requirements. The system uses API Gateway, Lambda, and DynamoDB with complex business rules stored in different formats (JSON rules, ML models, lookup tables). Rules change frequently and must be applied consistently across all requests.",
    question: "Which architecture achieves ULTRA-LOW latency while maintaining rule consistency and update flexibility?",
    options: [
        "Use Lambda with ElastiCache Redis for rule caching, DynamoDB Streams for rule updates, and provisioned concurrency for consistent performance",
        "Implement API Gateway with cached responses, Lambda Layers for rules, and DynamoDB DAX for data access",
        "Use Lambda@Edge for geo-distributed processing, S3 for rule storage, and CloudFront for global caching",
        "Deploy ECS Fargate with Redis cluster, EventBridge for rule updates, and Application Load Balancer"
    ],
    correct: 0,
    explanation: {
        correct: "Redis provides sub-millisecond rule access. DynamoDB Streams enable real-time rule propagation. Provisioned concurrency eliminates cold starts for consistent sub-50ms performance at scale.",
        whyWrong: {
            1: "API Gateway caching doesn't help with dynamic bidding logic; Lambda Layers require redeployment for rule updates",
            2: "Lambda@Edge has limited execution time and memory for complex business rules processing",
            3: "ECS adds container orchestration overhead compared to serverless Lambda for ultra-low latency requirements"
        },
        examStrategy: "Ultra-low latency + frequent updates: Redis caching + DynamoDB Streams + provisioned concurrency for consistent performance."
    }
},
{
    id: 'dva_206',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A Lambda function processes IoT sensor data and must maintain a sliding window of the last 1000 readings per sensor for real-time analytics. The function receives data from 10,000 sensors every 30 seconds and must calculate moving averages and detect anomalies.",
    question: "What data structure and storage approach provides EFFICIENT sliding window management?",
    options: [
        "Use DynamoDB with TTL for automatic cleanup and GSI for time-based queries",
        "Implement Redis sorted sets with scores as timestamps for automatic sliding window management",
        "Store data in S3 with Lambda periodic cleanup and DynamoDB for current window tracking",
        "Use Kinesis Data Analytics with tumbling windows and DynamoDB for persistent storage"
    ],
    correct: 1,
    explanation: {
        correct: "Redis sorted sets automatically maintain ordered data by timestamp. ZREMRANGEBYSCORE efficiently removes old data. ZRANGE provides fast sliding window queries for 1000 most recent readings.",
        whyWrong: {
            0: "DynamoDB TTL has up to 48-hour deletion delay, not suitable for precise 1000-reading windows",
            2: "S3 storage adds latency and complexity for real-time sliding window operations",
            3: "Kinesis Analytics tumbling windows don't provide the flexible sliding window needed per sensor"
        },
        examStrategy: "Sliding window data: Redis sorted sets for automatic timestamp-based ordering and efficient range operations."
    }
},
{
    id: 'dva_207',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A serverless application implements a distributed state machine using Step Functions. The workflow must handle long-running external API calls (up to 6 hours), support parallel branches with different timing requirements, and maintain exactly-once execution semantics even during AWS service failures.",
    question: "Which Step Functions pattern provides ROBUST distributed state machine with fault tolerance?",
    options: [
        "Use Step Functions Standard workflow with Activity Tasks for external APIs and Lambda for coordination logic",
        "Implement Express workflows with Lambda polling for external API completion and DynamoDB state tracking",
        "Use Standard workflow with Lambda callbacks for external APIs and built-in retry/error handling",
        "Deploy separate Step Functions per workflow branch with EventBridge for coordination"
    ],
    correct: 0,
    explanation: {
        correct: "Activity Tasks handle long-running external operations with worker polling. Standard workflows provide exactly-once execution with built-in state persistence and fault tolerance.",
        whyWrong: {
            1: "Express workflows have 5-minute limit and don't provide exactly-once semantics for 6-hour operations",
            2: "Lambda callbacks work but Activity Tasks are specifically designed for long-running external operations",
            3: "Separate Step Functions lose coordination benefits and exactly-once semantics across the distributed state machine"
        },
        examStrategy: "Long-running distributed workflows: Step Functions Standard + Activity Tasks for external operations + exactly-once semantics."
    }
},
{
    id: 'dva_208',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function processes file uploads and must generate multiple derivative files (thumbnails, transcripts, metadata). Each derivative type has different processing requirements and SLAs. Some derivatives can be processed in parallel while others depend on previous outputs.",
    question: "What serverless orchestration approach handles COMPLEX file processing dependencies efficiently?",
    options: [
        "Use Step Functions Map state for parallel processing with conditional workflows for dependencies",
        "Implement SQS with multiple queues and Lambda functions for each processing stage",
        "Use EventBridge with event routing rules and Lambda consumers for each derivative type",
        "Deploy Lambda with SNS fan-out pattern and SQS for dependency coordination"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions Map state handles parallel processing efficiently. Conditional workflows manage dependencies between derivative types. Built-in error handling and retry logic for complex orchestration.",
        whyWrong: {
            1: "SQS coordination requires custom dependency management logic and doesn't provide visual workflow representation",
            2: "EventBridge routing lacks the dependency orchestration capabilities needed for complex file processing chains",
            3: "SNS fan-out with SQS coordination adds complexity without providing dependency management benefits"
        },
        examStrategy: "Complex file processing: Step Functions Map state for parallel processing + conditional workflows for dependencies."
    }
},
{
    id: 'dva_209',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A multi-tenant SaaS platform uses Lambda functions with tenant-specific processing logic stored as JavaScript code in DynamoDB. The platform must execute tenant code safely in isolation, prevent infinite loops, and ensure tenant code cannot access other tenants' resources.",
    question: "Which approach provides SECURE tenant code execution with proper isolation?",
    options: [
        "Use VM2 library in Lambda with custom sandboxing, resource limits, and tenant-specific IAM execution roles",
        "Implement separate Lambda functions per tenant with tenant code in deployment packages",
        "Use Docker containers in ECS Fargate with tenant-specific code and security contexts",
        "Execute tenant code in Lambda Layers with environment variable isolation"
    ],
    correct: 0,
    explanation: {
        correct: "VM2 provides secure JavaScript sandboxing with timeout and resource controls. Custom sandboxing prevents access to sensitive APIs. Tenant-specific IAM roles ensure resource isolation.",
        whyWrong: {
            1: "Separate Lambda functions per tenant don't scale operationally and create massive deployment overhead",
            2: "ECS containers add infrastructure complexity compared to serverless Lambda for code execution sandboxing",
            3: "Lambda Layers don't provide execution isolation and environment variables don't sandbox code execution"
        },
        examStrategy: "Secure tenant code execution: VM2 sandboxing + resource limits + tenant-specific IAM roles for isolation."
    }
},
{
    id: 'dva_210',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function must process large CSV files (500MB+) from S3 and perform complex transformations before storing results. The function hits memory limits when loading entire files and timeout limits when processing row by row.",
    question: "What streaming approach optimizes LARGE file processing within Lambda constraints?",
    options: [
        "Use S3 Select to pre-filter data and process in smaller chunks with Lambda",
        "Implement streaming with S3 getObject stream and batch processing of rows in memory",
        "Split large files into smaller pieces using S3 multipart upload before processing",
        "Use Lambda with EFS mounted storage for temporary file processing"
    ],
    correct: 1,
    explanation: {
        correct: "S3 getObject streaming allows processing files of any size. Batch processing of rows (e.g., 1000 rows) in memory optimizes performance while staying within memory limits.",
        whyWrong: {
            0: "S3 Select is for querying specific data, not general CSV transformation and doesn't solve memory management",
            2: "File splitting changes source data structure and doesn't solve the streaming processing challenge",
            3: "EFS adds latency and complexity compared to streaming directly from S3"
        },
        examStrategy: "Large file processing in Lambda: S3 streaming + batch processing chunks in memory for efficiency."
    }
},
{
    id: 'dva_211',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A real-time multiplayer game uses API Gateway WebSocket API with Lambda for player actions. The game requires ordered message delivery per player, broadcast messages to groups of players, and connection state management across multiple Lambda executions.",
    question: "Which WebSocket architecture ensures ORDERED delivery and efficient connection management?",
    options: [
        "Use DynamoDB for connection tracking, SQS FIFO for ordered messages, and EventBridge for group broadcasts",
        "Implement Redis for connection state, Lambda with connection pooling, and direct WebSocket API calls",
        "Use DynamoDB Streams for message ordering, ElastiCache for connection tracking, and SNS for broadcasts",
        "Deploy connection state in Lambda global variables with in-memory message queuing"
    ],
    correct: 0,
    explanation: {
        correct: "DynamoDB provides durable connection tracking across Lambda executions. SQS FIFO ensures per-player message ordering. EventBridge efficiently handles group broadcast routing.",
        whyWrong: {
            1: "Redis connection state is good but Lambda connection pooling doesn't solve message ordering requirements",
            2: "DynamoDB Streams don't provide message ordering guarantees and add unnecessary complexity",
            3: "Lambda global variables don't persist across executions and lose connection state during scaling"
        },
        examStrategy: "WebSocket ordered messaging: DynamoDB connection tracking + SQS FIFO per-player ordering + EventBridge broadcasts."
    }
},
{
    id: 'dva_212',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A serverless application uses DynamoDB with complex query patterns including range scans, filter expressions, and aggregations. Query performance degrades as data volume grows, despite proper partition key design.",
    question: "What DynamoDB optimization strategy improves COMPLEX query performance at scale?",
    options: [
        "Implement DynamoDB Global Secondary Indexes with projection of frequently queried attributes",
        "Use DynamoDB parallel scans with multiple Lambda functions for large dataset processing",
        "Enable DynamoDB auto-scaling and increase read/write capacity for better performance",
        "Cache complex query results in ElastiCache with intelligent cache invalidation"
    ],
    correct: 0,
    explanation: {
        correct: "GSIs with attribute projection optimize access patterns by providing alternative partition/sort keys and including only necessary attributes, reducing I/O and improving performance.",
        whyWrong: {
            1: "Parallel scans help with throughput but don't optimize individual query patterns and increase cost",
            2: "Auto-scaling helps with capacity but doesn't solve query pattern optimization for complex operations",
            3: "Caching helps but doesn't solve the underlying query optimization and adds cache management complexity"
        },
        examStrategy: "DynamoDB complex queries: GSIs with attribute projection for optimized access patterns and reduced I/O."
    }
},
{
    id: 'dva_213',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A financial trading platform must process market data streams with guaranteed ordering, exactly-once processing, and sub-second latency. The system handles 100,000 messages per second from multiple exchanges with different message formats and delivery characteristics.",
    question: "Which streaming architecture guarantees ORDERING and exactly-once processing with minimal latency?",
    options: [
        "Use Kinesis Data Streams with partition keys by exchange, Lambda with checkpointing, and DynamoDB for deduplication",
        "Implement SQS FIFO queues per exchange with Lambda consumers and Redis for deduplication tracking",
        "Use EventBridge with custom buses per exchange and Lambda with idempotency keys",
        "Deploy Kinesis Data Firehose with Lambda transformation and S3 for exactly-once delivery"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis partitioning by exchange ensures ordering within each exchange. Lambda checkpointing provides exactly-once processing. DynamoDB deduplication handles duplicate messages across exchanges.",
        whyWrong: {
            1: "SQS FIFO has lower throughput limits (3000 messages/second) insufficient for 100,000 messages/second",
            2: "EventBridge doesn't provide the streaming guarantees needed for high-frequency trading data",
            3: "Kinesis Firehose has buffering delays that violate sub-second latency requirements"
        },
        examStrategy: "High-frequency ordered streaming: Kinesis partitioning + Lambda checkpointing + DynamoDB deduplication for exactly-once."
    }
},
{
    id: 'dva_214',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function integrates with multiple external APIs that have different rate limits (100/min, 1000/hour, 5000/day). The function must respect all rate limits while maximizing throughput and handling temporary API failures gracefully.",
    question: "What rate limiting approach handles MULTIPLE API constraints efficiently?",
    options: [
        "Implement token bucket algorithm in Redis with API-specific buckets and exponential backoff",
        "Use SQS with delayed messages and Lambda concurrency controls for rate limiting",
        "Store rate limit counters in DynamoDB with TTL and implement sliding window rate limiting",
        "Use API Gateway usage plans with different throttling settings for each external API"
    ],
    correct: 0,
    explanation: {
        correct: "Token bucket algorithm in Redis provides precise rate limiting for different APIs. API-specific buckets handle different limits. Exponential backoff handles temporary failures gracefully.",
        whyWrong: {
            1: "SQS delayed messages don't provide the precise rate control needed for different API limits",
            2: "DynamoDB with TTL lacks the real-time rate limiting precision needed for API throttling",
            3: "API Gateway usage plans control incoming requests, not outbound API calls to external services"
        },
        examStrategy: "Multi-API rate limiting: Redis token bucket algorithm + API-specific buckets + exponential backoff for failures."
    }
},
{
    id: 'dva_215',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A serverless data pipeline processes streaming data through multiple transformation stages. Each stage has different processing requirements, error handling needs, and scaling characteristics. Failed records must be processed through alternative paths without blocking the main pipeline.",
    question: "Which event-driven architecture provides ROBUST pipeline processing with error recovery?",
    options: [
        "Use EventBridge with multiple custom buses, dead letter queues, and Step Functions for error workflows",
        "Implement Kinesis Data Streams with Lambda consumers, error queues, and replay mechanisms",
        "Use SQS with multiple queues per stage and Lambda functions with custom retry logic",
        "Deploy Step Functions with parallel processing and built-in error handling"
    ],
    correct: 0,
    explanation: {
        correct: "EventBridge custom buses provide stage isolation. Dead letter queues capture failed events. Step Functions handle complex error recovery workflows without blocking main pipeline.",
        whyWrong: {
            1: "Kinesis replay is complex and may reprocess successful records when handling errors",
            2: "SQS approach requires custom orchestration and doesn't provide the event routing flexibility needed",
            3: "Step Functions alone don't provide the event-driven architecture needed for streaming data pipeline"
        },
        examStrategy: "Event-driven pipeline: EventBridge custom buses + DLQ + Step Functions error workflows for robust processing."
    }
},
{
    id: 'dva_216',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function must cache expensive computation results that are shared across multiple function invocations and execution contexts. Cache hits should respond in microseconds, and cache updates should be atomic to prevent race conditions.",
    question: "What caching strategy provides SHARED cache with atomic updates across Lambda executions?",
    options: [
        "Use ElastiCache Redis with atomic operations and connection pooling from Lambda",
        "Implement in-memory caching in Lambda global variables with DynamoDB persistence",
        "Use DynamoDB with conditional writes and Lambda environment variable caching",
        "Store cache data in S3 with Lambda local file system caching"
    ],
    correct: 0,
    explanation: {
        correct: "Redis provides microsecond access times and atomic operations (SETNX, transactions) for race-free updates. Connection pooling optimizes Lambda connection management.",
        whyWrong: {
            1: "Global variables don't share cache across different execution contexts and lose data during scaling",
            2: "DynamoDB has higher latency than Redis and environment variables don't provide shared caching",
            3: "S3 has seconds-level latency, not microseconds, and doesn't provide atomic update operations"
        },
        examStrategy: "Shared Lambda cache: ElastiCache Redis for microsecond access + atomic operations + connection pooling."
    }
},
{
    id: 'dva_217',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A content recommendation engine processes user behavior events in real-time and must update personalized recommendation models for millions of users. The system requires low-latency inference (< 10ms), frequent model updates, and A/B testing of different recommendation algorithms.",
    question: "Which architecture supports REAL-TIME personalization with model experimentation?",
    options: [
        "Use Lambda with pre-loaded models in memory, EventBridge for model updates, and feature flags for A/B testing",
        "Implement SageMaker real-time endpoints with Lambda orchestration and CloudWatch for experimentation tracking",
        "Use Lambda with models in S3, ElastiCache for user features, and Parameter Store for experiment configuration",
        "Deploy ECS with model serving containers, Kinesis for events, and custom A/B testing logic"
    ],
    correct: 0,
    explanation: {
        correct: "Pre-loaded models in Lambda memory provide sub-millisecond inference. EventBridge enables real-time model propagation. Feature flags enable sophisticated A/B testing with immediate configuration changes.",
        whyWrong: {
            1: "SageMaker endpoints have higher latency than in-memory Lambda models and add infrastructure cost",
            2: "S3 model loading adds latency and doesn't support the < 10ms inference requirement",
            3: "ECS adds container orchestration overhead compared to serverless Lambda for real-time inference"
        },
        examStrategy: "Real-time ML inference: Lambda in-memory models + EventBridge updates + feature flags for A/B testing."
    }
},
{
    id: 'dva_218',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A serverless application processes document workflows with human approval steps. Documents can be in various stages (draft, review, approved, rejected) and transitions must be tracked for audit purposes. Some approvals require multiple reviewers with quorum rules.",
    question: "What workflow pattern handles COMPLEX approval processes with audit requirements?",
    options: [
        "Use Step Functions with human tasks, DynamoDB for state tracking, and EventBridge for audit logging",
        "Implement SQS with Lambda processors and DynamoDB for workflow state management",
        "Use DynamoDB Streams with Lambda triggers for state transitions and CloudTrail for auditing",
        "Deploy custom workflow engine with Lambda and S3 for document storage and audit trails"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions human tasks handle approval workflows with timeouts. DynamoDB tracks complex state transitions. EventBridge provides comprehensive audit event routing and persistence.",
        whyWrong: {
            1: "SQS lacks the workflow orchestration capabilities needed for complex approval processes",
            2: "DynamoDB Streams are reactive and don't provide the proactive workflow orchestration needed",
            3: "Custom workflow engine adds development overhead compared to Step Functions built-in workflow capabilities"
        },
        examStrategy: "Complex approval workflows: Step Functions human tasks + DynamoDB state + EventBridge audit logging."
    }
},
{
    id: 'dva_219',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A Lambda function processes real-time sensor data and must detect anomalies using multiple detection algorithms (statistical, ML-based, rule-based). Each algorithm has different computational requirements and accuracy characteristics. The system must combine results intelligently.",
    question: "Which pattern provides MULTI-ALGORITHM anomaly detection with intelligent result fusion?",
    options: [
        "Use Step Functions parallel execution with Lambda functions per algorithm and result aggregation logic",
        "Implement EventBridge with algorithm-specific Lambda consumers and DynamoDB for result correlation",
        "Use SQS with multiple queues per algorithm and Lambda functions for result combination",
        "Deploy single Lambda function with all algorithms and weighted voting logic"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions parallel execution optimizes performance by running algorithms concurrently. Result aggregation logic can implement sophisticated voting, weighting, and confidence scoring.",
        whyWrong: {
            1: "EventBridge routing adds latency and complexity for real-time anomaly detection requirements",
            2: "SQS queues add latency and don't provide the orchestration needed for result combination",
            3: "Single Lambda function limits scalability and doesn't optimize for different algorithm requirements"
        },
        examStrategy: "Multi-algorithm processing: Step Functions parallel execution + Lambda per algorithm + intelligent result aggregation."
    }
},
{
    id: 'dva_220',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function must integrate with a legacy SOAP web service that requires client certificates, custom headers, and session management. The web service is slow (5-10 seconds response time) and occasionally unreliable.",
    question: "What integration approach handles LEGACY SOAP services with reliability requirements?",
    options: [
        "Use Lambda with custom HTTP client, connection pooling, circuit breaker pattern, and certificate management",
        "Implement API Gateway with VPC Link to legacy service and Lambda for response processing",
        "Use Step Functions with retry logic and Lambda for SOAP envelope construction",
        "Deploy Lambda in VPC with NAT Gateway and direct SOAP client integration"
    ],
    correct: 0,
    explanation: {
        correct: "Custom HTTP client handles SOAP specifics, certificates, and sessions. Connection pooling optimizes performance. Circuit breaker prevents cascade failures from unreliable legacy service.",
        whyWrong: {
            1: "API Gateway doesn't handle SOAP protocol specifics like envelope construction and session management",
            2: "Step Functions help with retries but don't solve the SOAP client integration challenges",
            3: "VPC deployment alone doesn't address SOAP protocol handling, certificates, and reliability patterns"
        },
        examStrategy: "Legacy SOAP integration: Lambda custom HTTP client + connection pooling + circuit breaker for reliability."
    }
},
{
    id: 'dva_221',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A global application uses DynamoDB Global Tables across 6 regions. The application must handle write conflicts gracefully, maintain eventual consistency for reads, and provide strong consistency for critical operations like financial transactions.",
    question: "Which DynamoDB Global Tables strategy balances PERFORMANCE with consistency requirements?",
    options: [
        "Use conditional writes with version numbers, strongly consistent reads for critical operations, and conflict resolution logic",
        "Implement write sharding across regions with application-level coordination and conflict detection",
        "Use DynamoDB Streams for cross-region synchronization with custom conflict resolution",
        "Designate primary write region for critical operations with read replicas for global access"
    ],
    correct: 0,
    explanation: {
        correct: "Conditional writes with version numbers prevent conflicts. Strongly consistent reads ensure accuracy for financial operations. Conflict resolution handles concurrent updates gracefully.",
        whyWrong: {
            1: "Write sharding with application coordination adds complexity without leveraging Global Tables built-in conflict resolution",
            2: "DynamoDB Streams approach duplicates Global Tables functionality and adds complexity",
            3: "Primary write region defeats the purpose of Global Tables multi-region write capability"
        },
        examStrategy: "Global Tables consistency: Conditional writes + version numbers + strongly consistent reads for critical operations."
    }
},
{
    id: 'dva_222',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function processes batch jobs that can take 5-14 minutes to complete. The function must provide progress updates to users, handle interruptions gracefully, and support job cancellation by users.",
    question: "What pattern supports LONG-RUNNING batch jobs with user interaction capabilities?",
    options: [
        "Use Step Functions with Lambda and DynamoDB for progress tracking and cancellation signals",
        "Implement SQS with Lambda polling and WebSocket API for real-time progress updates",
        "Use ECS Fargate tasks with EventBridge for progress events and Lambda for coordination",
        "Deploy Lambda with SNS for progress notifications and SQS for cancellation requests"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions handle long-running workflows with built-in state management. DynamoDB tracks progress and cancellation signals. Step Functions can be stopped for user cancellation.",
        whyWrong: {
            1: "SQS polling doesn't provide the workflow orchestration needed for complex batch job management",
            2: "ECS adds infrastructure complexity compared to serverless Step Functions for batch job orchestration",
            3: "SNS/SQS approach lacks the workflow state management needed for job progress and cancellation"
        },
        examStrategy: "Long-running batch jobs: Step Functions workflow + DynamoDB progress tracking + cancellation signal handling."
    }
},
{
    id: 'dva_223',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A serverless application implements a distributed lock mechanism for coordinating access to shared resources across multiple Lambda functions. Locks must have automatic expiration, be atomic, and handle high contention scenarios efficiently.",
    question: "Which distributed locking approach provides ATOMIC operations with high contention handling?",
    options: [
        "Use DynamoDB conditional writes with TTL for lock expiration and exponential backoff for contention",
        "Implement Redis SET with NX and EX options for atomic locking with automatic expiration",
        "Use S3 object creation for locks with lifecycle policies for automatic cleanup",
        "Implement custom locking with EventBridge and Lambda for lock coordination"
    ],
    correct: 1,
    explanation: {
        correct: "Redis SET NX EX provides atomic lock acquisition with expiration in a single operation. Redis handles high contention efficiently with sub-millisecond response times.",
        whyWrong: {
            0: "DynamoDB conditional writes work but have higher latency than Redis and consume write capacity under contention",
            2: "S3 has eventual consistency and high latency, unsuitable for distributed locking requirements",
            3: "Custom EventBridge coordination adds complexity and latency compared to purpose-built atomic operations"
        },
        examStrategy: "Distributed locking: Redis SET NX EX for atomic lock acquisition + expiration in single operation."
    }
},
{
    id: 'dva_224',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A Lambda function processes user-generated content and must scan for inappropriate content, extract metadata, generate thumbnails, and update search indexes. Processing steps have dependencies and different failure recovery requirements.",
    question: "What orchestration approach handles CONTENT processing with differential error handling?",
    options: [
        "Use Step Functions with Catch blocks for step-specific error handling and Parallel states for independent operations",
        "Implement EventBridge with retry policies and dead letter queues for each processing step",
        "Use SQS with separate queues for each processing stage and Lambda error handling",
        "Deploy Lambda with SNS fan-out and custom retry logic for each content processing step"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions Catch blocks enable step-specific error handling. Parallel states optimize independent operations. Built-in retry and error state management for complex workflows.",
        whyWrong: {
            1: "EventBridge lacks the workflow orchestration needed for dependent processing steps",
            2: "SQS queues require custom orchestration and don't provide dependency management",
            3: "SNS fan-out doesn't handle dependencies between processing steps"
        },
        examStrategy: "Content processing workflow: Step Functions with Catch blocks + Parallel states + step-specific error handling."
    }
},
{
    id: 'dva_225',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A trading platform requires sub-millisecond order matching with strict ordering guarantees. Orders arrive at high frequency (50,000/second) and must be matched against a constantly changing order book. The system must handle partial fills and order modifications.",
    question: "Which architecture achieves SUB-MILLISECOND order matching with strict ordering?",
    options: [
        "Use Lambda with in-memory order book, Kinesis for order streams, and DynamoDB for persistence",
        "Implement Redis with sorted sets for order book management and Lua scripts for atomic matching",
        "Use DynamoDB with conditional writes for order matching and Lambda for processing logic",
        "Deploy ECS with custom order matching engine and Kinesis for order stream processing"
    ],
    correct: 1,
    explanation: {
        correct: "Redis sorted sets maintain ordered price levels efficiently. Lua scripts provide atomic order matching operations. Sub-millisecond response times for high-frequency trading requirements.",
        whyWrong: {
            0: "Lambda has cold start issues and doesn't provide the consistent sub-millisecond performance needed",
            2: "DynamoDB latency is higher than Redis and conditional writes don't provide the complex matching logic needed",
            3: "ECS adds container orchestration overhead compared to optimized Redis data structures for order matching"
        },
        examStrategy: "Sub-millisecond order matching: Redis sorted sets + Lua scripts for atomic operations + consistent performance."
    }
},
{
    id: 'dva_226',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function integrates with multiple microservices that may be temporarily unavailable. The function must continue operating with degraded functionality when services are down and automatically recover when services return.",
    question: "What resilience pattern provides GRACEFUL degradation with automatic recovery?",
    options: [
        "Implement circuit breaker pattern with health checks and fallback responses for each service",
        "Use exponential backoff with maximum retry limits and error logging for service calls",
        "Deploy service mesh with automatic failover and load balancing across service instances",
        "Implement timeout controls with default responses and manual service status checking"
    ],
    correct: 0,
    explanation: {
        correct: "Circuit breaker prevents cascade failures and provides automatic state management. Health checks enable automatic recovery. Fallback responses maintain functionality during outages.",
        whyWrong: {
            1: "Exponential backoff helps with transient issues but doesn't provide graceful degradation",
            2: "Service mesh benefits don't apply to Lambda calling external microservices",
            3: "Timeout controls help but don't provide automatic recovery and graceful degradation capabilities"
        },
        examStrategy: "Service resilience: Circuit breaker pattern + health checks + fallback responses for graceful degradation."
    }
},
{
    id: 'dva_227',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A serverless application implements event sourcing with DynamoDB for event storage. The system must replay events to rebuild aggregate state, support temporal queries, and handle high write throughput for events while maintaining query performance.",
    question: "Which event sourcing pattern optimizes BOTH write throughput and query performance?",
    options: [
        "Use DynamoDB with partition key as aggregate ID, sort key as timestamp, and GSI for temporal queries",
        "Implement separate DynamoDB tables for events and snapshots with Lambda for state reconstruction",
        "Use Kinesis Data Streams for events with DynamoDB projections and Lambda for query processing",
        "Deploy event store in S3 with DynamoDB indexes and Lambda for event replay functionality"
    ],
    correct: 1,
    explanation: {
        correct: "Separate event and snapshot tables optimize for different access patterns. Snapshots reduce replay time. Lambda reconstruction provides flexibility for complex aggregate logic.",
        whyWrong: {
            0: "Single table design limits query flexibility and may create hot partitions for high-volume aggregates",
            2: "Kinesis adds streaming complexity and doesn't provide the persistence guarantees needed for event sourcing",
            3: "S3 event storage has higher latency and doesn't provide the transactional guarantees needed"
        },
        examStrategy: "Event sourcing optimization: Separate event/snapshot tables + Lambda reconstruction for performance and flexibility."
    }
},
{
    id: 'dva_228',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function processes webhook events from external systems and must handle duplicate events, out-of-order delivery, and varying payload formats from different vendors. Event processing must be idempotent.",
    question: "What webhook processing approach ensures IDEMPOTENT handling with format flexibility?",
    options: [
        "Use DynamoDB for event deduplication with TTL, payload transformation Lambda layers, and conditional processing",
        "Implement Redis for duplicate detection with vendor-specific processing logic and standardized output format",
        "Use SQS FIFO for event ordering with Lambda transformation and DynamoDB for processed event tracking",
        "Deploy API Gateway with request validation and Lambda with vendor-specific processors"
    ],
    correct: 0,
    explanation: {
        correct: "DynamoDB with TTL provides efficient deduplication. Lambda layers enable reusable payload transformation. Conditional processing ensures idempotency for varying formats.",
        whyWrong: {
            1: "Redis deduplication works but Lambda layers provide better code organization for vendor-specific formats",
            2: "SQS FIFO doesn't solve idempotency for duplicate events and adds unnecessary ordering overhead",
            3: "API Gateway validation doesn't handle the vendor format diversity and idempotency requirements"
        },
        examStrategy: "Idempotent webhook processing: DynamoDB deduplication + TTL + Lambda layers for format transformation."
    }
},
{
    id: 'dva_229',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A real-time recommendation system must update user profiles based on behavior events, recalculate recommendations using multiple algorithms, and serve personalized content with sub-100ms latency for 1 million concurrent users.",
    question: "Which architecture supports REAL-TIME personalization at massive scale with low latency?",
    options: [
        "Use Kinesis for events, Lambda for profile updates, Redis for user data, and pre-computed recommendations in DynamoDB",
        "Implement EventBridge for behavior routing, DynamoDB for profiles, and Lambda with machine learning inference",
        "Use SQS for event processing, ElastiCache for user profiles, and API Gateway caching for recommendations",
        "Deploy Kafka for events, Lambda for processing, and in-memory recommendation serving with ECS"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis handles high-throughput events. Lambda enables real-time profile updates. Redis provides sub-millisecond access. Pre-computed DynamoDB recommendations serve 1M users efficiently.",
        whyWrong: {
            1: "EventBridge routing adds latency for real-time requirements and doesn't optimize for high throughput",
            2: "SQS has delivery delays and API Gateway caching doesn't provide personalized recommendation caching",
            3: "Kafka adds infrastructure complexity and ECS containers have higher latency than optimized Redis access"
        },
        examStrategy: "Massive scale personalization: Kinesis events + Lambda updates + Redis profiles + pre-computed DynamoDB recommendations."
    }
},
{
    id: 'dva_230',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A Lambda function must coordinate distributed transactions across multiple databases (DynamoDB, RDS, external APIs) with ACID properties. Some operations may take several minutes to complete and require compensation on failure.",
    question: "What distributed transaction pattern provides ACID properties with compensation capabilities?",
    options: [
        "Use Step Functions to implement Saga pattern with compensation actions and transaction coordination",
        "Implement two-phase commit protocol with Lambda coordination and database locks",
        "Use DynamoDB transactions with Lambda orchestration for cross-database coordination",
        "Deploy custom transaction manager with SQS for coordination and manual rollback procedures"
    ],
    correct: 0,
    explanation: {
        correct: "Saga pattern provides distributed transaction semantics with compensation. Step Functions orchestrate long-running transactions. Compensation actions ensure consistency on failure.",
        whyWrong: {
            1: "Two-phase commit is complex, error-prone, and doesn't work well with external APIs and long durations",
            2: "DynamoDB transactions don't extend to RDS and external APIs, limiting cross-database coordination",
            3: "Custom transaction manager adds development complexity compared to proven Saga pattern implementation"
        },
        examStrategy: "Distributed transactions: Step Functions Saga pattern + compensation actions for ACID across multiple systems."
    }
},
{
    id: 'dva_231',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A serverless application processes video content with multiple processing pipelines (transcoding, thumbnail generation, content analysis, metadata extraction). Pipelines have different resource requirements and can run independently or in sequence based on content type.",
    question: "Which orchestration approach optimizes VIDEO processing with dynamic pipeline selection?",
    options: [
        "Use Step Functions with Choice states for pipeline selection and Map states for parallel processing of video segments",
        "Implement EventBridge with content-type routing to specialized Lambda functions for each pipeline",
        "Use SQS with separate queues for each pipeline and Lambda functions with custom orchestration logic",
        "Deploy MediaConvert with Lambda triggers and custom workflow coordination"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions Choice states enable dynamic pipeline selection based on content type. Map states optimize parallel segment processing. Built-in error handling for complex video workflows.",
        whyWrong: {
            1: "EventBridge routing lacks the workflow orchestration needed for sequential pipeline dependencies",
            2: "SQS approach requires custom orchestration and doesn't provide dynamic pipeline selection",
            3: "MediaConvert is for transcoding only and doesn't handle the full multi-pipeline orchestration needed"
        },
        examStrategy: "Video processing orchestration: Step Functions Choice states + Map states for dynamic pipelines + parallel processing."
    }
},
{
    id: 'dva_232',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function implements a rate-limited API client that must respect multiple rate limits (per-second, per-minute, per-hour) while maximizing throughput. The client serves multiple concurrent Lambda invocations.",
    question: "What rate limiting approach handles MULTIPLE time windows with shared state across invocations?",
    options: [
        "Use Redis with sliding window counters and atomic increment operations for precise rate limiting",
        "Implement DynamoDB with TTL-based counters and conditional writes for rate tracking",
        "Use SQS with delayed messages and Lambda concurrency controls for rate management",
        "Store rate limits in Lambda environment variables with in-memory tracking"
    ],
    correct: 0,
    explanation: {
        correct: "Redis sliding window counters handle multiple time windows efficiently. Atomic increments ensure accurate counting across concurrent invocations. Sub-millisecond performance for rate checks.",
        whyWrong: {
            1: "DynamoDB has higher latency than Redis and TTL cleanup isn't precise enough for rate limiting",
            2: "SQS delayed messages don't provide the real-time rate limiting precision needed",
            3: "Environment variables don't share state across different Lambda execution contexts"
        },
        examStrategy: "Multi-window rate limiting: Redis sliding window counters + atomic operations for shared state across invocations."
    }
},
{
    id: 'dva_233',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A financial application must implement audit logging with cryptographic integrity, ensuring logs cannot be tampered with or deleted. The system processes millions of transactions daily and must provide real-time audit queries.",
    question: "Which audit logging approach provides CRYPTOGRAPHIC integrity with real-time query capabilities?",
    options: [
        "Use DynamoDB with hash chaining for integrity verification and GSI for real-time queries",
        "Implement blockchain-based logging with Lambda and S3 for immutable audit trails",
        "Use CloudWatch Logs with log file validation and ElastiSearch for real-time search",
        "Deploy custom audit service with cryptographic signatures and RDS for query performance"
    ],
    correct: 0,
    explanation: {
        correct: "DynamoDB hash chaining creates cryptographic integrity chain. GSI enables real-time queries on audit data. Scales to millions of transactions with consistent performance.",
        whyWrong: {
            1: "Blockchain adds enormous complexity and performance overhead for audit logging",
            2: "CloudWatch Logs lack the cryptographic integrity verification needed for financial audit requirements",
            3: "Custom audit service adds development overhead compared to DynamoDB's proven scalability"
        },
        examStrategy: "Cryptographic audit logging: DynamoDB hash chaining + GSI for integrity verification + real-time queries."
    }
},
{
    id: 'dva_234',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function processes IoT device commands and must ensure exactly-once delivery to devices, handle offline devices gracefully, and track command status across the entire lifecycle.",
    question: "What command processing pattern ensures EXACTLY-ONCE delivery with offline device handling?",
    options: [
        "Use DynamoDB for command tracking with TTL, SQS for device queues, and Lambda for retry logic",
        "Implement IoT Device Management with command tracking and automatic retry mechanisms",
        "Use Step Functions for command workflows with device state tracking and timeout handling",
        "Deploy custom command queue with Redis and Lambda for device communication"
    ],
    correct: 1,
    explanation: {
        correct: "IoT Device Management provides built-in command delivery guarantees, offline device handling, and status tracking. Purpose-built for exactly-once IoT command scenarios.",
        whyWrong: {
            0: "DynamoDB tracking requires custom exactly-once logic and doesn't handle IoT device connectivity",
            2: "Step Functions add workflow overhead for simple command delivery requirements",
            3: "Custom queue implementation duplicates IoT Device Management capabilities"
        },
        examStrategy: "IoT command delivery: AWS IoT Device Management for exactly-once delivery + offline handling + status tracking."
    }
},
{
    id: 'dva_235',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A serverless application implements complex business rules that change frequently. Rules are expressed in domain-specific language (DSL), must be evaluated in real-time with sub-10ms latency, and support complex conditions with nested logic.",
    question: "Which rule engine approach provides FAST evaluation with DSL flexibility?",
    options: [
        "Use Lambda with compiled rule expressions cached in memory and Redis for rule storage",
        "Implement rule evaluation service with DynamoDB storage and Lambda for rule compilation",
        "Use Step Functions with rule-based Choice states and parameter-driven decision logic",
        "Deploy custom JavaScript engine in Lambda with rule sandboxing and compilation caching"
    ],
    correct: 0,
    explanation: {
        correct: "Compiled rule expressions in Lambda memory provide sub-10ms evaluation. Redis stores and distributes rules efficiently. Memory caching eliminates compilation overhead for real-time performance.",
        whyWrong: {
            1: "DynamoDB rule storage and Lambda compilation add latency that violates sub-10ms requirements",
            2: "Step Functions Choice states lack the DSL flexibility and have higher latency than in-memory evaluation",
            3: "Custom JavaScript engine adds development complexity compared to established rule compilation techniques"
        },
        examStrategy: "Fast rule evaluation: Lambda compiled expressions + memory caching + Redis rule distribution for sub-10ms performance."
    }
},
{
    id: 'dva_236',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A Lambda function must coordinate background tasks across multiple services with different completion times and failure rates. Tasks can be cancelled, rescheduled, or have dependencies on other tasks.",
    question: "What task coordination approach handles COMPLEX task relationships with cancellation support?",
    options: [
        "Use Step Functions with task tokens for external coordination and cancellation capabilities",
        "Implement SQS with delayed messages and Lambda for task scheduling and dependency tracking",
        "Use EventBridge scheduled events with Lambda for task execution and DynamoDB for coordination",
        "Deploy custom task scheduler with DynamoDB and Lambda for complex dependency management"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions task tokens enable external task coordination. Built-in cancellation support. Task dependencies handled through workflow design. Comprehensive task state management.",
        whyWrong: {
            1: "SQS lacks the task dependency coordination and cancellation capabilities needed",
            2: "EventBridge scheduled events don't provide task dependency management and cancellation",
            3: "Custom scheduler duplicates Step Functions capabilities and adds development overhead"
        },
        examStrategy: "Complex task coordination: Step Functions task tokens + cancellation + dependency management through workflow design."
    }
},
{
    id: 'dva_237',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A real-time analytics platform aggregates streaming data from multiple sources with different schemas and update frequencies. The system must maintain materialized views with eventual consistency and support complex analytical queries.",
    question: "Which streaming analytics approach provides MATERIALIZED views with schema flexibility?",
    options: [
        "Use Kinesis Data Analytics with Lambda output to DynamoDB for materialized view updates",
        "Implement Kinesis Data Streams with Lambda consumers and DynamoDB for view storage",
        "Use EventBridge with Lambda aggregators and RDS for complex analytical query support",
        "Deploy Kinesis Data Firehose with S3 storage and Athena for analytical processing"
    ],
    correct: 1,
    explanation: {
        correct: "Kinesis Data Streams handle multiple sources with different schemas. Lambda consumers provide flexible aggregation logic. DynamoDB materialized views support eventual consistency and complex queries.",
        whyWrong: {
            0: "Kinesis Data Analytics has limited schema flexibility and SQL-based processing constraints",
            2: "EventBridge adds routing overhead and RDS doesn't optimize for materialized view update patterns",
            3: "Kinesis Firehose/Athena approach is batch-oriented, not real-time streaming analytics"
        },
        examStrategy: "Streaming materialized views: Kinesis Data Streams + Lambda flexible aggregation + DynamoDB view storage."
    }
},
{
    id: 'dva_238',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function processes financial market data and must maintain strict ordering of trades within each security while allowing parallel processing across different securities. Trade processing includes validation, enrichment, and persistence.",
    question: "What processing pattern maintains STRICT ordering per security with parallel processing?",
    options: [
        "Use Kinesis Data Streams with partition key as security ID and Lambda consumers for ordered processing",
        "Implement SQS FIFO queues per security with Lambda functions for trade processing",
        "Use EventBridge with routing rules by security and Lambda for parallel processing",
        "Deploy DynamoDB with conditional writes and Lambda for trade validation and ordering"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis partitioning by security ID ensures strict ordering within each security. Lambda consumers can process different securities in parallel. Built-in ordering guarantees.",
        whyWrong: {
            1: "SQS FIFO per security creates management overhead and doesn't scale efficiently to many securities",
            2: "EventBridge routing doesn't provide ordering guarantees within security processing",
            3: "DynamoDB conditional writes don't inherently provide processing order guarantees"
        },
        examStrategy: "Per-entity ordering: Kinesis partitioning by entity ID for strict ordering + parallel processing across entities."
    }
},
{
    id: 'dva_239',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A serverless application implements a message delivery system with priority queues, dead letter handling, and message scheduling. Messages can have different priorities, delivery delays, and retry policies based on message type.",
    question: "Which message system architecture supports PRIORITY queuing with flexible delivery policies?",
    options: [
        "Use multiple SQS queues with priority-based Lambda polling and custom scheduling logic",
        "Implement Redis with priority queues and Lambda with custom message delivery coordination",
        "Use EventBridge with priority routing rules and Lambda consumers with retry logic",
        "Deploy Step Functions with message scheduling and priority-based workflow execution"
    ],
    correct: 1,
    explanation: {
        correct: "Redis priority queues (sorted sets) provide efficient priority ordering. Custom Lambda coordination enables flexible delivery policies. Sub-millisecond message retrieval performance.",
        whyWrong: {
            0: "Multiple SQS queues create management overhead and don't provide true priority queue semantics",
            2: "EventBridge routing doesn't provide priority queue semantics and adds routing latency",
            3: "Step Functions are designed for workflows, not message queue management with priority semantics"
        },
        examStrategy: "Priority message queuing: Redis sorted sets for priority ordering + Lambda coordination for flexible policies."
    }
},
{
    id: 'dva_240',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function implements a caching layer for expensive database queries with intelligent cache warming, TTL management, and cache invalidation based on data changes.",
    question: "What caching strategy provides INTELLIGENT warming with change-based invalidation?",
    options: [
        "Use ElastiCache Redis with TTL, DynamoDB Streams for invalidation triggers, and Lambda for cache warming",
        "Implement DynamoDB cache with TTL attributes and Lambda triggers for cache management",
        "Use API Gateway caching with Lambda cache control headers and custom invalidation logic",
        "Deploy custom cache in Lambda memory with periodic refresh and manual invalidation"
    ],
    correct: 0,
    explanation: {
        correct: "Redis provides high-performance caching with TTL support. DynamoDB Streams trigger cache invalidation on data changes. Lambda enables intelligent cache warming strategies.",
        whyWrong: {
            1: "DynamoDB as cache is less efficient than Redis and doesn't provide the performance benefits needed",
            2: "API Gateway caching doesn't provide the intelligent warming and fine-grained invalidation needed",
            3: "Lambda memory cache doesn't persist across executions and lacks distributed cache benefits"
        },
        examStrategy: "Intelligent caching: ElastiCache Redis + DynamoDB Streams invalidation + Lambda warming for optimal performance."
    }
},
{
    id: 'dva_241',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A real-time collaboration platform handles document editing with operational transformation, conflict resolution, and presence awareness for thousands of concurrent users per document. Changes must be synchronized in real-time with guaranteed delivery.",
    question: "Which collaboration architecture provides OPERATIONAL transformation with guaranteed real-time delivery?",
    options: [
        "Use API Gateway WebSocket with Lambda, DynamoDB for document state, and operational transformation algorithms",
        "Implement Kinesis Data Streams for operations, Lambda for transformation, and ElastiCache for document state",
        "Use EventBridge for operation routing with Lambda processors and DynamoDB Streams for change propagation",
        "Deploy custom WebSocket service with Redis for document state and operational transformation logic"
    ],
    correct: 0,
    explanation: {
        correct: "WebSocket API provides real-time bidirectional communication. Lambda handles operational transformation algorithms. DynamoDB provides consistent document state with strong consistency options.",
        whyWrong: {
            1: "Kinesis is designed for streaming analytics, not real-time collaborative editing with bidirectional communication",
            2: "EventBridge routing adds latency for real-time collaboration and doesn't provide bidirectional communication",
            3: "Custom WebSocket service adds infrastructure complexity compared to managed API Gateway WebSocket"
        },
        examStrategy: "Real-time collaboration: API Gateway WebSocket + Lambda operational transformation + DynamoDB document state."
    }
},
{
    id: 'dva_242',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A Lambda function processes user uploads and must scan files for viruses, extract metadata, validate formats, and store results. Some processing steps can fail independently and should not block other processing.",
    question: "What processing pattern allows INDEPENDENT step failures without blocking other operations?",
    options: [
        "Use EventBridge with separate event types for each processing step and independent Lambda consumers",
        "Implement Step Functions with Parallel states and Continue On Error for independent processing",
        "Use SQS with separate queues for each processing step and Lambda error handling",
        "Deploy Lambda with SNS fan-out pattern and dead letter queues for failed operations"
    ],
    correct: 0,
    explanation: {
        correct: "EventBridge event types enable independent processing steps. Separate Lambda consumers can fail independently. Failed events can be retried without affecting successful operations.",
        whyWrong: {
            1: "Step Functions Parallel states don't continue on error by default and couple step failures",
            2: "SQS requires custom coordination and doesn't provide independent failure handling",
            3: "SNS fan-out doesn't provide the event-driven independence needed for processing steps"
        },
        examStrategy: "Independent processing steps: EventBridge event types + separate Lambda consumers for isolated failure handling."
    }
},
{
    id: 'dva_243',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A serverless application implements a recommendation engine that must process user behavior in real-time, update machine learning models incrementally, and serve personalized recommendations with sub-100ms latency for millions of users.",
    question: "Which ML architecture provides REAL-TIME learning with ultra-low latency serving?",
    options: [
        "Use Kinesis for behavior events, Lambda for incremental learning, and Redis for model serving with pre-computed recommendations",
        "Implement SageMaker real-time endpoints with Lambda orchestration and DynamoDB for user features",
        "Use EventBridge for behavior routing, Lambda for model updates, and API Gateway caching for recommendations",
        "Deploy custom ML service with ECS, Kinesis for events, and ElastiCache for recommendation serving"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis handles high-throughput behavior events. Lambda enables real-time incremental learning. Redis serves pre-computed recommendations with sub-millisecond latency for millions of users.",
        whyWrong: {
            1: "SageMaker endpoints have higher latency than pre-computed recommendations and don't provide incremental learning",
            2: "EventBridge adds routing latency and API Gateway caching doesn't provide personalized recommendation caching",
            3: "ECS adds container overhead compared to serverless Lambda for real-time ML processing"
        },
        examStrategy: "Real-time ML recommendations: Kinesis events + Lambda incremental learning + Redis pre-computed serving."
    }
},
{
    id: 'dva_244',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function coordinates microservices calls with circuit breaker patterns, bulkhead isolation, and timeout controls. The function must maintain service health state and adapt behavior based on real-time service performance.",
    question: "What microservices coordination approach provides ADAPTIVE resilience with health state management?",
    options: [
        "Use Lambda with Redis for circuit breaker state, health check scheduling, and adaptive timeout controls",
        "Implement custom service mesh with Lambda, DynamoDB for health state, and EventBridge for coordination",
        "Use Step Functions for service coordination with retry policies and timeout configurations",
        "Deploy API Gateway with Lambda authorizers and custom health check logic"
    ],
    correct: 0,
    explanation: {
        correct: "Redis stores circuit breaker state across Lambda invocations. Health check scheduling enables proactive monitoring. Adaptive timeouts based on real-time performance data.",
        whyWrong: {
            1: "Custom service mesh adds complexity and DynamoDB has higher latency than Redis for circuit breaker state",
            2: "Step Functions are for workflows, not real-time circuit breaker state management",
            3: "API Gateway authorizers don't provide the microservices coordination and resilience patterns needed"
        },
        examStrategy: "Adaptive microservices resilience: Lambda + Redis circuit breaker state + health scheduling + adaptive timeouts."
    }
},
{
    id: 'dva_245',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A financial trading application must implement order management with partial fills, order modifications, and real-time risk checks. Orders have complex lifecycle states and must maintain audit trails for regulatory compliance.",
    question: "Which order management pattern provides COMPLEX lifecycle handling with regulatory audit requirements?",
    options: [
        "Use DynamoDB with item collections for order state, DynamoDB Streams for audit trails, and Lambda for risk processing",
        "Implement event sourcing with DynamoDB events, Lambda for state reconstruction, and S3 for audit storage",
        "Use Step Functions for order workflows with DynamoDB state storage and CloudTrail for audit logging",
        "Deploy custom order management with RDS for state, Lambda for processing, and separate audit database"
    ],
    correct: 1,
    explanation: {
        correct: "Event sourcing captures all order state changes for audit compliance. Lambda reconstruction enables complex state transitions. S3 provides immutable audit storage with lifecycle management.",
        whyWrong: {
            0: "DynamoDB item collections help but don't provide the immutable audit trail needed for regulatory compliance",
            2: "Step Functions workflows don't capture the detailed state change audit trail required for financial compliance",
            3: "Custom solution with RDS adds complexity compared to event sourcing pattern for audit and state management"
        },
        examStrategy: "Financial order management: Event sourcing + Lambda state reconstruction + S3 audit storage for compliance."
    }
},
{
    id: 'dva_246',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function processes real-time sensor readings and must detect patterns across multiple sensors, maintain rolling statistics, and trigger alerts based on complex conditions involving multiple time windows.",
    question: "What pattern recognition approach handles MULTI-SENSOR analysis with rolling statistics?",
    options: [
        "Use Redis with time series data structures and Lua scripts for complex pattern detection across sensors",
        "Implement DynamoDB with sensor data partitioning and Lambda for cross-sensor analysis",
        "Use Kinesis Data Analytics with windowing functions and Lambda for alert processing",
        "Deploy custom analytics with ElastiCache and Lambda for real-time pattern processing"
    ],
    correct: 0,
    explanation: {
        correct: "Redis time series provide efficient rolling statistics. Lua scripts enable complex cross-sensor pattern detection with atomic operations. Sub-millisecond pattern analysis performance.",
        whyWrong: {
            1: "DynamoDB partitioning doesn't optimize for cross-sensor analysis and has higher latency than Redis",
            2: "Kinesis Analytics windowing is limited compared to Redis time series flexibility for complex patterns",
            3: "Custom analytics with ElastiCache duplicates Redis time series capabilities without added benefits"
        },
        examStrategy: "Multi-sensor pattern detection: Redis time series + Lua scripts for cross-sensor analysis + rolling statistics."
    }
},
{
    id: 'dva_247',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A global content distribution platform must route user requests to optimal processing regions based on content type, user location, processing capacity, and real-time latency measurements. Routing decisions must adapt dynamically.",
    question: "Which global routing architecture provides DYNAMIC optimization with real-time adaptation?",
    options: [
        "Use Route 53 with health checks, Lambda@Edge for routing logic, and DynamoDB Global Tables for routing state",
        "Implement CloudFront with custom origins, Lambda for routing decisions, and ElastiCache for latency tracking",
        "Use API Gateway with regional endpoints, Lambda for load balancing, and Parameter Store for routing configuration",
        "Deploy Application Load Balancer with Lambda targets and custom routing algorithm"
    ],
    correct: 0,
    explanation: {
        correct: "Route 53 health checks provide real-time endpoint monitoring. Lambda@Edge enables dynamic routing logic at edge locations. Global Tables provide consistent routing state across regions.",
        whyWrong: {
            1: "CloudFront custom origins don't provide the dynamic routing flexibility needed for real-time optimization",
            2: "API Gateway regional endpoints don't provide global traffic routing optimization",
            3: "ALB with Lambda targets doesn't provide global routing capabilities across regions"
        },
        examStrategy: "Dynamic global routing: Route 53 health checks + Lambda@Edge routing + Global Tables state for real-time optimization."
    }
},
{
    id: 'dva_248',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A Lambda function implements a workflow engine that must execute user-defined workflows with conditional logic, loops, and parallel execution. Workflows are defined in JSON and can be modified at runtime.",
    question: "What workflow execution approach handles USER-DEFINED workflows with runtime modification capabilities?",
    options: [
        "Use Step Functions with dynamic workflow generation from JSON definitions and Lambda for custom logic",
        "Implement custom workflow interpreter in Lambda with DynamoDB for workflow state and JSON parsing",
        "Use EventBridge with workflow routing rules and Lambda functions for execution steps",
        "Deploy workflow engine with SQS coordination and Lambda for step processing"
    ],
    correct: 1,
    explanation: {
        correct: "Custom interpreter in Lambda provides flexibility for user-defined JSON workflows. DynamoDB state management enables runtime modifications. Full control over workflow semantics and execution.",
        whyWrong: {
            0: "Step Functions don't support dynamic workflow generation from JSON at runtime",
            2: "EventBridge routing lacks the workflow execution semantics needed for conditional logic and loops",
            3: "SQS coordination doesn't provide the workflow state management needed for complex user-defined workflows"
        },
        examStrategy: "User-defined workflows: Custom Lambda interpreter + DynamoDB state + JSON parsing for runtime flexibility."
    }
},
{
    id: 'dva_249',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A serverless application processes financial transactions with complex validation rules, fraud detection, and regulatory reporting. Each transaction type has different processing requirements and compliance needs.",
    question: "Which transaction processing architecture handles COMPLEX validation with regulatory compliance?",
    options: [
        "Use Step Functions with transaction-type routing, Lambda validators, and DynamoDB for compliance tracking",
        "Implement EventBridge with transaction routing, specialized Lambda processors, and S3 for regulatory storage",
        "Use SQS with transaction queues, Lambda validation, and RDS for compliance reporting",
        "Deploy custom transaction processor with Lambda, DynamoDB for state, and CloudWatch for audit logging"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions provide transaction workflow orchestration. Transaction-type routing enables specialized processing. DynamoDB compliance tracking ensures regulatory audit trails.",
        whyWrong: {
            1: "EventBridge routing lacks the transaction workflow orchestration needed for complex financial processing",
            2: "SQS queues don't provide the transaction state management and workflow orchestration required",
            3: "Custom processor duplicates Step Functions workflow capabilities without compliance-specific benefits"
        },
        examStrategy: "Complex transaction processing: Step Functions orchestration + type routing + DynamoDB compliance tracking."
    }
},
{
    id: 'dva_250',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function coordinates batch processing jobs with dependencies, resource allocation, and priority scheduling. Jobs can have different computational requirements and must be scheduled efficiently across available resources.",
    question: "What batch coordination approach optimizes RESOURCE allocation with dependency management?",
    options: [
        "Use Step Functions for job workflows with resource allocation logic and DynamoDB for job queue management",
        "Implement custom scheduler with SQS priority queues and Lambda for resource coordination",
        "Use EventBridge with job routing and Lambda for resource allocation and dependency tracking",
        "Deploy AWS Batch with Lambda triggers and custom dependency coordination logic"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions handle job dependencies through workflow design. Resource allocation logic in workflow states. DynamoDB provides efficient job queue management with priority support.",
        whyWrong: {
            1: "Custom scheduler duplicates proven workflow orchestration capabilities of Step Functions",
            2: "EventBridge routing doesn't provide the job dependency orchestration needed for batch processing",
            3: "AWS Batch is good for compute but Step Functions provide better dependency and resource coordination"
        },
        examStrategy: "Batch job coordination: Step Functions dependency workflows + resource allocation logic + DynamoDB queue management."
    }
},
{
    id: 'dva_251',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A real-time gaming platform handles player matchmaking with skill-based algorithms, latency optimization, and dynamic game session creation. Matchmaking must consider player preferences, geographic location, and real-time server capacity.",
    question: "Which matchmaking architecture optimizes SKILL-BASED matching with latency and capacity considerations?",
    options: [
        "Use Lambda with Redis for player queues, geolocation-based matching, and DynamoDB for session management",
        "Implement GameLift with custom matchmaking logic and Lambda for session coordination",
        "Use SQS with player queuing, Lambda for matching algorithms, and Step Functions for session creation",
        "Deploy custom matchmaking with ElastiCache, Lambda processing, and EventBridge for session events"
    ],
    correct: 0,
    explanation: {
        correct: "Redis provides efficient player queue management with skill-based sorting. Geolocation-based matching optimizes latency. DynamoDB handles session state with global consistency.",
        whyWrong: {
            1: "GameLift FlexMatch is good but Redis provides more flexibility for custom skill-based algorithms",
            2: "SQS doesn't provide the real-time matching capabilities needed for gaming latency requirements",
            3: "Custom solution with ElastiCache duplicates Redis capabilities without additional benefits"
        },
        examStrategy: "Gaming matchmaking: Lambda + Redis skill queues + geolocation matching + DynamoDB session management."
    }
},
{
    id: 'dva_252',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function processes e-commerce orders and must coordinate inventory checks, payment processing, shipping arrangements, and notification delivery. Each step can fail independently and requires different retry strategies.",
    question: "What order processing pattern handles INDEPENDENT failures with step-specific retry strategies?",
    options: [
        "Use Step Functions with Catch blocks for step-specific error handling and different retry configurations per step",
        "Implement EventBridge with retry policies and dead letter queues for each processing step",
        "Use SQS with separate queues for each step and Lambda functions with custom retry logic",
        "Deploy Saga pattern with Lambda coordination and compensation actions for failures"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions Catch blocks enable step-specific error handling. Different retry configurations per step. Built-in state management for complex order processing workflows.",
        whyWrong: {
            1: "EventBridge lacks the workflow orchestration needed for coordinated order processing steps",
            2: "SQS approach requires custom orchestration and doesn't provide step-specific retry configuration",
            3: "Saga pattern is for distributed transactions but doesn't provide step-specific retry strategies"
        },
        examStrategy: "Order processing with failures: Step Functions Catch blocks + step-specific retry configs + workflow orchestration."
    }
},
{
    id: 'dva_253',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A serverless analytics platform processes streaming data from IoT devices and must maintain real-time dashboards, detect anomalies, and trigger automated responses. The system handles millions of data points per minute with sub-second latency requirements.",
    question: "Which streaming analytics architecture provides REAL-TIME dashboards with automated response capabilities?",
    options: [
        "Use Kinesis Data Streams with Lambda processors, ElastiCache for dashboard data, and EventBridge for automated responses",
        "Implement Kinesis Data Analytics with dashboard outputs and Lambda for anomaly detection and response",
        "Use EventBridge with streaming rules, Lambda for analytics, and WebSocket API for real-time dashboard updates",
        "Deploy custom analytics with Kinesis, Redis for aggregations, and Step Functions for response automation"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis Data Streams handle high-throughput IoT data. Lambda processors provide flexible analytics. ElastiCache enables sub-second dashboard queries. EventBridge coordinates automated responses.",
        whyWrong: {
            1: "Kinesis Data Analytics has limited flexibility compared to Lambda processors for complex analytics",
            2: "EventBridge streaming rules don't provide the throughput needed for millions of data points per minute",
            3: "Custom analytics duplicates proven Kinesis capabilities and Step Functions add latency for real-time responses"
        },
        examStrategy: "Real-time IoT analytics: Kinesis streams + Lambda processors + ElastiCache dashboards + EventBridge automation."
    }
},
{
    id: 'dva_254',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A Lambda function implements a state machine for user onboarding with multiple steps, conditional branching based on user type, and external service integrations. The process can take days to complete with user interactions.",
    question: "What state machine approach handles LONG-RUNNING onboarding with user interactions?",
    options: [
        "Use Step Functions Standard workflows with human tasks and external service callbacks for long-running processes",
        "Implement custom state machine with DynamoDB state storage and Lambda for process coordination",
        "Use SQS with delayed messages for timing and Lambda for state transitions and user interactions",
        "Deploy workflow engine with EventBridge events and Lambda for state management"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions Standard workflows handle long-running processes (up to 1 year). Human tasks enable user interactions. External service callbacks handle async integrations efficiently.",
        whyWrong: {
            1: "Custom state machine duplicates Step Functions capabilities and adds development overhead",
            2: "SQS delayed messages don't provide the state machine orchestration needed for complex onboarding flows",
            3: "EventBridge with custom workflow lacks the built-in state machine capabilities and human task support"
        },
        examStrategy: "Long-running user onboarding: Step Functions Standard + human tasks + service callbacks for multi-day processes."
    }
},
{
    id: 'dva_255',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A content delivery platform must optimize video streaming based on user device capabilities, network conditions, and content popularity. The system needs to make real-time decisions about bitrate, CDN selection, and caching strategies.",
    question: "Which content optimization architecture provides REAL-TIME streaming decisions with adaptive delivery?",
    options: [
        "Use Lambda@Edge for device detection and routing decisions with CloudFront for adaptive streaming delivery",
        "Implement API Gateway with Lambda for content optimization and CloudFront with multiple origins",
        "Use MediaConvert with Lambda triggers and CloudFront with custom origins for adaptive streaming",
        "Deploy custom content optimizer with Lambda, ElastiCache for decisions, and CloudFront distribution"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda@Edge runs at edge locations for real-time device detection and routing. CloudFront provides adaptive streaming with origin selection based on Lambda@Edge decisions.",
        whyWrong: {
            1: "API Gateway adds latency compared to Lambda@Edge for real-time content optimization decisions",
            2: "MediaConvert is for transcoding, not real-time streaming optimization and routing decisions",
            3: "Custom optimizer with central Lambda doesn't provide edge-location optimization that Lambda@Edge enables"
        },
        examStrategy: "Real-time content optimization: Lambda@Edge device detection + routing decisions + CloudFront adaptive streaming."
    }
},
{
    id: 'dva_256',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function coordinates API calls to multiple external services with different authentication methods, rate limits, and response formats. The function must aggregate responses and handle service failures gracefully.",
    question: "What external service coordination approach handles MULTIPLE authentication methods with graceful failure handling?",
    options: [
        "Use Lambda with service-specific adapters, circuit breaker patterns, and response aggregation logic",
        "Implement API Gateway with multiple backend integrations and Lambda for response processing",
        "Use Step Functions for service coordination with retry policies and error handling per service",
        "Deploy service mesh with Lambda and custom authentication handling for each external service"
    ],
    correct: 0,
    explanation: {
        correct: "Service-specific adapters handle authentication diversity. Circuit breakers provide graceful failure handling. Response aggregation logic combines different formats efficiently.",
        whyWrong: {
            1: "API Gateway backend integrations don't handle the authentication diversity and circuit breaker patterns needed",
            2: "Step Functions add workflow overhead for simple service coordination and response aggregation",
            3: "Service mesh benefits don't apply to Lambda calling external services outside the mesh"
        },
        examStrategy: "Multi-service coordination: Lambda service adapters + circuit breakers + response aggregation for diverse external APIs."
    }
},
{
    id: 'dva_257',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A financial risk management system processes market data in real-time and must calculate portfolio risk metrics, detect limit breaches, and generate alerts with sub-second latency. Calculations involve complex mathematical models with large datasets.",
    question: "Which risk calculation architecture provides SUB-SECOND latency with complex mathematical processing?",
    options: [
        "Use Lambda with pre-loaded models in memory, Redis for market data caching, and EventBridge for alert generation",
        "Implement SageMaker real-time endpoints with Lambda coordination and DynamoDB for risk storage",
        "Use Kinesis Data Analytics for real-time calculations with Lambda for alert processing",
        "Deploy ECS with mathematical libraries and Redis for data caching and calculation optimization"
    ],
    correct: 0,
    explanation: {
        correct: "Pre-loaded models in Lambda memory enable sub-second calculations. Redis provides sub-millisecond market data access. EventBridge ensures reliable alert generation and delivery.",
        whyWrong: {
            1: "SageMaker endpoints have higher latency than in-memory Lambda calculations for sub-second requirements",
            2: "Kinesis Data Analytics has limited mathematical modeling capabilities compared to custom Lambda calculations",
            3: "ECS adds container orchestration overhead compared to optimized Lambda memory-based calculations"
        },
        examStrategy: "Sub-second risk calculations: Lambda in-memory models + Redis market data + EventBridge alerts for real-time processing."
    }
},
{
    id: 'dva_258',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function processes social media content and must classify posts, extract mentions, detect sentiment, and update user profiles. Processing can handle millions of posts per hour with varying content complexity.",
    question: "What content processing approach handles HIGH-VOLUME social media analysis efficiently?",
    options: [
        "Use SQS for post queuing with Lambda processors and DynamoDB for user profile updates",
        "Implement Kinesis Data Streams with Lambda consumers for content analysis and batch profile updates",
        "Use EventBridge with content routing and specialized Lambda functions for each analysis type",
        "Deploy Step Functions for content workflows with parallel processing and profile coordination"
    ],
    correct: 1,
    explanation: {
        correct: "Kinesis Data Streams handle high-volume social media data efficiently. Lambda consumers provide parallel content analysis. Batch profile updates optimize DynamoDB write performance.",
        whyWrong: {
            0: "SQS has throughput limitations and individual profile updates don't optimize for high-volume processing",
            2: "EventBridge routing adds latency for high-volume content processing requirements",
            3: "Step Functions add workflow overhead for simple content analysis tasks and don't optimize for volume"
        },
        examStrategy: "High-volume content processing: Kinesis streams + Lambda consumers + batch profile updates for efficiency."
    }
},
{
    id: 'dva_259',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A serverless application implements a distributed cache with intelligent prefetching, cache coherence across regions, and adaptive expiration policies. The cache serves millions of requests per second with sub-millisecond latency.",
    question: "Which distributed caching architecture provides INTELLIGENT prefetching with global coherence?",
    options: [
        "Use ElastiCache Redis clusters with Lambda for prefetching logic and DynamoDB Global Tables for cache metadata",
        "Implement multi-region Redis with Lambda coordination and EventBridge for cache invalidation events",
        "Use DynamoDB Global Tables with DAX for caching and Lambda for intelligent prefetching",
        "Deploy custom cache with ElastiCache, Lambda@Edge for prefetching, and CloudFront integration"
    ],
    correct: 0,
    explanation: {
        correct: "Redis clusters provide sub-millisecond access for millions of requests. Lambda prefetching logic enables intelligent cache warming. Global Tables ensure cache metadata coherence across regions.",
        whyWrong: {
            1: "Multi-region Redis coordination is complex and EventBridge adds latency for cache coherence",
            2: "DynamoDB with DAX doesn't provide the sub-millisecond performance needed for millions of requests per second",
            3: "Custom cache with Lambda@Edge adds complexity without providing better performance than Redis clusters"
        },
        examStrategy: "Distributed intelligent caching: Redis clusters + Lambda prefetching + Global Tables metadata for coherence."
    }
},
{
    id: 'dva_260',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A Lambda function coordinates order fulfillment across multiple warehouses with inventory allocation, shipping optimization, and delivery tracking. Orders must be fulfilled from optimal locations based on inventory and delivery constraints.",
    question: "What fulfillment coordination approach optimizes WAREHOUSE allocation with delivery constraints?",
    options: [
        "Use Lambda with optimization algorithms, DynamoDB for inventory tracking, and Step Functions for fulfillment workflows",
        "Implement EventBridge with warehouse routing and Lambda functions for inventory management",
        "Use SQS with warehouse-specific queues and Lambda for allocation and shipping coordination",
        "Deploy custom optimizer with Lambda, ElastiCache for inventory, and API Gateway for warehouse integration"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda optimization algorithms handle complex allocation logic. DynamoDB provides real-time inventory tracking. Step Functions orchestrate multi-step fulfillment workflows efficiently.",
        whyWrong: {
            1: "EventBridge routing doesn't provide the optimization algorithms needed for warehouse allocation",
            2: "SQS warehouse queues don't provide the optimization logic needed for optimal allocation decisions",
            3: "Custom optimizer duplicates proven capabilities and API Gateway doesn't optimize warehouse coordination"
        },
        examStrategy: "Order fulfillment optimization: Lambda algorithms + DynamoDB inventory + Step Functions workflow orchestration."
    }
},
{
    id: 'dva_261',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A real-time fraud detection system analyzes transaction patterns, user behavior, and device fingerprints to score transactions in under 50ms. The system must handle 100,000 transactions per second and continuously update fraud models.",
    question: "Which fraud detection architecture achieves SUB-50MS latency with continuous model updates?",
    options: [
        "Use Lambda with in-memory fraud models, Redis for behavior tracking, and Kinesis for real-time model updates",
        "Implement SageMaker real-time endpoints with Lambda coordination and DynamoDB for fraud scoring",
        "Use API Gateway with caching, Lambda for fraud logic, and ElastiCache for transaction data",
        "Deploy custom fraud engine with ECS, Redis for models, and Kinesis for transaction processing"
    ],
    correct: 0,
    explanation: {
        correct: "In-memory Lambda models provide sub-50ms scoring. Redis enables sub-millisecond behavior lookups. Kinesis real-time updates keep fraud models current with latest patterns.",
        whyWrong: {
            1: "SageMaker endpoints have higher latency than in-memory models and don't meet sub-50ms requirements",
            2: "API Gateway caching doesn't help with real-time fraud scoring and doesn't optimize for transaction volume",
            3: "ECS adds container overhead compared to optimized Lambda in-memory processing for latency requirements"
        },
        examStrategy: "Sub-50ms fraud detection: Lambda in-memory models + Redis behavior tracking + Kinesis model updates."
    }
},
{
    id: 'dva_262',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function processes document workflows with approval chains, document versioning, and audit trails. Documents can be in multiple formats and require different approval processes based on document type and content.",
    question: "What document workflow approach handles COMPLEX approval chains with format diversity?",
    options: [
        "Use Step Functions for approval workflows with Lambda document processors and DynamoDB for versioning",
        "Implement EventBridge with document routing and Lambda functions for format-specific processing",
        "Use SQS with approval queues and Lambda for document processing and version management",
        "Deploy custom workflow with Lambda, S3 for document storage, and DynamoDB for workflow state"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions handle complex approval chains with conditional logic. Lambda processors handle format diversity. DynamoDB provides efficient document versioning and state management.",
        whyWrong: {
            1: "EventBridge routing doesn't provide the approval chain orchestration needed for complex document workflows",
            2: "SQS queues lack the workflow orchestration capabilities needed for complex approval chains",
            3: "Custom workflow duplicates Step Functions capabilities without providing additional benefits"
        },
        examStrategy: "Complex document workflows: Step Functions approval chains + Lambda format processors + DynamoDB versioning."
    }
},
{
    id: 'dva_263',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "hard",
    timeRecommendation: 150,
    scenario: "A serverless application implements real-time collaborative editing with conflict resolution, presence awareness, and version history. Multiple users can edit the same document simultaneously with guaranteed consistency.",
    question: "Which collaborative editing architecture provides CONFLICT resolution with guaranteed consistency?",
    options: [
        "Use API Gateway WebSocket with Lambda operational transformation and DynamoDB for document state",
        "Implement EventBridge for operation routing with Lambda conflict resolution and S3 for version storage",
        "Use Kinesis Data Streams for operations with Lambda processors and ElastiCache for document state",
        "Deploy custom collaboration server with WebSocket API, Redis for state, and operational transformation"
    ],
    correct: 0,
    explanation: {
        correct: "WebSocket API provides real-time bidirectional communication. Lambda operational transformation resolves conflicts. DynamoDB ensures consistent document state with strong consistency options.",
        whyWrong: {
            1: "EventBridge routing adds latency for real-time collaboration and doesn't provide bidirectional communication",
            2: "Kinesis is designed for streaming analytics, not real-time collaborative editing with conflict resolution",
            3: "Custom collaboration server adds infrastructure complexity compared to managed API Gateway WebSocket"
        },
        examStrategy: "Real-time collaboration: WebSocket API + Lambda operational transformation + DynamoDB consistent state."
    }
},
{
    id: 'dva_264',
    domain: "Domain 1: Development with AWS Services",
    difficulty: "medium",
    timeRecommendation: 110,
    scenario: "A Lambda function coordinates microservices for e-commerce checkout with payment processing, inventory updates, order creation, and notification delivery. Each service has different SLAs and failure characteristics.",
    question: "What checkout coordination approach handles DIFFERENT SLAs with service-specific failure handling?",
    options: [
        "Use Step Functions with service-specific timeout and retry configurations for checkout orchestration",
        "Implement Saga pattern with Lambda coordination and compensation actions for checkout failures",
        "Use EventBridge with service routing and Lambda functions with individual error handling",
        "Deploy choreography pattern with SQS and Lambda for distributed checkout coordination"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions enable service-specific timeout and retry configurations. Checkout orchestration handles different SLAs. Built-in error handling for complex e-commerce workflows.",
        whyWrong: {
            1: "Saga pattern is for distributed transactions but doesn't optimize for service-specific SLAs and configurations",
            2: "EventBridge routing doesn't provide the orchestration needed for coordinated checkout processes",
            3: "Choreography pattern lacks central coordination for SLA management and error handling"
        },
        examStrategy: "E-commerce checkout coordination: Step Functions service-specific configs + orchestration for different SLAs."
    }
}, 

        // BATCH 5: 60 Advanced Security & Compliance Questions - Domain 2: Security (24%)
// Expert-level security architectures and compliance scenarios

{
    id: 'dva_265',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global financial services platform implements zero-trust architecture across serverless applications spanning multiple AWS accounts and regions. Every service interaction must be authenticated, encrypted, and logged. The platform handles millions of transactions daily with regulatory requirements for data residency and audit trails.",
    question: "Which zero-trust implementation provides COMPREHENSIVE security with regulatory compliance across multi-account architecture?",
    options: [
        "Use service mesh with mTLS for all communications, AWS IAM Roles Anywhere for service identity, CloudTrail across all accounts, and region-specific data residency controls",
        "Implement API Gateway with custom authorizers, Lambda with VPC isolation, and cross-account IAM roles with MFA requirements",
        "Use AWS Organizations with SCPs, GuardDuty across accounts, and EventBridge for security event correlation",
        "Deploy separate security accounts with centralized logging, cross-account access controls, and compliance monitoring"
    ],
    correct: 0,
    explanation: {
        correct: "Service mesh mTLS provides mutual authentication for all communications. IAM Roles Anywhere enables service identity without long-term credentials. Multi-account CloudTrail ensures complete audit trails. Region controls meet data residency requirements.",
        whyWrong: {
            1: "Custom authorizers and VPC isolation don't provide the comprehensive service-to-service authentication needed for zero-trust",
            2: "SCPs and GuardDuty provide governance and threat detection but don't implement zero-trust service authentication",
            3: "Centralized logging helps with compliance but doesn't provide the fundamental zero-trust authentication architecture"
        },
        examStrategy: "Zero-trust multi-account: Service mesh mTLS + IAM Roles Anywhere + comprehensive CloudTrail + regional data controls."
    }
},
{
    id: 'dva_266',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A Lambda function processes PII data and must implement data masking, encryption key rotation, and access logging. Different data fields require different protection levels (tokenization for SSNs, encryption for addresses, hashing for phone numbers).",
    question: "What data protection strategy provides FIELD-LEVEL security with appropriate protection methods?",
    options: [
        "Use AWS Payment Cryptography for tokenization, KMS for encryption with automatic rotation, and custom hashing with salt storage in Parameter Store",
        "Implement client-side encryption for all fields with Lambda decryption and CloudWatch for access logging",
        "Use DynamoDB with attribute-level encryption and Lambda for field-specific processing logic",
        "Deploy field-level encryption with CloudFront and Lambda@Edge for data transformation"
    ],
    correct: 0,
    explanation: {
        correct: "AWS Payment Cryptography provides secure tokenization for sensitive identifiers. KMS automatic rotation ensures key security. Custom hashing with salts provides appropriate protection for phone numbers with audit logging.",
        whyWrong: {
            1: "Client-side encryption for all fields doesn't provide the differentiated protection levels needed for different data types",
            2: "DynamoDB attribute encryption doesn't provide tokenization for SSNs or appropriate field-level differentiation",
            3: "CloudFront field-level encryption is for web applications, not Lambda data processing with differentiated protection"
        },
        examStrategy: "Field-level data protection: Payment Cryptography tokenization + KMS encryption + custom hashing for differentiated security."
    }
},
{
    id: 'dva_267',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A multi-tenant SaaS platform must enforce tenant data isolation at the database, application, and network levels. Each tenant has different compliance requirements (SOC2, HIPAA, PCI DSS) and must be able to prove complete data isolation for audits.",
    question: "Which multi-tenant isolation architecture provides PROVABLE separation with compliance-specific controls?",
    options: [
        "Use separate VPCs per compliance tier, tenant-specific KMS keys, DynamoDB with tenant partition isolation, and AWS Config for compliance monitoring",
        "Implement row-level security with tenant IDs, application-level validation, and shared infrastructure with role-based access",
        "Use separate AWS accounts per tenant with account-level isolation and cross-account resource sharing",
        "Deploy tenant-specific Lambda functions with dedicated resources and shared database with encryption"
    ],
    correct: 0,
    explanation: {
        correct: "Separate VPCs provide network isolation by compliance tier. Tenant-specific KMS keys ensure cryptographic separation. DynamoDB partition isolation provides database-level separation. AWS Config proves compliance configuration.",
        whyWrong: {
            1: "Row-level security and application validation can be bypassed and don't provide the provable isolation needed for strict compliance",
            2: "Separate accounts provide maximum isolation but are operationally complex and don't scale to many tenants efficiently",
            3: "Shared database with encryption doesn't provide the tenant isolation needed for different compliance requirements"
        },
        examStrategy: "Multi-tenant compliance isolation: VPC per compliance tier + tenant KMS keys + partition isolation + Config monitoring."
    }
},
{
    id: 'dva_268',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A serverless application implements real-time threat detection using machine learning models to analyze user behavior patterns. The system must detect account takeovers, credential stuffing, and anomalous access patterns with sub-second response times.",
    question: "What threat detection architecture provides REAL-TIME ML analysis with immediate response capabilities?",
    options: [
        "Use Kinesis Data Streams for behavior events, Lambda with pre-loaded ML models, and EventBridge for automated response actions",
        "Implement GuardDuty with custom threat intelligence and Lambda for response automation",
        "Use Amazon Fraud Detector with real-time scoring and Step Functions for response workflows",
        "Deploy custom ML models in SageMaker with real-time endpoints and API Gateway for scoring"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis handles high-volume behavior events in real-time. Pre-loaded ML models in Lambda provide sub-second analysis. EventBridge enables immediate automated response actions based on threat scores.",
        whyWrong: {
            1: "GuardDuty focuses on infrastructure threats, not application-level user behavior analysis with custom ML models",
            2: "Amazon Fraud Detector is good but Lambda with pre-loaded models provides more flexibility for custom behavior analysis",
            3: "SageMaker endpoints have higher latency than in-memory Lambda models for sub-second threat detection requirements"
        },
        examStrategy: "Real-time threat detection: Kinesis events + Lambda pre-loaded ML + EventBridge automated response for sub-second analysis."
    }
},
{
    id: 'dva_269',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A financial application must implement cryptographic proof of data integrity for audit purposes. All transactions must be cryptographically signed, have tamper-proof storage, and support efficient verification of data chains for regulatory compliance.",
    question: "Which cryptographic integrity approach provides TAMPER-PROOF storage with efficient verification?",
    options: [
        "Use QLDB for immutable ledger storage with cryptographic verification, AWS KMS for signing, and Lambda for verification workflows",
        "Implement blockchain-based storage with smart contracts for data integrity and distributed verification",
        "Use DynamoDB with hash chains, digital signatures with AWS KMS, and S3 Object Lock for immutable storage",
        "Deploy custom cryptographic solution with HSM signing and distributed storage across multiple regions"
    ],
    correct: 0,
    explanation: {
        correct: "QLDB provides immutable ledger with built-in cryptographic verification. AWS KMS handles secure signing operations. Lambda workflows enable efficient verification processes for regulatory compliance.",
        whyWrong: {
            1: "Blockchain implementation adds enormous complexity and performance overhead for financial audit requirements",
            2: "DynamoDB hash chains work but QLDB provides better built-in cryptographic integrity and verification capabilities",
            3: "Custom cryptographic solutions add development complexity compared to purpose-built QLDB ledger capabilities"
        },
        examStrategy: "Cryptographic integrity: QLDB immutable ledger + KMS signing + Lambda verification for tamper-proof audit trails."
    }
},
{
    id: 'dva_270',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function integrates with external APIs that require OAuth 2.0, API keys, and certificate-based authentication. Credentials must be rotated automatically, and expired credentials should trigger immediate alerts and fallback mechanisms.",
    question: "What credential management approach provides AUTOMATED rotation with failure handling?",
    options: [
        "Use Secrets Manager with automatic rotation for OAuth tokens, Lambda rotation functions for API keys, and EventBridge for credential expiry alerts",
        "Implement Parameter Store for all credentials with manual rotation procedures and CloudWatch alarms",
        "Use IAM roles with temporary credentials and external service federation where possible",
        "Store credentials in encrypted S3 with Lambda-based rotation and SNS notifications"
    ],
    correct: 0,
    explanation: {
        correct: "Secrets Manager provides automatic OAuth token rotation with versioning. Lambda rotation functions handle API key refresh. EventBridge enables immediate alerts and automated fallback mechanisms.",
        whyWrong: {
            1: "Parameter Store lacks automatic rotation capabilities and manual procedures don't meet availability requirements",
            2: "IAM roles with federation are ideal but many external services don't support AWS IAM federation",
            3: "S3 credential storage adds complexity and doesn't provide the automatic rotation capabilities needed"
        },
        examStrategy: "Automated credential rotation: Secrets Manager OAuth rotation + Lambda API key rotation + EventBridge alerts."
    }
},
{
    id: 'dva_271',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A healthcare platform processes patient data across multiple regions with HIPAA compliance requirements. The system must implement differential privacy, consent management, data lineage tracking, and right-to-be-forgotten capabilities while maintaining analytical capabilities.",
    question: "Which privacy-preserving architecture provides DIFFERENTIAL privacy with GDPR compliance capabilities?",
    options: [
        "Use Lake Formation with data lineage tracking, Lambda for differential privacy algorithms, Cognito for consent management, and automated deletion workflows with Step Functions",
        "Implement custom privacy engine with Lambda, DynamoDB for consent tracking, and manual data deletion processes",
        "Use Amazon Macie for data discovery with Lambda for privacy processing and S3 for data storage",
        "Deploy separate data lakes per region with custom privacy controls and manual compliance procedures"
    ],
    correct: 0,
    explanation: {
        correct: "Lake Formation provides comprehensive data lineage tracking. Lambda implements differential privacy algorithms. Cognito manages consent with audit trails. Step Functions automate GDPR deletion workflows.",
        whyWrong: {
            1: "Custom privacy engine and manual deletion don't provide the comprehensive compliance capabilities needed for healthcare",
            2: "Macie focuses on data discovery, not differential privacy implementation and consent management",
            3: "Separate data lakes with manual procedures don't scale and lack automated compliance capabilities"
        },
        examStrategy: "Healthcare privacy compliance: Lake Formation lineage + Lambda differential privacy + Cognito consent + automated deletion."
    }
},
{
    id: 'dva_272',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A serverless API must implement adaptive authentication based on risk factors including device fingerprinting, behavioral analysis, and geolocation. High-risk requests require additional verification while maintaining user experience for legitimate users.",
    question: "What adaptive authentication approach provides RISK-BASED security with optimal user experience?",
    options: [
        "Use Cognito with advanced security features, Lambda for risk scoring with device fingerprinting, and conditional MFA based on risk levels",
        "Implement custom authentication with machine learning risk models and progressive authentication challenges",
        "Use API Gateway with Lambda authorizers for risk calculation and external identity providers for verification",
        "Deploy multi-factor authentication for all requests with risk-based timeout adjustments"
    ],
    correct: 0,
    explanation: {
        correct: "Cognito advanced security provides built-in risk assessment. Lambda risk scoring enables custom device fingerprinting and behavioral analysis. Conditional MFA maintains UX while securing high-risk requests.",
        whyWrong: {
            1: "Custom authentication implementation increases security risk and development complexity compared to Cognito's proven capabilities",
            2: "API Gateway authorizers with external providers add latency and complexity for risk-based authentication",
            3: "Universal MFA degrades user experience and doesn't provide the adaptive risk-based security needed"
        },
        examStrategy: "Adaptive authentication: Cognito advanced security + Lambda risk scoring + conditional MFA for risk-based UX."
    }
},
{
    id: 'dva_273',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A global application must implement data sovereignty controls ensuring that citizen data never leaves specific geographic boundaries. The system uses DynamoDB Global Tables, Lambda functions, and S3 storage across multiple regions with complex data routing requirements.",
    question: "Which data sovereignty architecture ensures GEOGRAPHIC boundaries with global application functionality?",
    options: [
        "Use regional DynamoDB tables with Lambda data residency logic, S3 with region-specific buckets, and API Gateway regional endpoints with routing controls",
        "Implement data classification with Lambda, cross-region replication controls, and geographic access restrictions",
        "Use separate AWS accounts per region with data transfer restrictions and manual data governance",
        "Deploy region-specific applications with no cross-region data sharing and manual data synchronization"
    ],
    correct: 0,
    explanation: {
        correct: "Regional DynamoDB tables ensure data stays in jurisdiction. Lambda residency logic prevents unauthorized data movement. S3 region-specific buckets with API Gateway routing provide complete geographic control.",
        whyWrong: {
            1: "Data classification alone doesn't prevent cross-border data movement and lacks enforcement mechanisms",
            2: "Separate accounts provide isolation but reduce application functionality and create operational complexity",
            3: "Region-specific applications with no sharing defeat the purpose of global application functionality"
        },
        examStrategy: "Data sovereignty: Regional data stores + Lambda residency logic + API Gateway routing for geographic boundaries."
    }
},
{
    id: 'dva_274',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function processes financial transactions and must implement transaction signing, non-repudiation, and audit logging. Each transaction must be cryptographically signed by the user and the system, with immutable audit trails for regulatory compliance.",
    question: "What transaction security approach provides NON-REPUDIATION with comprehensive audit capabilities?",
    options: [
        "Use AWS KMS for transaction signing with separate keys for users and system, QLDB for immutable audit trails, and CloudTrail for API logging",
        "Implement digital signatures with HSM, DynamoDB for transaction storage, and CloudWatch for audit logging",
        "Use JWT tokens for transaction authentication with Lambda validation and S3 for audit storage",
        "Deploy blockchain-based transaction logging with smart contracts for verification and distributed storage"
    ],
    correct: 0,
    explanation: {
        correct: "KMS provides cryptographic signing with separate keys ensuring non-repudiation. QLDB offers immutable audit trails with cryptographic verification. CloudTrail logs all API operations for complete audit coverage.",
        whyWrong: {
            1: "HSM signing is secure but more complex and expensive than KMS for Lambda integration",
            2: "JWT tokens provide authentication but don't offer the cryptographic non-repudiation needed for financial transactions",
            3: "Blockchain implementation adds enormous complexity and performance overhead for transaction audit requirements"
        },
        examStrategy: "Transaction non-repudiation: KMS dual signing + QLDB immutable audit + CloudTrail API logging for compliance."
    }
},
{
    id: 'dva_275',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A microservices architecture implements security orchestration and automated response (SOAR) for incident handling. The system must detect security events, correlate threats across services, execute automated containment actions, and generate compliance reports.",
    question: "Which SOAR architecture provides AUTOMATED threat correlation with compliance reporting?",
    options: [
        "Use Security Hub for event aggregation, EventBridge for correlation rules, Step Functions for response workflows, and Lambda for automated containment actions",
        "Implement custom SIEM with Lambda, CloudWatch for event collection, and manual incident response procedures",
        "Use GuardDuty with custom threat intelligence, SNS for alerting, and manual security team response",
        "Deploy third-party SOAR platform with API integration to AWS services and external security tools"
    ],
    correct: 0,
    explanation: {
        correct: "Security Hub aggregates security findings from multiple services. EventBridge correlation rules enable intelligent threat detection. Step Functions orchestrate complex response workflows. Lambda provides automated containment.",
        whyWrong: {
            1: "Custom SIEM implementation lacks the comprehensive security service integrations that Security Hub provides",
            2: "GuardDuty with manual response doesn't provide the automation and orchestration needed for SOAR",
            3: "Third-party SOAR platforms may lack deep AWS integration and add vendor dependency"
        },
        examStrategy: "Security orchestration: Security Hub aggregation + EventBridge correlation + Step Functions response + Lambda containment."
    }
},
{
    id: 'dva_276',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A serverless application implements API rate limiting with different limits per user tier, geographic region, and API endpoint. The system must handle burst traffic, prevent abuse, and maintain fair usage across legitimate users.",
    question: "What rate limiting approach provides SOPHISTICATED controls with burst handling and fairness?",
    options: [
        "Use API Gateway usage plans with Lambda for dynamic limit calculation, Redis for distributed rate limiting, and CloudWatch for monitoring",
        "Implement token bucket algorithm in Lambda with DynamoDB for rate tracking and exponential backoff",
        "Use AWS WAF rate limiting rules with geographic restrictions and API Gateway throttling",
        "Deploy custom rate limiter with ElastiCache and Lambda for complex rate calculation logic"
    ],
    correct: 0,
    explanation: {
        correct: "API Gateway usage plans provide tier-based limits. Lambda calculates dynamic limits based on region and endpoint. Redis distributed rate limiting handles burst traffic efficiently with fairness algorithms.",
        whyWrong: {
            1: "Token bucket in Lambda with DynamoDB lacks the distributed performance needed for high-throughput rate limiting",
            2: "AWS WAF rate limiting is too basic for sophisticated multi-dimensional rate limiting requirements",
            3: "Custom rate limiter duplicates proven capabilities and doesn't provide the API Gateway integration benefits"
        },
        examStrategy: "Sophisticated rate limiting: API Gateway usage plans + Lambda dynamic limits + Redis distributed limiting."
    }
},
{
    id: 'dva_277',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A financial services platform must implement insider threat detection using user behavior analytics, privilege escalation monitoring, and data access pattern analysis. The system analyzes millions of events daily and must detect subtle anomalies indicating potential insider threats.",
    question: "Which insider threat detection architecture provides COMPREHENSIVE behavioral analysis with anomaly detection?",
    options: [
        "Use CloudTrail for all user actions, Kinesis Data Analytics for behavior analysis, Lambda for anomaly detection algorithms, and EventBridge for alert generation",
        "Implement custom user behavior analytics with machine learning models, DynamoDB for behavior storage, and real-time scoring",
        "Use GuardDuty with custom rules, CloudWatch for metrics collection, and manual analysis of security events",
        "Deploy third-party security analytics platform with API integration for data collection and analysis"
    ],
    correct: 0,
    explanation: {
        correct: "CloudTrail captures comprehensive user actions. Kinesis Data Analytics processes millions of events for behavior patterns. Lambda implements sophisticated anomaly algorithms. EventBridge provides reliable alerting.",
        whyWrong: {
            1: "Custom analytics implementation lacks the scale and proven capabilities of Kinesis Data Analytics for processing millions of events",
            2: "GuardDuty focuses on infrastructure threats, not sophisticated user behavior analytics for insider threats",
            3: "Third-party platforms may lack deep AWS integration and visibility into all user actions across services"
        },
        examStrategy: "Insider threat detection: CloudTrail comprehensive logging + Kinesis analytics + Lambda anomaly detection + EventBridge alerts."
    }
},
{
    id: 'dva_278',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A Lambda function handles payment processing and must implement PCI DSS compliance including cardholder data encryption, secure key management, access logging, and network security. The function integrates with external payment processors.",
    question: "What PCI DSS compliance approach provides COMPREHENSIVE protection for payment processing?",
    options: [
        "Use Payment Cryptography for card tokenization, VPC with private subnets, KMS for key management, and comprehensive CloudTrail logging",
        "Implement client-side encryption for card data with Lambda decryption and DynamoDB storage",
        "Use third-party payment tokenization service with Lambda integration and standard security controls",
        "Deploy Lambda in isolated VPC with custom encryption and manual audit procedures"
    ],
    correct: 0,
    explanation: {
        correct: "Payment Cryptography provides PCI-compliant tokenization. VPC private subnets ensure network isolation. KMS meets key management requirements. CloudTrail provides comprehensive audit logging for PCI compliance.",
        whyWrong: {
            1: "Client-side encryption with Lambda decryption doesn't provide the tokenization and compliance controls needed for PCI DSS",
            2: "Third-party tokenization helps but doesn't address network security, key management, and audit logging requirements",
            3: "Custom encryption and manual procedures don't provide the comprehensive PCI DSS compliance framework needed"
        },
        examStrategy: "PCI DSS compliance: Payment Cryptography tokenization + VPC isolation + KMS key management + CloudTrail auditing."
    }
},
{
    id: 'dva_279',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A serverless application implements attribute-based access control (ABAC) with dynamic policy evaluation based on user attributes, resource properties, environmental conditions, and real-time risk assessment.",
    question: "Which ABAC implementation provides DYNAMIC policy evaluation with real-time risk assessment?",
    options: [
        "Use Cognito with custom attributes, Lambda authorizer for policy evaluation, Redis for attribute caching, and real-time risk scoring",
        "Implement IAM with attribute-based conditions and Lambda for policy management",
        "Use API Gateway with custom authorizers and DynamoDB for attribute storage",
        "Deploy external ABAC engine with API integration and custom policy management"
    ],
    correct: 0,
    explanation: {
        correct: "Cognito provides comprehensive attribute management. Lambda authorizer enables dynamic policy evaluation with complex logic. Redis caching optimizes attribute access. Real-time risk scoring enhances security decisions.",
        whyWrong: {
            1: "IAM attribute conditions are limited compared to custom ABAC logic and don't support complex policy evaluation",
            2: "API Gateway authorizers alone lack the comprehensive attribute management and risk assessment capabilities",
            3: "External ABAC engines may lack deep AWS integration and real-time attribute access"
        },
        examStrategy: "Dynamic ABAC: Cognito attributes + Lambda policy evaluation + Redis caching + real-time risk scoring."
    }
},
{
    id: 'dva_280',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function processes sensitive documents and must implement content-based security controls including data loss prevention (DLP), content classification, and automated redaction based on document type and sensitivity level.",
    question: "What content security approach provides AUTOMATED DLP with intelligent redaction capabilities?",
    options: [
        "Use Amazon Macie for content classification, Lambda for DLP rule evaluation, and automated redaction with Textract for document processing",
        "Implement custom content analysis with Lambda, regular expressions for sensitive data detection, and manual redaction processes",
        "Use Comprehend for entity detection with Lambda for content filtering and S3 for secure storage",
        "Deploy third-party DLP solution with API integration and custom content processing workflows"
    ],
    correct: 0,
    explanation: {
        correct: "Macie provides intelligent content classification with ML. Lambda enables custom DLP rule evaluation. Textract processes documents for automated redaction based on sensitivity levels.",
        whyWrong: {
            1: "Custom content analysis with regex lacks the intelligence and accuracy of ML-based classification and detection",
            2: "Comprehend entity detection is good but doesn't provide comprehensive DLP capabilities and classification",
            3: "Third-party DLP solutions may lack deep integration with AWS document processing services"
        },
        examStrategy: "Automated DLP: Macie content classification + Lambda DLP rules + Textract document processing for intelligent redaction."
    }
},
{
    id: 'dva_281',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global application implements continuous compliance monitoring across multiple regulatory frameworks (SOX, GDPR, HIPAA, PCI DSS) with automated evidence collection, compliance scoring, and remediation workflows.",
    question: "Which compliance automation architecture provides CONTINUOUS monitoring with multi-framework support?",
    options: [
        "Use AWS Config for resource compliance, Security Hub for security findings, Lambda for compliance scoring, and Step Functions for automated remediation workflows",
        "Implement custom compliance engine with CloudWatch metrics, manual evidence collection, and quarterly compliance reports",
        "Use AWS Audit Manager with Config rules and manual compliance assessment procedures",
        "Deploy third-party GRC platform with API integration and custom compliance monitoring"
    ],
    correct: 0,
    explanation: {
        correct: "AWS Config monitors resource compliance continuously. Security Hub aggregates security findings across frameworks. Lambda calculates compliance scores. Step Functions automate remediation for multiple regulatory requirements.",
        whyWrong: {
            1: "Custom compliance engine lacks the comprehensive monitoring and automation capabilities needed for multiple frameworks",
            2: "Audit Manager helps with evidence collection but doesn't provide the continuous monitoring and automated remediation needed",
            3: "Third-party GRC platforms may lack deep AWS integration and real-time compliance monitoring capabilities"
        },
        examStrategy: "Continuous compliance: Config resource monitoring + Security Hub findings + Lambda scoring + Step Functions remediation."
    }
},
{
    id: 'dva_282',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A serverless API implements security event correlation across multiple data sources including CloudTrail, VPC Flow Logs, and application logs. The system must detect complex attack patterns and generate actionable security intelligence.",
    question: "What security correlation approach provides INTELLIGENT pattern detection with actionable insights?",
    options: [
        "Use Kinesis Data Streams for log ingestion, Lambda for correlation algorithms, ElastiSearch for pattern storage, and EventBridge for alert routing",
        "Implement centralized logging with CloudWatch Insights and manual pattern analysis",
        "Use GuardDuty findings with custom correlation rules and SNS for alerting",
        "Deploy SIEM solution with custom log parsing and manual threat hunting procedures"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis ingests high-volume security logs. Lambda correlation algorithms detect complex patterns across data sources. ElastiSearch enables pattern storage and querying. EventBridge routes actionable alerts.",
        whyWrong: {
            1: "CloudWatch Insights with manual analysis doesn't provide the automated correlation needed for complex attack patterns",
            2: "GuardDuty findings alone don't correlate application logs and custom sources for comprehensive pattern detection",
            3: "SIEM solutions may lack the real-time processing and AWS-native integration needed for serverless environments"
        },
        examStrategy: "Security correlation: Kinesis log ingestion + Lambda correlation + ElastiSearch patterns + EventBridge actionable alerts."
    }
},
{
    id: 'dva_283',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A financial trading platform implements real-time fraud detection with machine learning models that must be updated continuously based on new fraud patterns. The system processes millions of transactions per hour and must adapt to emerging threats within minutes.",
    question: "Which adaptive fraud detection architecture provides REAL-TIME model updates with continuous learning?",
    options: [
        "Use Kinesis for transaction streams, Lambda for real-time scoring, SageMaker for model training, and EventBridge for model deployment automation",
        "Implement static fraud models in Lambda with periodic batch updates and manual model deployment",
        "Use Amazon Fraud Detector with pre-built models and standard rule-based detection",
        "Deploy custom ML pipeline with batch processing and weekly model updates"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis handles high-volume transaction streams. Lambda provides real-time scoring with updated models. SageMaker enables continuous model training. EventBridge automates rapid model deployment for emerging threats.",
        whyWrong: {
            1: "Static models with periodic updates can't adapt quickly enough to emerging fraud patterns in financial trading",
            2: "Fraud Detector pre-built models lack the customization and rapid adaptation needed for trading-specific fraud patterns",
            3: "Custom ML pipeline with weekly updates is too slow for financial fraud detection requiring minute-level adaptation"
        },
        examStrategy: "Adaptive fraud detection: Kinesis streams + Lambda real-time scoring + SageMaker continuous training + EventBridge deployment."
    }
},
{
    id: 'dva_284',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A Lambda function implements secure multi-party computation (SMPC) for privacy-preserving analytics across multiple organizations. Each party contributes data without revealing sensitive information, and results must be cryptographically verifiable.",
    question: "What SMPC approach provides PRIVACY-PRESERVING computation with cryptographic verification?",
    options: [
        "Use Lambda with homomorphic encryption libraries, KMS for key management, and cryptographic commitment schemes for verification",
        "Implement federated learning with TensorFlow and encrypted parameter sharing",
        "Use differential privacy with Lambda and shared dataset aggregation",
        "Deploy secure enclaves with custom cryptographic protocols and manual verification"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda homomorphic encryption enables computation on encrypted data. KMS provides secure key management for multi-party scenarios. Cryptographic commitments ensure verifiable results without revealing inputs.",
        whyWrong: {
            1: "Federated learning is for ML model training, not general secure multi-party computation for analytics",
            2: "Differential privacy adds noise but doesn't provide the multi-party computation capabilities needed",
            3: "Secure enclaves require specialized hardware and add complexity compared to cryptographic approaches"
        },
        examStrategy: "Secure multi-party computation: Lambda homomorphic encryption + KMS key management + cryptographic commitments."
    }
},
{
    id: 'dva_285',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A healthcare platform implements consent management with granular permissions, consent withdrawal processing, and audit trails for GDPR compliance. Users can grant/revoke consent for different data uses and must receive real-time updates about data processing.",
    question: "Which consent management architecture provides GRANULAR control with real-time processing and GDPR compliance?",
    options: [
        "Use Cognito for identity management, DynamoDB for consent storage with fine-grained permissions, Lambda for consent processing, and EventBridge for real-time updates",
        "Implement custom consent system with user preferences in database and email notifications",
        "Use third-party consent management platform with API integration and standard consent workflows",
        "Deploy manual consent processes with paper forms and periodic consent review procedures"
    ],
    correct: 0,
    explanation: {
        correct: "Cognito provides identity foundation for consent. DynamoDB enables granular permission storage with quick lookups. Lambda processes consent changes. EventBridge provides real-time updates for GDPR transparency.",
        whyWrong: {
            1: "Custom consent system lacks the comprehensive audit trails and real-time capabilities needed for GDPR compliance",
            2: "Third-party platforms may not provide the granular control and AWS integration needed for healthcare data",
            3: "Manual processes with paper forms don't meet GDPR requirements for digital consent management and real-time updates"
        },
        examStrategy: "GDPR consent management: Cognito identity + DynamoDB granular permissions + Lambda processing + EventBridge real-time updates."
    }
},
{
    id: 'dva_286',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A serverless application implements security token binding to prevent token theft and replay attacks. Tokens must be cryptographically bound to client certificates and validated on each request with minimal performance impact.",
    question: "What token binding approach provides ANTI-THEFT protection with efficient validation?",
    options: [
        "Use API Gateway with mutual TLS, Lambda for token-certificate binding validation, and Redis for binding cache",
        "Implement JWT tokens with client certificate fingerprints and Lambda validation logic",
        "Use OAuth with PKCE and standard bearer token validation",
        "Deploy custom token binding with HSM certificate validation and database lookups"
    ],
    correct: 0,
    explanation: {
        correct: "API Gateway mTLS provides certificate validation. Lambda validates cryptographic binding between tokens and certificates. Redis caching optimizes binding validation performance.",
        whyWrong: {
            1: "JWT with certificate fingerprints provides some binding but lacks the comprehensive mTLS validation for strong binding",
            2: "OAuth with PKCE helps with authorization but doesn't provide the token-certificate binding needed for anti-theft",
            3: "Custom token binding with HSM adds complexity and performance overhead compared to API Gateway mTLS"
        },
        examStrategy: "Token binding anti-theft: API Gateway mTLS + Lambda binding validation + Redis caching for performance."
    }
},
{
    id: 'dva_287',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global financial services platform implements regulatory compliance automation with automated evidence collection, cross-border data governance, and real-time compliance monitoring across multiple jurisdictions (EU, US, APAC).",
    question: "Which regulatory automation architecture provides CROSS-BORDER compliance with automated evidence collection?",
    options: [
        "Use AWS Config across all regions, Lambda for jurisdiction-specific compliance logic, S3 with Cross-Region Replication for evidence storage, and Step Functions for compliance workflows",
        "Implement manual compliance procedures with regional teams and quarterly compliance reports",
        "Use AWS Organizations with region-specific SCPs and manual evidence collection processes",
        "Deploy separate compliance systems per region with manual coordination and annual audits"
    ],
    correct: 0,
    explanation: {
        correct: "Multi-region AWS Config provides automated evidence collection. Lambda handles jurisdiction-specific compliance requirements. S3 CRR ensures evidence availability. Step Functions orchestrate complex compliance workflows.",
        whyWrong: {
            1: "Manual procedures don't scale for global financial services and lack the automation needed for continuous compliance",
            2: "SCPs provide governance but don't collect evidence or handle jurisdiction-specific compliance requirements",
            3: "Separate systems with manual coordination create compliance gaps and don't leverage automation benefits"
        },
        examStrategy: "Cross-border compliance automation: Multi-region Config + Lambda jurisdiction logic + S3 CRR evidence + Step Functions workflows."
    }
},
{
    id: 'dva_288',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A Lambda function implements privacy-preserving record linkage across multiple datasets without revealing individual records. The system must identify matching records while maintaining privacy through cryptographic techniques.",
    question: "What privacy-preserving linkage approach provides RECORD matching without revealing individual data?",
    options: [
        "Use Lambda with Bloom filter encoding, homomorphic encryption for comparison operations, and differential privacy for result protection",
        "Implement hash-based record matching with salt values and encrypted dataset storage",
        "Use machine learning clustering with privacy-preserving algorithms and anonymized datasets",
        "Deploy secure multi-party computation with custom protocols and manual result verification"
    ],
    correct: 0,
    explanation: {
        correct: "Bloom filters enable private set intersection without revealing individual records. Homomorphic encryption allows encrypted comparisons. Differential privacy protects linkage results from inference attacks.",
        whyWrong: {
            1: "Hash-based matching with salts provides some privacy but doesn't offer the cryptographic guarantees of Bloom filters",
            2: "ML clustering doesn't provide the privacy-preserving record linkage capabilities needed for individual record matching",
            3: "Custom SMPC protocols add development complexity compared to established Bloom filter and homomorphic encryption techniques"
        },
        examStrategy: "Privacy-preserving linkage: Lambda Bloom filters + homomorphic encryption + differential privacy for record matching."
    }
},
{
    id: 'dva_289',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A serverless application implements hardware security module (HSM) integration for key generation, secure key storage, and cryptographic operations with FIPS 140-2 Level 3 compliance for financial regulatory requirements.",
    question: "Which HSM integration architecture provides FIPS compliance with optimal performance for serverless applications?",
    options: [
        "Use CloudHSM cluster with Lambda VPC integration, dedicated HSM connection pooling, and CloudHSM client caching for performance optimization",
        "Implement AWS KMS with FIPS endpoints and standard Lambda integration",
        "Use external HSM with API integration and custom connection management",
        "Deploy on-premises HSM with VPN connectivity and manual key management"
    ],
    correct: 0,
    explanation: {
        correct: "CloudHSM provides FIPS 140-2 Level 3 compliance. VPC integration enables secure Lambda access. Connection pooling and client caching optimize performance for serverless workloads.",
        whyWrong: {
            1: "AWS KMS provides FIPS endpoints but is Level 2, not Level 3 compliance required for certain financial regulations",
            2: "External HSM with API integration may not provide the performance and security needed for high-volume serverless applications",
            3: "On-premises HSM with VPN adds network latency and complexity compared to native CloudHSM integration"
        },
        examStrategy: "FIPS HSM integration: CloudHSM cluster + Lambda VPC + connection pooling + client caching for serverless performance."
    }
},
{
    id: 'dva_290',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A Lambda function implements secure code review automation using static analysis, dependency scanning, and vulnerability assessment. The system must integrate with CI/CD pipelines and provide real-time security feedback to developers.",
    question: "What secure code review approach provides COMPREHENSIVE analysis with developer-friendly feedback?",
    options: [
        "Use CodeGuru Reviewer for static analysis, Lambda for dependency scanning with vulnerability databases, and EventBridge for real-time developer notifications",
        "Implement custom static analysis tools with manual code review processes and weekly security reports",
        "Use third-party security scanning tools with API integration and batch processing",
        "Deploy manual security reviews with expert analysis and quarterly vulnerability assessments"
    ],
    correct: 0,
    explanation: {
        correct: "CodeGuru Reviewer provides ML-powered static analysis. Lambda dependency scanning checks vulnerability databases. EventBridge enables real-time feedback integration with developer workflows.",
        whyWrong: {
            1: "Custom tools and manual processes don't scale and lack the comprehensive analysis needed for modern development",
            2: "Third-party tools with batch processing don't provide the real-time feedback needed for agile development",
            3: "Manual reviews and quarterly assessments are too slow for continuous development and don't scale"
        },
        examStrategy: "Automated secure code review: CodeGuru static analysis + Lambda dependency scanning + EventBridge real-time feedback."
    }
},
{
    id: 'dva_291',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A financial application implements quantum-resistant cryptography preparation with algorithm agility, key management transition planning, and crypto inventory management for post-quantum security readiness.",
    question: "Which quantum-readiness approach provides ALGORITHM agility with transition planning capabilities?",
    options: [
        "Use Lambda with pluggable cryptographic libraries, Parameter Store for algorithm configuration, KMS for hybrid key management, and automated crypto inventory tracking",
        "Implement quantum-resistant algorithms directly in Lambda code with hardcoded configurations",
        "Use standard cryptographic libraries with manual algorithm updates and periodic reviews",
        "Deploy quantum cryptography hardware with specialized protocols and custom implementations"
    ],
    correct: 0,
    explanation: {
        correct: "Pluggable crypto libraries enable algorithm agility. Parameter Store allows configuration changes without code deployment. KMS supports hybrid classical/quantum-resistant keys. Automated inventory tracks crypto usage.",
        whyWrong: {
            1: "Hardcoded quantum-resistant algorithms lack the agility needed for transition planning and algorithm updates",
            2: "Standard libraries with manual updates don't provide the agility and inventory management needed for quantum transition",
            3: "Quantum cryptography hardware is experimental and not practical for current applications requiring transition planning"
        },
        examStrategy: "Quantum readiness: Lambda pluggable crypto + Parameter Store config + KMS hybrid keys + automated inventory."
    }
},
{
    id: 'dva_292',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A serverless application implements biometric authentication with template protection, liveness detection, and privacy-preserving matching. Biometric templates must never be stored in plaintext and matching must occur without revealing template data.",
    question: "What biometric security approach provides TEMPLATE protection with privacy-preserving matching?",
    options: [
        "Use Lambda with homomorphic encryption for templates, Cognito for identity management, and secure template matching algorithms",
        "Implement encrypted biometric storage with Lambda decryption and standard matching algorithms",
        "Use third-party biometric service with API integration and cloud-based template storage",
        "Deploy on-device biometric processing with local template storage and manual verification"
    ],
    correct: 0,
    explanation: {
        correct: "Homomorphic encryption enables encrypted template matching without decryption. Cognito provides identity foundation. Secure matching algorithms prevent template reconstruction from matching operations.",
        whyWrong: {
            1: "Encrypted storage with Lambda decryption exposes templates during processing and doesn't provide privacy-preserving matching",
            2: "Third-party services may not provide the template protection and privacy-preserving capabilities needed",
            3: "On-device processing avoids cloud storage but doesn't provide the centralized matching needed for serverless applications"
        },
        examStrategy: "Biometric template protection: Lambda homomorphic encryption + Cognito identity + secure matching algorithms."
    }
},
{
    id: 'dva_293',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global healthcare platform implements federated identity management across multiple healthcare organizations with different identity providers, consent requirements, and regulatory compliance needs (HIPAA, GDPR, local regulations).",
    question: "Which federated identity architecture provides MULTI-ORGANIZATION integration with comprehensive compliance?",
    options: [
        "Use Cognito federated identities with SAML/OIDC providers, Lambda for consent orchestration, Step Functions for compliance workflows, and organization-specific attribute mapping",
        "Implement custom federation with manual identity provider integration and basic consent management",
        "Use single sign-on solution with standard protocols and manual compliance procedures",
        "Deploy separate identity systems per organization with manual user coordination and periodic synchronization"
    ],
    correct: 0,
    explanation: {
        correct: "Cognito federated identities support multiple SAML/OIDC providers. Lambda orchestrates complex consent requirements. Step Functions handle multi-jurisdiction compliance. Attribute mapping enables organization-specific requirements.",
        whyWrong: {
            1: "Custom federation lacks the proven capabilities and security features of Cognito federated identities",
            2: "SSO with manual compliance doesn't handle the complex consent and regulatory requirements of healthcare",
            3: "Separate identity systems create user experience issues and don't enable the federation needed for healthcare collaboration"
        },
        examStrategy: "Federated healthcare identity: Cognito federation + Lambda consent + Step Functions compliance + organization mapping."
    }
},
{
    id: 'dva_294',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A Lambda function implements secure machine learning inference with model protection, input validation, and output sanitization. ML models must be protected from extraction attacks and adversarial inputs.",
    question: "What secure ML inference approach provides MODEL protection with adversarial input defense?",
    options: [
        "Use Lambda with encrypted model storage, input validation and sanitization, differential privacy for outputs, and adversarial detection algorithms",
        "Implement standard ML inference with basic input validation and standard model deployment",
        "Use SageMaker endpoints with default security controls and standard inference workflows",
        "Deploy ML models in containers with encrypted storage and manual security procedures"
    ],
    correct: 0,
    explanation: {
        correct: "Encrypted model storage prevents extraction. Input validation and sanitization defend against adversarial attacks. Differential privacy protects model outputs. Adversarial detection identifies attack attempts.",
        whyWrong: {
            1: "Standard ML inference lacks the adversarial protections and model security needed for sensitive applications",
            2: "SageMaker endpoints provide some security but don't include adversarial defense and model protection capabilities",
            3: "Container deployment with manual procedures doesn't provide the comprehensive ML security needed"
        },
        examStrategy: "Secure ML inference: Lambda encrypted models + input validation + differential privacy + adversarial detection."
    }
},
{
    id: 'dva_295',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A financial trading platform implements real-time security monitoring with behavioral analytics, anomaly detection, and automated threat hunting. The system analyzes trader behavior, transaction patterns, and system interactions to detect insider threats and market manipulation.",
    question: "Which security monitoring architecture provides BEHAVIORAL analytics with automated threat hunting?",
    options: [
        "Use Kinesis for behavioral data streams, Lambda for real-time analytics, ElastiSearch for pattern storage, and Step Functions for automated investigation workflows",
        "Implement custom security monitoring with manual analyst review and periodic threat hunting exercises",
        "Use CloudWatch metrics with standard alerting and manual investigation procedures",
        "Deploy SIEM solution with log aggregation and weekly threat hunting activities"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis processes high-volume behavioral data. Lambda enables real-time analytics and anomaly detection. ElastiSearch stores and queries behavioral patterns. Step Functions automate investigation workflows.",
        whyWrong: {
            1: "Custom monitoring with manual review doesn't provide the real-time analysis needed for financial trading security",
            2: "CloudWatch metrics alone don't provide the behavioral analytics capabilities needed for sophisticated threat detection",
            3: "SIEM with weekly hunting is too slow for financial trading environments requiring real-time threat detection"
        },
        examStrategy: "Real-time security monitoring: Kinesis behavioral streams + Lambda analytics + ElastiSearch patterns + Step Functions hunting."
    }
},
{
    id: 'dva_296',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A serverless application implements certificate transparency monitoring with automated certificate validation, suspicious certificate detection, and incident response for domain security management.",
    question: "What certificate transparency approach provides AUTOMATED monitoring with incident response capabilities?",
    options: [
        "Use Lambda for certificate transparency log monitoring, CloudWatch for alert generation, Step Functions for incident response workflows, and SNS for stakeholder notifications",
        "Implement manual certificate monitoring with periodic log reviews and email notifications",
        "Use third-party certificate monitoring service with API integration and standard alerting",
        "Deploy custom certificate tracking with database storage and manual incident procedures"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda monitors CT logs automatically for certificate issuance. CloudWatch generates alerts for suspicious certificates. Step Functions orchestrate incident response. SNS provides stakeholder notifications.",
        whyWrong: {
            1: "Manual certificate monitoring doesn't scale and misses the real-time detection needed for domain security",
            2: "Third-party services may not provide the customization and AWS integration needed for comprehensive monitoring",
            3: "Custom tracking with manual procedures lacks the automation needed for effective certificate transparency monitoring"
        },
        examStrategy: "Certificate transparency monitoring: Lambda CT log monitoring + CloudWatch alerts + Step Functions response + SNS notifications."
    }
},
{
    id: 'dva_297',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A healthcare platform implements homomorphic encryption for privacy-preserving analytics across multiple healthcare organizations. Patient data must remain encrypted during computation while enabling statistical analysis and research collaboration.",
    question: "Which homomorphic encryption architecture enables PRIVACY-PRESERVING analytics with multi-organization collaboration?",
    options: [
        "Use Lambda with homomorphic encryption libraries, S3 for encrypted data sharing, Step Functions for computation orchestration, and KMS for key management coordination",
        "Implement standard encryption with secure multi-party computation protocols and manual key management",
        "Use differential privacy with data anonymization and standard analytics workflows",
        "Deploy federated learning with encrypted parameter sharing and custom privacy protocols"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda homomorphic libraries enable encrypted computation. S3 provides secure encrypted data sharing. Step Functions orchestrate complex multi-party computations. KMS coordinates key management across organizations.",
        whyWrong: {
            1: "Standard encryption with SMPC is complex and doesn't provide the computational flexibility of homomorphic encryption",
            2: "Differential privacy with anonymization doesn't enable the encrypted computation needed for privacy-preserving analytics",
            3: "Federated learning is for ML training, not general privacy-preserving analytics across healthcare datasets"
        },
        examStrategy: "Homomorphic analytics: Lambda encrypted computation + S3 data sharing + Step Functions orchestration + KMS coordination."
    }
},
{
    id: 'dva_298',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function implements secure API gateway with request signing, replay attack prevention, and rate limiting based on client identity and risk assessment. API calls must be authenticated and authorized with fine-grained permissions.",
    question: "What secure API approach provides REQUEST signing with comprehensive attack prevention?",
    options: [
        "Use API Gateway with AWS Signature v4, Lambda authorizer for risk-based rate limiting, and DynamoDB for request tracking to prevent replay attacks",
        "Implement JWT tokens with API Gateway validation and standard rate limiting",
        "Use OAuth 2.0 with bearer tokens and basic API Gateway throttling",
        "Deploy custom API authentication with manual request validation and fixed rate limits"
    ],
    correct: 0,
    explanation: {
        correct: "AWS Signature v4 provides cryptographic request signing. Lambda authorizer enables risk-based rate limiting with client assessment. DynamoDB tracks requests to prevent replay attacks effectively.",
        whyWrong: {
            1: "JWT tokens provide authentication but don't offer request signing and replay prevention capabilities",
            2: "OAuth 2.0 with bearer tokens lacks request signing and doesn't provide the fine-grained security controls needed",
            3: "Custom authentication with manual validation doesn't scale and lacks the cryptographic protections needed"
        },
        examStrategy: "Secure API gateway: Signature v4 request signing + Lambda risk-based limiting + DynamoDB replay prevention."
    }
},
{
    id: 'dva_299',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global financial platform implements security orchestration across multiple cloud providers and on-premises systems with unified threat detection, cross-platform incident response, and compliance reporting for multiple regulatory frameworks.",
    question: "Which multi-cloud security orchestration provides UNIFIED threat detection with cross-platform incident response?",
    options: [
        "Use AWS Security Hub as central aggregation point, Lambda for cross-platform integration, Step Functions for unified incident response, and EventBridge for multi-cloud event correlation",
        "Implement separate security systems per cloud provider with manual correlation and reporting",
        "Use third-party SIEM with multi-cloud connectors and standard incident response procedures",
        "Deploy custom security orchestration platform with API integrations and manual workflow management"
    ],
    correct: 0,
    explanation: {
        correct: "Security Hub provides central aggregation across platforms. Lambda integrates with multiple cloud APIs. Step Functions orchestrate complex cross-platform responses. EventBridge correlates events from multiple sources.",
        whyWrong: {
            1: "Separate systems per cloud provider create security gaps and don't enable unified threat detection and response",
            2: "Third-party SIEM may not provide the deep AWS integration and automation capabilities needed for orchestration",
            3: "Custom platform development adds complexity and maintenance overhead compared to leveraging AWS security services"
        },
        examStrategy: "Multi-cloud security orchestration: Security Hub aggregation + Lambda integration + Step Functions response + EventBridge correlation."
    }
},
{
    id: 'dva_300',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A serverless application implements privacy-preserving machine learning with federated learning, differential privacy, and secure aggregation. Multiple parties contribute training data without revealing individual records or model parameters.",
    question: "What privacy-preserving ML approach provides FEDERATED learning with secure aggregation?",
    options: [
        "Use Lambda for local model training, homomorphic encryption for parameter aggregation, differential privacy for gradient protection, and S3 for secure model distribution",
        "Implement centralized ML training with data anonymization and standard privacy controls",
        "Use SageMaker with encrypted training data and standard federated learning frameworks",
        "Deploy custom ML platform with manual privacy controls and basic data protection"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda enables distributed local training. Homomorphic encryption allows secure parameter aggregation without revealing individual contributions. Differential privacy protects gradients. S3 distributes models securely.",
        whyWrong: {
            1: "Centralized training with anonymization doesn't provide the federated capabilities needed for multi-party collaboration",
            2: "SageMaker standard federated learning lacks the homomorphic encryption and differential privacy needed for strong privacy guarantees",
            3: "Custom ML platform with manual controls doesn't provide the cryptographic privacy protections needed"
        },
        examStrategy: "Privacy-preserving federated ML: Lambda distributed training + homomorphic aggregation + differential privacy + secure distribution."
    }
},
{
    id: 'dva_301',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A financial services platform implements continuous security assessment with automated penetration testing, vulnerability management, and security posture monitoring. The system must identify security weaknesses and provide remediation guidance without disrupting production systems.",
    question: "Which continuous security assessment provides AUTOMATED testing with safe production monitoring?",
    options: [
        "Use AWS Inspector for vulnerability scanning, Lambda for custom security tests, Step Functions for assessment orchestration, and Systems Manager for safe remediation automation",
        "Implement manual penetration testing with quarterly assessments and annual security reviews",
        "Use third-party security testing tools with scheduled scans and manual result analysis",
        "Deploy custom security scanning with basic vulnerability detection and manual remediation"
    ],
    correct: 0,
    explanation: {
        correct: "AWS Inspector provides automated vulnerability scanning. Lambda enables custom security tests. Step Functions orchestrate complex assessments safely. Systems Manager automates remediation without production disruption.",
        whyWrong: {
            1: "Manual testing with quarterly assessments is too slow for continuous security assessment and doesn't scale",
            2: "Third-party tools may not integrate well with AWS services and lack the automation needed for continuous assessment",
            3: "Custom scanning with manual remediation doesn't provide the comprehensive capabilities and safety controls needed"
        },
        examStrategy: "Continuous security assessment: Inspector vulnerability scanning + Lambda custom tests + Step Functions orchestration + Systems Manager remediation."
    }
},
{
    id: 'dva_302',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A Lambda function implements secure communications with perfect forward secrecy, message authentication, and replay protection for sensitive financial transactions. Communication security must prevent eavesdropping and tampering.",
    question: "What secure communication approach provides PERFECT forward secrecy with comprehensive message protection?",
    options: [
        "Use TLS 1.3 with ephemeral key exchange, HMAC for message authentication, nonce-based replay protection, and KMS for key management",
        "Implement custom encryption with static keys and basic message validation",
        "Use standard HTTPS with API Gateway and basic authentication mechanisms",
        "Deploy VPN connectivity with network-level encryption and manual key management"
    ],
    correct: 0,
    explanation: {
        correct: "TLS 1.3 ephemeral keys provide perfect forward secrecy. HMAC ensures message authentication and integrity. Nonce-based protection prevents replay attacks. KMS provides secure key management.",
        whyWrong: {
            1: "Custom encryption with static keys doesn't provide perfect forward secrecy and may have implementation vulnerabilities",
            2: "Standard HTTPS without additional protections doesn't provide the message authentication and replay protection needed",
            3: "VPN with manual key management doesn't provide perfect forward secrecy and adds operational complexity"
        },
        examStrategy: "Secure communications: TLS 1.3 ephemeral keys + HMAC authentication + nonce replay protection + KMS management."
    }
},
{
    id: 'dva_303',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A global healthcare platform implements data localization with encrypted cross-border communication, jurisdiction-specific access controls, and automated compliance monitoring for GDPR, HIPAA, and local healthcare regulations.",
    question: "Which data localization architecture provides JURISDICTION-SPECIFIC controls with automated compliance monitoring?",
    options: [
        "Use regional data residency with Lambda jurisdiction routing, KMS region-specific keys, AWS Config for compliance monitoring, and Step Functions for cross-border workflows",
        "Implement global data storage with manual access controls and periodic compliance reviews",
        "Use separate applications per region with manual data synchronization and compliance procedures",
        "Deploy centralized data with encryption and basic geographic access restrictions"
    ],
    correct: 0,
    explanation: {
        correct: "Regional data residency ensures jurisdiction compliance. Lambda routing enforces access controls by jurisdiction. KMS region-specific keys provide localized encryption. Config monitors compliance continuously.",
        whyWrong: {
            1: "Global storage with manual controls doesn't meet data localization requirements and lacks automation",
            2: "Separate applications per region create operational complexity and don't enable necessary cross-border workflows",
            3: "Centralized data with basic restrictions doesn't provide the jurisdiction-specific controls needed for healthcare"
        },
        examStrategy: "Healthcare data localization: Regional residency + Lambda jurisdiction routing + KMS regional keys + Config compliance monitoring."
    }
},
{
    id: 'dva_304',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A serverless application implements secure container scanning with vulnerability assessment, malware detection, and compliance checking for container images used in Lambda container deployments.",
    question: "What container security approach provides COMPREHENSIVE scanning with compliance verification?",
    options: [
        "Use Amazon ECR with enhanced scanning, Lambda for custom malware detection, AWS Config for compliance checking, and EventBridge for automated response",
        "Implement manual container scanning with periodic vulnerability assessments and basic security controls",
        "Use third-party container security tools with API integration and standard scanning workflows",
        "Deploy custom container analysis with basic vulnerability detection and manual compliance checks"
    ],
    correct: 0,
    explanation: {
        correct: "ECR enhanced scanning provides comprehensive vulnerability assessment. Lambda enables custom malware detection algorithms. Config checks compliance against security baselines. EventBridge automates response workflows.",
        whyWrong: {
            1: "Manual scanning doesn't scale and lacks the comprehensive analysis needed for container security",
            2: "Third-party tools may not integrate well with AWS container services and Lambda deployments",
            3: "Custom analysis lacks the comprehensive scanning capabilities and proven security detection of ECR enhanced scanning"
        },
        examStrategy: "Container security scanning: ECR enhanced scanning + Lambda malware detection + Config compliance + EventBridge response."
    }
},
{
    id: 'dva_305',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A financial trading platform implements advanced persistent threat (APT) detection with behavioral analysis, lateral movement detection, and automated containment. The system analyzes user activities, network traffic, and system interactions to identify sophisticated attacks.",
    question: "Which APT detection architecture provides BEHAVIORAL analysis with automated containment capabilities?",
    options: [
        "Use VPC Flow Logs with Kinesis, Lambda for behavioral analytics, machine learning models for anomaly detection, and Step Functions for automated containment workflows",
        "Implement basic intrusion detection with signature-based rules and manual threat hunting",
        "Use GuardDuty with standard threat detection and manual incident response procedures",
        "Deploy SIEM solution with log correlation and weekly threat analysis activities"
    ],
    correct: 0,
    explanation: {
        correct: "VPC Flow Logs with Kinesis provide network behavior data. Lambda analytics detect subtle patterns. ML models identify anomalies indicating APT activity. Step Functions automate rapid containment responses.",
        whyWrong: {
            1: "Signature-based detection misses sophisticated APTs that use novel techniques and doesn't provide behavioral analysis",
            2: "GuardDuty standard detection may miss advanced persistent threats requiring custom behavioral analysis",
            3: "SIEM with weekly analysis is too slow for APT detection requiring real-time behavioral monitoring"
        },
        examStrategy: "APT detection: VPC Flow Logs + Kinesis + Lambda behavioral analytics + ML anomaly detection + Step Functions containment."
    }
},
{
    id: 'dva_306',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A Lambda function implements secure backup and recovery with encryption key escrow, automated backup verification, and disaster recovery testing. Backup data must be protected and recoverable even during security incidents.",
    question: "What secure backup approach provides KEY escrow with automated verification and disaster recovery testing?",
    options: [
        "Use AWS Backup with KMS encryption, Lambda for backup verification, key escrow with multiple authorized parties, and Step Functions for disaster recovery testing workflows",
        "Implement manual backup procedures with basic encryption and periodic recovery testing",
        "Use standard backup solutions with encrypted storage and manual verification processes",
        "Deploy custom backup system with basic encryption and manual disaster recovery procedures"
    ],
    correct: 0,
    explanation: {
        correct: "AWS Backup provides automated encrypted backups. KMS key escrow ensures recovery during incidents. Lambda verifies backup integrity automatically. Step Functions orchestrate comprehensive DR testing.",
        whyWrong: {
            1: "Manual procedures don't scale and lack the automation needed for reliable backup verification and DR testing",
            2: "Standard solutions without escrow may not provide recovery during security incidents affecting key access",
            3: "Custom backup systems lack the proven reliability and automation capabilities of AWS Backup"
        },
        examStrategy: "Secure backup recovery: AWS Backup + KMS escrow + Lambda verification + Step Functions DR testing workflows."
    }
},
{
    id: 'dva_307',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A global application implements security information sharing with threat intelligence feeds, indicators of compromise (IoC) distribution, and collaborative defense mechanisms across multiple organizations and security vendors.",
    question: "Which threat intelligence architecture provides COLLABORATIVE defense with automated IoC distribution?",
    options: [
        "Use AWS Security Hub for IoC aggregation, Lambda for threat intelligence processing, EventBridge for distribution, and Step Functions for collaborative response coordination",
        "Implement manual threat intelligence sharing with email distribution and periodic security meetings",
        "Use third-party threat intelligence platform with API integration and standard sharing protocols",
        "Deploy custom threat sharing system with manual IoC distribution and basic collaboration features"
    ],
    correct: 0,
    explanation: {
        correct: "Security Hub aggregates IoCs from multiple sources. Lambda processes and enriches threat intelligence. EventBridge distributes to multiple organizations. Step Functions coordinate collaborative responses.",
        whyWrong: {
            1: "Manual sharing with email distribution is too slow and doesn't scale for real-time threat intelligence needs",
            2: "Third-party platforms may not provide the AWS integration and automation needed for collaborative defense",
            3: "Custom threat sharing lacks the proven capabilities and automation of AWS security services"
        },
        examStrategy: "Collaborative threat intelligence: Security Hub IoC aggregation + Lambda processing + EventBridge distribution + Step Functions coordination."
    }
},
{
    id: 'dva_308',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A serverless application implements secure software supply chain with dependency scanning, code signing verification, and build artifact integrity checking. All components must be verified before deployment to production.",
    question: "What supply chain security approach provides COMPREHENSIVE verification with integrity checking?",
    options: [
        "Use CodeGuru for dependency analysis, AWS Signer for code signing, Lambda for integrity verification, and Step Functions for deployment approval workflows",
        "Implement manual code review with basic dependency checking and standard deployment procedures",
        "Use third-party supply chain security tools with API integration and manual verification",
        "Deploy custom security scanning with basic integrity checks and manual approval processes"
    ],
    correct: 0,
    explanation: {
        correct: "CodeGuru analyzes dependencies for vulnerabilities. AWS Signer provides cryptographic code signing. Lambda verifies integrity of build artifacts. Step Functions orchestrate secure deployment approvals.",
        whyWrong: {
            1: "Manual review and basic checking don't provide the comprehensive automated verification needed for supply chain security",
            2: "Third-party tools may not integrate well with AWS deployment pipelines and lack native code signing integration",
            3: "Custom scanning with manual processes lacks the automation and cryptographic verification needed"
        },
        examStrategy: "Supply chain security: CodeGuru dependency analysis + AWS Signer code signing + Lambda integrity verification + Step Functions approval."
    }
},
{
    id: 'dva_309',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A healthcare platform implements privacy-preserving contact tracing with cryptographic protocols, anonymous proximity detection, and automated exposure notification while protecting individual privacy and location data.",
    question: "Which privacy-preserving contact tracing provides ANONYMOUS detection with automated notification capabilities?",
    options: [
        "Use Lambda with private set intersection protocols, Bluetooth Low Energy proximity detection, homomorphic encryption for exposure calculation, and EventBridge for anonymous notifications",
        "Implement centralized location tracking with basic anonymization and manual contact tracing",
        "Use GPS tracking with encrypted storage and automated contact identification",
        "Deploy custom contact tracing with basic privacy controls and manual exposure notification"
    ],
    correct: 0,
    explanation: {
        correct: "Private set intersection enables anonymous contact detection. BLE proximity detection protects location privacy. Homomorphic encryption calculates exposure without revealing individual data. EventBridge provides anonymous notifications.",
        whyWrong: {
            1: "Centralized location tracking with basic anonymization doesn't provide the strong privacy guarantees needed",
            2: "GPS tracking violates privacy principles and doesn't provide the anonymous proximity detection needed",
            3: "Custom contact tracing with basic privacy controls lacks the cryptographic protections needed for healthcare privacy"
        },
        examStrategy: "Privacy-preserving contact tracing: Lambda private set intersection + BLE proximity + homomorphic encryption + anonymous EventBridge notifications."
    }
},
{
    id: 'dva_310',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function implements secure audit log streaming with tamper detection, log integrity verification, and real-time security monitoring. Audit logs must be protected from modification and provide reliable evidence for compliance.",
    question: "What audit log security approach provides TAMPER detection with real-time integrity verification?",
    options: [
        "Use CloudTrail with log file validation, Kinesis for real-time streaming, Lambda for integrity checking with hash chains, and EventBridge for tamper alerts",
        "Implement basic logging with CloudWatch and manual log review procedures",
        "Use standard syslog with encrypted transmission and periodic log analysis",
        "Deploy custom audit system with basic integrity checks and manual verification"
    ],
    correct: 0,
    explanation: {
        correct: "CloudTrail log file validation provides cryptographic integrity. Kinesis enables real-time streaming. Lambda hash chains detect tampering. EventBridge provides immediate tamper alerts for compliance.",
        whyWrong: {
            1: "Basic CloudWatch logging lacks the cryptographic integrity verification and tamper detection needed for audit logs",
            2: "Standard syslog doesn't provide the tamper detection and real-time integrity verification needed",
            3: "Custom audit systems lack the proven cryptographic capabilities of CloudTrail log file validation"
        },
        examStrategy: "Secure audit logging: CloudTrail validation + Kinesis streaming + Lambda hash chains + EventBridge tamper alerts."
    }
},
{
    id: 'dva_311',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A financial services platform implements zero-knowledge proof authentication with credential verification, privacy-preserving identity checks, and regulatory compliance validation without revealing sensitive customer information.",
    question: "Which zero-knowledge authentication provides CREDENTIAL verification with privacy-preserving compliance validation?",
    options: [
        "Use Lambda with zero-knowledge proof libraries, blockchain for credential anchoring, homomorphic encryption for compliance checks, and automated verification workflows",
        "Implement standard authentication with encrypted credential storage and manual compliance verification",
        "Use digital certificates with public key infrastructure and standard identity verification",
        "Deploy custom authentication with basic privacy controls and manual credential verification"
    ],
    correct: 0,
    explanation: {
        correct: "Zero-knowledge proofs enable credential verification without revealing data. Blockchain provides tamper-proof credential anchoring. Homomorphic encryption allows compliance checks on encrypted data. Automated workflows ensure scalability.",
        whyWrong: {
            1: "Standard authentication with encrypted storage doesn't provide zero-knowledge verification capabilities",
            2: "Digital certificates with PKI don't provide the privacy-preserving verification needed for sensitive financial data",
            3: "Custom authentication with basic privacy controls lacks the cryptographic capabilities of zero-knowledge proofs"
        },
        examStrategy: "Zero-knowledge authentication: Lambda ZK proofs + blockchain anchoring + homomorphic compliance + automated verification."
    }
},
{
    id: 'dva_312',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A serverless application implements secure multi-cloud key management with key federation, cross-cloud encryption, and unified key governance across AWS, Azure, and Google Cloud platforms.",
    question: "What multi-cloud key management provides FEDERATION with unified governance across cloud platforms?",
    options: [
        "Use AWS KMS as primary with cross-cloud federation, Lambda for key synchronization, unified policies through AWS Organizations, and CloudTrail for governance monitoring",
        "Implement separate key management per cloud with manual synchronization and basic governance",
        "Use third-party key management solution with multi-cloud connectors and standard policies",
        "Deploy custom key federation with manual governance and basic cross-cloud integration"
    ],
    correct: 0,
    explanation: {
        correct: "AWS KMS provides primary key management with federation capabilities. Lambda synchronizes keys across clouds securely. Organizations enables unified policy governance. CloudTrail provides comprehensive audit trails.",
        whyWrong: {
            1: "Separate key management per cloud creates governance gaps and operational complexity without federation benefits",
            2: "Third-party solutions may not provide the deep integration and governance capabilities needed for comprehensive multi-cloud management",
            3: "Custom key federation lacks the proven security and governance capabilities of AWS KMS and Organizations"
        },
        examStrategy: "Multi-cloud key management: KMS primary federation + Lambda synchronization + Organizations governance + CloudTrail monitoring."
    }
},
{
    id: 'dva_313',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A global healthcare platform implements secure genomic data processing with differential privacy, homomorphic encryption, and privacy-preserving genomic analysis for research collaboration while protecting individual genetic privacy.",
    question: "Which genomic privacy architecture provides DIFFERENTIAL privacy with secure collaborative research capabilities?",
    options: [
        "Use Lambda with homomorphic encryption for genomic computation, differential privacy algorithms for result protection, federated learning for collaborative analysis, and secure multi-party computation protocols",
        "Implement standard genomic analysis with basic anonymization and manual privacy controls",
        "Use encrypted genomic storage with standard analytics and basic access controls",
        "Deploy custom genomic platform with basic privacy features and manual research coordination"
    ],
    correct: 0,
    explanation: {
        correct: "Homomorphic encryption enables computation on encrypted genomic data. Differential privacy protects individual genetic information in research results. Federated learning enables collaboration without sharing raw data. SMPC protocols ensure secure computation.",
        whyWrong: {
            1: "Standard genomic analysis with basic anonymization doesn't provide the strong privacy guarantees needed for genetic data",
            2: "Encrypted storage with standard analytics doesn't enable privacy-preserving computation on genomic data",
            3: "Custom genomic platform lacks the advanced cryptographic capabilities needed for privacy-preserving genomic research"
        },
        examStrategy: "Genomic privacy: Lambda homomorphic encryption + differential privacy + federated learning + secure multi-party computation."
    }
},
{
    id: 'dva_314',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A Lambda function implements secure IoT device management with device identity verification, secure firmware updates, and anomaly detection for connected healthcare devices requiring FDA compliance.",
    question: "What IoT security approach provides DEVICE identity verification with FDA-compliant anomaly detection?",
    options: [
        "Use AWS IoT Device Management with X.509 certificates, Lambda for firmware verification, machine learning anomaly detection, and comprehensive audit logging for FDA compliance",
        "Implement basic IoT connectivity with manual device management and periodic security updates",
        "Use standard IoT protocols with encrypted communication and basic device monitoring",
        "Deploy custom IoT platform with basic security controls and manual compliance procedures"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Device Management provides secure device identity with X.509 certificates. Lambda verifies firmware integrity. ML anomaly detection identifies security threats. Comprehensive audit logging meets FDA compliance requirements.",
        whyWrong: {
            1: "Manual device management doesn't scale and lacks the security controls needed for healthcare IoT devices",
            2: "Standard IoT protocols without comprehensive security don't provide the device identity verification and anomaly detection needed",
            3: "Custom IoT platform lacks the proven security capabilities and FDA compliance features of AWS IoT services"
        },
        examStrategy: "Secure IoT device management: IoT Device Management + X.509 certificates + Lambda firmware verification + ML anomaly detection."
    }
},
{
    id: 'dva_315',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A financial trading platform implements secure order routing with transaction privacy, front-running prevention, and market manipulation detection using cryptographic techniques and behavioral analysis.",
    question: "Which secure order routing provides TRANSACTION privacy with manipulation detection capabilities?",
    options: [
        "Use Lambda with order encryption, commit-reveal schemes for transaction privacy, behavioral analytics for manipulation detection, and real-time monitoring with EventBridge",
        "Implement standard order routing with basic encryption and manual surveillance",
        "Use encrypted communication with standard trading protocols and periodic analysis",
        "Deploy custom trading system with basic privacy controls and manual monitoring"
    ],
    correct: 0,
    explanation: {
        correct: "Order encryption protects transaction details. Commit-reveal schemes prevent front-running by hiding order details until execution. Behavioral analytics detect manipulation patterns. EventBridge enables real-time monitoring.",
        whyWrong: {
            1: "Standard order routing with manual surveillance doesn't provide the privacy protections and automated detection needed",
            2: "Encrypted communication alone doesn't provide transaction privacy and manipulation detection capabilities",
            3: "Custom trading systems lack the sophisticated cryptographic and behavioral analysis capabilities needed"
        },
        examStrategy: "Secure order routing: Lambda order encryption + commit-reveal schemes + behavioral analytics + EventBridge monitoring."
    }
},
{
    id: 'dva_316',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A serverless application implements secure machine learning model deployment with model encryption, inference privacy, and adversarial attack protection for sensitive financial prediction models.",
    question: "What secure ML deployment provides MODEL encryption with adversarial attack protection?",
    options: [
        "Use Lambda with encrypted model storage, differential privacy for inference results, adversarial example detection, and secure model serving with homomorphic encryption",
        "Implement standard ML deployment with basic model protection and manual security monitoring",
        "Use SageMaker with encrypted endpoints and standard inference security",
        "Deploy custom ML platform with basic encryption and manual adversarial detection"
    ],
    correct: 0,
    explanation: {
        correct: "Encrypted model storage protects intellectual property. Differential privacy protects inference results from data extraction. Adversarial detection identifies attack attempts. Homomorphic encryption enables secure inference.",
        whyWrong: {
            1: "Standard ML deployment lacks the comprehensive security protections needed for sensitive financial models",
            2: "SageMaker encrypted endpoints provide some protection but don't include adversarial detection and inference privacy",
            3: "Custom ML platform lacks the advanced security capabilities needed for financial model protection"
        },
        examStrategy: "Secure ML model deployment: Lambda encrypted storage + differential privacy + adversarial detection + homomorphic inference."
    }
},
{
    id: 'dva_317',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global application implements privacy-preserving analytics with synthetic data generation, statistical disclosure control, and utility preservation for cross-border healthcare research collaboration.",
    question: "Which privacy-preserving analytics provides SYNTHETIC data generation with utility preservation for research collaboration?",
    options: [
        "Use Lambda with generative adversarial networks for synthetic data, differential privacy for disclosure control, utility metrics for quality assessment, and federated analytics for collaboration",
        "Implement data anonymization with manual synthetic data creation and basic privacy controls",
        "Use standard analytics with encrypted data and basic privacy protection",
        "Deploy custom analytics platform with basic synthetic data generation and manual privacy controls"
    ],
    correct: 0,
    explanation: {
        correct: "GANs generate high-quality synthetic data preserving statistical properties. Differential privacy provides formal disclosure control. Utility metrics ensure research value. Federated analytics enable collaboration without data sharing.",
        whyWrong: {
            1: "Manual synthetic data creation doesn't scale and lacks the quality needed for research applications",
            2: "Standard analytics with encryption doesn't provide synthetic data generation and privacy-preserving collaboration",
            3: "Custom analytics platform lacks the advanced synthetic data generation and privacy capabilities needed"
        },
        examStrategy: "Privacy-preserving analytics: Lambda GANs synthetic data + differential privacy + utility metrics + federated collaboration."
    }
},
{
    id: 'dva_318',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A Lambda function implements secure document processing with content-based access control, automatic classification, and information rights management for sensitive legal documents.",
    question: "What secure document processing provides CONTENT-BASED access control with automatic classification capabilities?",
    options: [
        "Use Lambda with Amazon Textract for document analysis, Comprehend for content classification, fine-grained access controls based on content sensitivity, and automated rights management",
        "Implement manual document classification with basic access controls and standard document storage",
        "Use standard document management with folder-based permissions and manual classification",
        "Deploy custom document system with basic content analysis and manual access control"
    ],
    correct: 0,
    explanation: {
        correct: "Textract extracts document content for analysis. Comprehend classifies content sensitivity automatically. Content-based access controls provide fine-grained security. Automated rights management enforces document policies.",
        whyWrong: {
            1: "Manual document classification doesn't scale and lacks the accuracy needed for content-based security",
            2: "Folder-based permissions don't provide content-based access controls and manual classification is error-prone",
            3: "Custom document systems lack the advanced NLP capabilities needed for automatic content classification"
        },
        examStrategy: "Secure document processing: Lambda + Textract analysis + Comprehend classification + content-based access + automated rights management."
    }
},
{
    id: 'dva_319',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A financial services platform implements regulatory technology (RegTech) automation with real-time compliance monitoring, automated reporting, and adaptive controls that respond to regulatory changes across multiple jurisdictions.",
    question: "Which RegTech automation provides REAL-TIME compliance monitoring with adaptive regulatory controls?",
    options: [
        "Use AWS Config for compliance monitoring, Lambda for regulatory rule processing, Step Functions for automated reporting workflows, and EventBridge for regulatory change adaptation",
        "Implement manual compliance monitoring with periodic reporting and basic regulatory tracking",
        "Use third-party RegTech solution with API integration and standard compliance workflows",
        "Deploy custom compliance system with basic monitoring and manual regulatory adaptation"
    ],
    correct: 0,
    explanation: {
        correct: "AWS Config provides real-time compliance monitoring across resources. Lambda processes complex regulatory rules. Step Functions automate comprehensive reporting. EventBridge adapts to regulatory changes automatically.",
        whyWrong: {
            1: "Manual compliance monitoring doesn't provide the real-time capabilities and automation needed for modern regulatory requirements",
            2: "Third-party RegTech solutions may not provide the AWS integration and customization needed for specific regulatory requirements",
            3: "Custom compliance systems lack the proven capabilities and automation of AWS compliance services"
        },
        examStrategy: "RegTech automation: Config real-time monitoring + Lambda rule processing + Step Functions reporting + EventBridge adaptation."
    }
},
{
    id: 'dva_320',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A serverless application implements secure communication protocols with post-quantum cryptography preparation, algorithm agility, and forward secrecy for long-term security against quantum computing threats.",
    question: "What quantum-resistant communication provides POST-QUANTUM cryptography with algorithm agility?",
    options: [
        "Use Lambda with post-quantum cryptographic libraries, hybrid classical/quantum-resistant protocols, algorithm agility through configuration, and automated cipher suite updates",
        "Implement standard TLS with classical cryptography and manual security updates",
        "Use current encryption standards with planned future quantum-resistant upgrades",
        "Deploy custom cryptographic protocols with basic quantum resistance and manual algorithm management"
    ],
    correct: 0,
    explanation: {
        correct: "Post-quantum cryptographic libraries provide quantum resistance. Hybrid protocols ensure current security with future protection. Algorithm agility enables updates without code changes. Automated updates respond to cryptographic advances.",
        whyWrong: {
            1: "Standard TLS with classical cryptography will be vulnerable to quantum computing advances",
            2: "Current standards with planned upgrades don't provide the algorithm agility needed for quantum transition",
            3: "Custom cryptographic protocols are error-prone and lack the proven security of established post-quantum algorithms"
        },
        examStrategy: "Quantum-resistant communication: Lambda post-quantum libraries + hybrid protocols + algorithm agility + automated updates."
    }
},
{
    id: 'dva_321',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A global healthcare platform implements secure multi-stakeholder data sharing with attribute-based encryption, policy-based access control, and automated consent enforcement across healthcare providers, researchers, and patients.",
    question: "Which multi-stakeholder architecture provides ATTRIBUTE-BASED encryption with automated consent enforcement?",
    options: [
        "Use Lambda with attribute-based encryption schemes, Cognito for multi-stakeholder identity, automated policy enforcement based on consent, and real-time access control decisions",
        "Implement role-based access control with manual consent management and basic data sharing",
        "Use standard encryption with permission-based access and manual consent verification",
        "Deploy custom data sharing platform with basic encryption and manual stakeholder coordination"
    ],
    correct: 0,
    explanation: {
        correct: "Attribute-based encryption enables fine-grained data sharing based on stakeholder attributes. Cognito manages multi-stakeholder identities. Automated consent enforcement ensures compliance. Real-time decisions enable dynamic access control.",
        whyWrong: {
            1: "Role-based access control doesn't provide the fine-grained attribute-based sharing needed for healthcare",
            2: "Standard encryption with manual consent doesn't scale and lacks the automated enforcement needed",
            3: "Custom data sharing platform lacks the sophisticated encryption and consent management capabilities needed"
        },
        examStrategy: "Multi-stakeholder healthcare sharing: Lambda attribute-based encryption + Cognito multi-identity + automated consent + real-time access control."
    }
},
{
    id: 'dva_322',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function implements secure edge computing with trusted execution environments, attestation protocols, and secure communication for processing sensitive data at edge locations.",
    question: "What secure edge computing provides TRUSTED execution with attestation protocols for sensitive data processing?",
    options: [
        "Use AWS Nitro Enclaves with Lambda, remote attestation for trust verification, encrypted communication channels, and secure data processing in isolated environments",
        "Implement standard edge computing with basic encryption and manual security verification",
        "Use edge devices with standard security controls and encrypted data transmission",
        "Deploy custom edge security with basic isolation and manual trust verification"
    ],
    correct: 0,
    explanation: {
        correct: "Nitro Enclaves provide trusted execution environments with hardware isolation. Remote attestation verifies enclave integrity. Encrypted communication protects data in transit. Isolated processing ensures sensitive data security.",
        whyWrong: {
            1: "Standard edge computing lacks the trusted execution environment and attestation needed for sensitive data",
            2: "Edge devices with standard controls don't provide the hardware-based trust and isolation needed",
            3: "Custom edge security lacks the proven security capabilities of hardware-based trusted execution environments"
        },
        examStrategy: "Secure edge computing: Nitro Enclaves trusted execution + remote attestation + encrypted communication + isolated processing."
    }
},
{
    id: 'dva_323',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A financial trading platform implements secure distributed consensus with Byzantine fault tolerance, cryptographic verification, and real-time transaction ordering for high-frequency trading operations.",
    question: "Which distributed consensus provides BYZANTINE fault tolerance with cryptographic verification for trading operations?",
    options: [
        "Use Lambda with Byzantine fault tolerant consensus algorithms, cryptographic signatures for transaction verification, real-time ordering protocols, and automated conflict resolution",
        "Implement standard distributed systems with basic consensus and manual conflict resolution",
        "Use blockchain-based consensus with proof-of-work and standard transaction processing",
        "Deploy custom consensus protocol with basic fault tolerance and manual verification"
    ],
    correct: 0,
    explanation: {
        correct: "Byzantine fault tolerant algorithms handle malicious node behavior. Cryptographic signatures ensure transaction authenticity. Real-time ordering protocols meet trading latency requirements. Automated conflict resolution ensures consistency.",
        whyWrong: {
            1: "Standard distributed systems with basic consensus don't provide Byzantine fault tolerance needed for financial trading",
            2: "Blockchain proof-of-work doesn't meet the real-time latency requirements for high-frequency trading",
            3: "Custom consensus protocols are complex and error-prone compared to proven Byzantine fault tolerant algorithms"
        },
        examStrategy: "Distributed trading consensus: Lambda Byzantine fault tolerance + cryptographic verification + real-time ordering + automated resolution."
    }
},
{
    id: 'dva_324',
    domain: "Domain 2: Security",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A serverless application implements secure software composition analysis with dependency risk assessment, license compliance checking, and vulnerability tracking for open source components used in financial applications.",
    question: "What software composition analysis provides DEPENDENCY risk assessment with comprehensive compliance checking?",
    options: [
        "Use CodeGuru Reviewer for dependency analysis, Lambda for license compliance verification, vulnerability database integration, and automated risk scoring for financial compliance",
        "Implement manual dependency review with basic license checking and periodic vulnerability scans",
        "Use third-party composition analysis tools with API integration and standard compliance workflows",
        "Deploy custom dependency scanning with basic vulnerability detection and manual compliance procedures"
    ],
    correct: 0,
    explanation: {
        correct: "CodeGuru Reviewer analyzes dependencies comprehensively. Lambda verifies license compliance automatically. Vulnerability database integration provides current threat data. Automated risk scoring enables financial compliance decisions.",
        whyWrong: {
            1: "Manual dependency review doesn't scale and lacks the comprehensive analysis needed for financial applications",
            2: "Third-party tools may not provide the AWS integration and financial compliance capabilities needed",
            3: "Custom dependency scanning lacks the comprehensive vulnerability databases and compliance checking needed"
        },
        examStrategy: "Software composition analysis: CodeGuru dependency analysis + Lambda license compliance + vulnerability integration + automated risk scoring."
    }
},

        // BATCH 6: 60 Expert Deployment & Performance Questions - Domain 3: Deployment (23%) + Performance Optimization
// Advanced CI/CD security, multi-region orchestration, and performance under security constraints

{
    id: 'dva_325',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global financial services platform implements secure CI/CD pipelines across multiple AWS accounts and regions with compliance validation, security scanning, and automated deployment approvals. The pipeline must handle PCI DSS compliance checks, penetration testing, and regulatory approval workflows before production deployment.",
    question: "Which secure CI/CD architecture provides COMPREHENSIVE compliance validation with automated security scanning across multi-account deployments?",
    options: [
        "Use CodePipeline with cross-account deployment, AWS Security Hub for compliance aggregation, CodeGuru for security scanning, and Step Functions for regulatory approval workflows",
        "Implement separate CI/CD pipelines per account with manual security reviews and basic compliance checking",
        "Use third-party CI/CD platform with security plugins and manual compliance validation procedures",
        "Deploy custom CI/CD solution with basic security scanning and manual approval processes"
    ],
    correct: 0,
    explanation: {
        correct: "CodePipeline cross-account deployment enables centralized security controls. Security Hub aggregates compliance findings across accounts. CodeGuru provides automated security scanning. Step Functions orchestrate complex regulatory workflows.",
        whyWrong: {
            1: "Separate pipelines per account create security gaps and don't provide centralized compliance validation",
            2: "Third-party platforms may lack deep AWS integration and compliance-specific security scanning capabilities",
            3: "Custom CI/CD solutions lack the proven security capabilities and compliance features of AWS native services"
        },
        examStrategy: "Secure multi-account CI/CD: CodePipeline cross-account + Security Hub compliance + CodeGuru scanning + Step Functions approval workflows."
    }
},
{
    id: 'dva_326',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A serverless application requires blue/green deployments with automated performance testing, security validation, and intelligent traffic shifting based on real-time metrics. The deployment must automatically rollback if performance degrades or security issues are detected.",
    question: "What blue/green deployment approach provides INTELLIGENT traffic shifting with automated security validation and performance-based rollback?",
    options: [
        "Use CodeDeploy blue/green with Lambda hooks for security validation, CloudWatch custom metrics for performance monitoring, and automated rollback triggers",
        "Implement manual blue/green deployment with basic health checks and manual traffic shifting",
        "Use API Gateway stages with manual traffic distribution and periodic performance testing",
        "Deploy separate environments with DNS switching and manual security validation"
    ],
    correct: 0,
    explanation: {
        correct: "CodeDeploy blue/green provides automated traffic shifting. Lambda hooks enable custom security validation. CloudWatch custom metrics monitor performance in real-time. Automated rollback triggers respond immediately to issues.",
        whyWrong: {
            1: "Manual blue/green deployment lacks the automation and real-time responsiveness needed for production deployments",
            2: "API Gateway stages with manual traffic control don't provide the comprehensive monitoring and automation needed",
            3: "DNS switching with manual validation is too slow for responsive rollback and lacks integrated security validation"
        },
        examStrategy: "Intelligent blue/green: CodeDeploy automation + Lambda security hooks + CloudWatch performance metrics + automated rollback triggers."
    }
},
{
    id: 'dva_327',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A multi-region serverless application implements disaster recovery with cross-region failover, data synchronization, and automated recovery testing. The system must maintain sub-second failover times while ensuring data consistency and compliance across regions.",
    question: "Which disaster recovery architecture provides SUB-SECOND failover with data consistency and automated recovery testing?",
    options: [
        "Use Route 53 health checks with failover routing, DynamoDB Global Tables for data sync, Lambda for automated testing, and EventBridge for recovery orchestration",
        "Implement manual disaster recovery with periodic data backups and manual failover procedures",
        "Use CloudFormation StackSets for infrastructure replication and manual recovery testing",
        "Deploy active/passive setup with scheduled data synchronization and quarterly DR testing"
    ],
    correct: 0,
    explanation: {
        correct: "Route 53 health checks provide sub-second failover detection. DynamoDB Global Tables ensure data consistency across regions. Lambda automates recovery testing. EventBridge orchestrates complex recovery workflows.",
        whyWrong: {
            1: "Manual disaster recovery cannot achieve sub-second failover times and lacks automation for testing",
            2: "StackSets help with infrastructure but don't provide data synchronization and automated failover capabilities",
            3: "Scheduled synchronization and quarterly testing don't meet sub-second failover and continuous validation requirements"
        },
        examStrategy: "Sub-second disaster recovery: Route 53 health checks + Global Tables sync + Lambda automated testing + EventBridge orchestration."
    }
},
{
    id: 'dva_328',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A Lambda function deployment pipeline implements infrastructure as code with security policy validation, compliance scanning, and automated remediation. CloudFormation templates must be validated against security best practices before deployment.",
    question: "What infrastructure as code approach provides SECURITY policy validation with automated remediation capabilities?",
    options: [
        "Use CloudFormation Guard for policy validation, AWS Config for compliance scanning, Systems Manager for automated remediation, and CodePipeline for integration",
        "Implement manual CloudFormation review with basic security checks and manual remediation",
        "Use third-party infrastructure scanning tools with manual policy validation and basic remediation",
        "Deploy custom policy validation with basic compliance checking and manual remediation procedures"
    ],
    correct: 0,
    explanation: {
        correct: "CloudFormation Guard validates security policies against templates. AWS Config provides continuous compliance scanning. Systems Manager automates remediation actions. CodePipeline integrates security validation into deployment workflow.",
        whyWrong: {
            1: "Manual CloudFormation review doesn't scale and lacks comprehensive security policy validation",
            2: "Third-party tools may not integrate well with AWS CloudFormation and lack automated remediation capabilities",
            3: "Custom policy validation lacks the comprehensive rule sets and automation of CloudFormation Guard and AWS Config"
        },
        examStrategy: "Secure Infrastructure as Code: CloudFormation Guard validation + Config compliance + Systems Manager remediation + CodePipeline integration."
    }
},
{
    id: 'dva_329',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A global application implements canary deployments with progressive traffic shifting, A/B testing, and automated rollback based on business metrics, performance indicators, and user experience data collected from multiple regions.",
    question: "Which canary deployment strategy provides PROGRESSIVE traffic shifting with business metric-driven rollback across multiple regions?",
    options: [
        "Use AWS App Mesh for traffic control, Lambda for metrics collection, CloudWatch custom metrics for business KPIs, and Step Functions for rollback orchestration",
        "Implement manual canary deployment with basic traffic splitting and periodic metric review",
        "Use API Gateway weighted routing with standard metrics and manual rollback decisions",
        "Deploy separate canary environments with manual traffic control and basic monitoring"
    ],
    correct: 0,
    explanation: {
        correct: "App Mesh provides sophisticated traffic control across regions. Lambda collects custom business metrics. CloudWatch custom metrics track business KPIs. Step Functions orchestrate complex rollback logic based on multiple criteria.",
        whyWrong: {
            1: "Manual canary deployment lacks the progressive automation and business metric integration needed for sophisticated deployments",
            2: "API Gateway weighted routing doesn't provide the business metric collection and automated rollback capabilities",
            3: "Separate environments with manual control don't enable progressive traffic shifting and automated business metric evaluation"
        },
        examStrategy: "Progressive canary deployment: App Mesh traffic control + Lambda metrics collection + CloudWatch business KPIs + Step Functions rollback orchestration."
    }
},
{
    id: 'dva_330',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A serverless application deployment must implement immutable infrastructure with versioned artifacts, cryptographic integrity verification, and audit trails. All deployment artifacts must be tamper-proof and traceable.",
    question: "What immutable infrastructure approach provides CRYPTOGRAPHIC integrity with comprehensive audit trails?",
    options: [
        "Use ECR with image signing, S3 with object versioning and integrity checking, CloudTrail for audit trails, and Lambda for deployment verification",
        "Implement basic artifact storage with version control and manual integrity checking",
        "Use CodeArtifact with standard versioning and basic audit logging",
        "Deploy custom artifact management with basic versioning and manual verification"
    ],
    correct: 0,
    explanation: {
        correct: "ECR image signing provides cryptographic integrity for container artifacts. S3 versioning and integrity checking ensure immutable storage. CloudTrail provides comprehensive audit trails. Lambda automates deployment verification.",
        whyWrong: {
            1: "Basic artifact storage lacks cryptographic integrity verification and comprehensive audit capabilities",
            2: "CodeArtifact provides versioning but doesn't include cryptographic signing and integrity verification",
            3: "Custom artifact management lacks the proven security capabilities and comprehensive audit features needed"
        },
        examStrategy: "Immutable infrastructure: ECR image signing + S3 versioned integrity + CloudTrail audit + Lambda verification automation."
    }
},
{
    id: 'dva_331',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A financial trading platform implements zero-downtime deployments with microsecond-level performance requirements, real-time health monitoring, and automated performance optimization during deployment phases.",
    question: "Which zero-downtime deployment architecture maintains MICROSECOND performance with real-time optimization during deployment?",
    options: [
        "Use Lambda provisioned concurrency with pre-warmed execution environments, X-Ray for microsecond-level tracing, ElastiCache for performance optimization, and EventBridge for deployment coordination",
        "Implement rolling deployment with basic health checks and manual performance monitoring",
        "Use blue/green deployment with standard metrics and periodic performance testing",
        "Deploy with ECS capacity providers and basic auto-scaling for performance management"
    ],
    correct: 0,
    explanation: {
        correct: "Provisioned concurrency eliminates cold starts for consistent microsecond performance. X-Ray provides detailed performance tracing. ElastiCache optimizes data access during deployments. EventBridge coordinates complex deployment workflows.",
        whyWrong: {
            1: "Rolling deployment with basic health checks cannot maintain microsecond-level performance requirements",
            2: "Blue/green with standard metrics doesn't provide the real-time optimization needed for microsecond performance",
            3: "ECS capacity providers don't address the microsecond-level performance requirements for financial trading"
        },
        examStrategy: "Microsecond-level zero-downtime: Lambda provisioned concurrency + X-Ray tracing + ElastiCache optimization + EventBridge coordination."
    }
},
{
    id: 'dva_332',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A multi-tenant SaaS platform requires tenant-specific deployments with isolated testing environments, custom configuration management, and automated tenant onboarding workflows.",
    question: "What multi-tenant deployment approach provides TENANT-SPECIFIC deployments with automated onboarding workflows?",
    options: [
        "Use CloudFormation nested stacks for tenant isolation, Parameter Store for configuration management, Step Functions for onboarding workflows, and CodePipeline for tenant-specific deployments",
        "Implement shared deployment pipeline with manual tenant configuration and basic onboarding procedures",
        "Use separate AWS accounts per tenant with individual deployment pipelines and manual setup",
        "Deploy single application with runtime tenant configuration and manual onboarding processes"
    ],
    correct: 0,
    explanation: {
        correct: "CloudFormation nested stacks provide tenant infrastructure isolation. Parameter Store manages tenant-specific configurations. Step Functions automate complex onboarding workflows. CodePipeline enables tenant-specific deployment customization.",
        whyWrong: {
            1: "Shared deployment with manual configuration doesn't provide the isolation and automation needed for multi-tenant SaaS",
            2: "Separate accounts per tenant create operational overhead and don't scale efficiently for large tenant bases",
            3: "Single application with runtime configuration doesn't provide the deployment isolation needed for different tenant requirements"
        },
        examStrategy: "Multi-tenant deployment: CloudFormation nested stacks + Parameter Store config + Step Functions onboarding + CodePipeline customization."
    }
},
{
    id: 'dva_333',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A global application implements performance-aware deployment orchestration with latency optimization, geographic traffic routing, and adaptive resource allocation based on real-time demand patterns across regions.",
    question: "Which performance-aware deployment provides LATENCY optimization with adaptive resource allocation across regions?",
    options: [
        "Use CloudFront with Lambda@Edge for edge deployment, Route 53 latency-based routing, Auto Scaling with predictive scaling, and CloudWatch cross-region metrics",
        "Implement manual deployment coordination with basic regional distribution and fixed resource allocation",
        "Use multi-region deployment with standard load balancing and manual resource management",
        "Deploy with regional load balancers and periodic resource adjustment based on utilization reports"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda@Edge enables deployment at edge locations for latency optimization. Route 53 latency-based routing directs traffic optimally. Predictive scaling adapts resources based on demand patterns. Cross-region metrics enable global optimization.",
        whyWrong: {
            1: "Manual deployment coordination cannot provide the real-time latency optimization and adaptive allocation needed",
            2: "Standard load balancing without latency optimization doesn't address geographic performance requirements",
            3: "Periodic resource adjustment is too slow for real-time adaptive allocation based on demand patterns"
        },
        examStrategy: "Performance-aware global deployment: Lambda@Edge latency optimization + Route 53 latency routing + predictive scaling + cross-region metrics."
    }
},
{
    id: 'dva_334',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A serverless application implements continuous integration with dependency vulnerability scanning, license compliance checking, and automated security patch management integrated into the deployment pipeline.",
    question: "What continuous integration approach provides COMPREHENSIVE vulnerability scanning with automated patch management?",
    options: [
        "Use CodeBuild with integrated vulnerability scanning, License Manager for compliance, Systems Manager Patch Manager for automation, and CodePipeline for CI/CD integration",
        "Implement manual dependency scanning with basic license checking and periodic security updates",
        "Use third-party scanning tools with manual integration and standard patch management procedures",
        "Deploy custom scanning solution with basic vulnerability detection and manual patch application"
    ],
    correct: 0,
    explanation: {
        correct: "CodeBuild integrated scanning catches vulnerabilities early. License Manager ensures compliance automatically. Systems Manager Patch Manager automates security updates. CodePipeline integrates security into CI/CD workflow.",
        whyWrong: {
            1: "Manual dependency scanning doesn't scale and lacks the automation needed for continuous security management",
            2: "Third-party tools may not integrate well with AWS services and lack comprehensive patch management automation",
            3: "Custom scanning solutions lack the proven capabilities and comprehensive automation of AWS security services"
        },
        examStrategy: "Secure continuous integration: CodeBuild vulnerability scanning + License Manager compliance + Patch Manager automation + CodePipeline integration."
    }
},
{
    id: 'dva_335',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A financial application requires deployment pipeline security with code signing, artifact provenance tracking, and supply chain attack prevention. Every component must be cryptographically verified before production deployment.",
    question: "Which secure deployment pipeline provides CRYPTOGRAPHIC verification with supply chain attack prevention?",
    options: [
        "Use AWS Signer for code signing, CodeGuru for provenance tracking, AWS Config for supply chain monitoring, and Lambda for verification workflows",
        "Implement basic code signing with manual verification and standard deployment security",
        "Use third-party signing solution with manual provenance tracking and basic supply chain checks",
        "Deploy custom verification system with basic signing and manual supply chain validation"
    ],
    correct: 0,
    explanation: {
        correct: "AWS Signer provides cryptographic code signing with hardware security. CodeGuru tracks artifact provenance through the pipeline. Config monitors supply chain integrity. Lambda automates verification workflows.",
        whyWrong: {
            1: "Basic code signing with manual verification doesn't provide the comprehensive supply chain protection needed",
            2: "Third-party solutions may not integrate well with AWS services and lack comprehensive provenance tracking",
            3: "Custom verification systems lack the proven security capabilities and automation of AWS signing services"
        },
        examStrategy: "Secure deployment pipeline: AWS Signer cryptographic signing + CodeGuru provenance + Config supply chain monitoring + Lambda verification."
    }
},
{
    id: 'dva_336',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function deployment requires environment-specific configuration with encryption, secret rotation, and audit logging. Configuration changes must be tracked and rolled back if issues occur.",
    question: "What configuration management approach provides ENCRYPTED configuration with secret rotation and rollback capabilities?",
    options: [
        "Use AWS AppConfig for configuration management, Secrets Manager for secret rotation, Parameter Store for encrypted values, and CloudTrail for audit logging",
        "Implement environment variables with manual secret management and basic change tracking",
        "Use configuration files in S3 with manual encryption and periodic secret updates",
        "Deploy custom configuration management with basic encryption and manual rollback procedures"
    ],
    correct: 0,
    explanation: {
        correct: "AppConfig provides configuration management with rollback capabilities. Secrets Manager automates secret rotation. Parameter Store encrypts configuration values. CloudTrail provides comprehensive audit trails.",
        whyWrong: {
            1: "Environment variables with manual management don't provide the encryption, rotation, and rollback capabilities needed",
            2: "S3 configuration files lack the automation and rollback capabilities needed for production configuration management",
            3: "Custom configuration management lacks the proven capabilities and automation of AWS configuration services"
        },
        examStrategy: "Secure configuration management: AppConfig rollback + Secrets Manager rotation + Parameter Store encryption + CloudTrail auditing."
    }
},
{
    id: 'dva_337',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global microservices platform implements service mesh deployment with traffic encryption, service discovery, and performance optimization across multiple regions with thousands of service instances.",
    question: "Which service mesh deployment provides TRAFFIC encryption with service discovery and performance optimization at scale?",
    options: [
        "Use AWS App Mesh with Envoy proxies, X-Ray for performance monitoring, Route 53 for service discovery, and CloudWatch for metrics aggregation across regions",
        "Implement manual service networking with basic encryption and manual service discovery",
        "Use ALB with target groups and basic health checking for service management",
        "Deploy custom service mesh with basic encryption and manual configuration management"
    ],
    correct: 0,
    explanation: {
        correct: "App Mesh with Envoy provides comprehensive traffic encryption and control. X-Ray monitors performance across service interactions. Route 53 enables service discovery. CloudWatch aggregates metrics across thousands of instances.",
        whyWrong: {
            1: "Manual service networking cannot scale to thousands of instances and lacks comprehensive encryption and discovery",
            2: "ALB with target groups doesn't provide service mesh capabilities like traffic encryption and advanced routing",
            3: "Custom service mesh implementation lacks the proven scalability and performance optimization of App Mesh"
        },
        examStrategy: "Service mesh at scale: App Mesh traffic encryption + X-Ray performance monitoring + Route 53 discovery + CloudWatch metrics aggregation."
    }
},
{
    id: 'dva_338',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A serverless application implements feature flag deployment with gradual rollout, A/B testing, and automated rollback based on conversion metrics and error rates from real user monitoring.",
    question: "What feature flag deployment approach provides GRADUAL rollout with automated rollback based on real user metrics?",
    options: [
        "Use AWS AppConfig feature flags with CloudWatch RUM for user monitoring, Lambda for metrics analysis, and automated rollback triggers",
        "Implement manual feature flags with basic metrics collection and manual rollback procedures",
        "Use third-party feature flag service with API integration and standard rollback mechanisms",
        "Deploy custom feature flag system with basic rollout control and manual metric evaluation"
    ],
    correct: 0,
    explanation: {
        correct: "AppConfig feature flags provide gradual rollout capabilities. CloudWatch RUM collects real user monitoring data. Lambda analyzes conversion metrics and error rates. Automated triggers enable immediate rollback based on metrics.",
        whyWrong: {
            1: "Manual feature flags lack the automation and real user monitoring needed for data-driven rollback decisions",
            2: "Third-party services may not integrate well with AWS monitoring and lack automated rollback capabilities",
            3: "Custom feature flag systems lack the proven capabilities and automation of AppConfig with CloudWatch integration"
        },
        examStrategy: "Feature flag deployment: AppConfig gradual rollout + CloudWatch RUM monitoring + Lambda metrics analysis + automated rollback triggers."
    }
},
{
    id: 'dva_339',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A financial trading platform requires sub-millisecond deployment updates with hot-swapping capabilities, zero-disruption service updates, and real-time performance monitoring during deployment transitions.",
    question: "Which hot-swap deployment architecture provides SUB-MILLISECOND updates with zero disruption and real-time monitoring?",
    options: [
        "Use Lambda provisioned concurrency with alias-based routing, X-Ray for real-time tracing, ElastiCache for hot data swapping, and EventBridge for deployment coordination",
        "Implement blue/green deployment with load balancer switching and standard health monitoring",
        "Use ECS with rolling updates and basic health checks for service transitions",
        "Deploy with auto-scaling groups and manual service replacement procedures"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda aliases enable instant traffic switching for sub-millisecond updates. Provisioned concurrency ensures zero cold start disruption. X-Ray provides real-time performance monitoring. ElastiCache enables hot data swapping.",
        whyWrong: {
            1: "Blue/green deployment with load balancer switching cannot achieve sub-millisecond update requirements",
            2: "ECS rolling updates have too much latency for sub-millisecond requirements and may cause service disruption",
            3: "Auto-scaling with manual procedures cannot provide the sub-millisecond updates and zero disruption needed"
        },
        examStrategy: "Sub-millisecond hot-swap: Lambda alias routing + provisioned concurrency + X-Ray real-time monitoring + ElastiCache hot swapping."
    }
},
{
    id: 'dva_340',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A multi-region application implements deployment pipeline orchestration with cross-region coordination, dependency management, and automated rollback across geographic regions.",
    question: "What cross-region deployment orchestration provides DEPENDENCY management with automated rollback coordination?",
    options: [
        "Use Step Functions for deployment orchestration, CloudFormation StackSets for cross-region coordination, EventBridge for dependency management, and automated rollback workflows",
        "Implement manual deployment coordination with basic cross-region scripts and manual rollback procedures",
        "Use separate deployment pipelines per region with manual coordination and basic dependency tracking",
        "Deploy with region-specific automation and manual cross-region synchronization"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions orchestrate complex cross-region deployments with dependency management. StackSets coordinate infrastructure across regions. EventBridge manages deployment dependencies. Automated workflows enable coordinated rollback.",
        whyWrong: {
            1: "Manual deployment coordination cannot handle complex cross-region dependencies and lacks automated rollback capabilities",
            2: "Separate pipelines per region create coordination challenges and don't provide comprehensive dependency management",
            3: "Manual cross-region synchronization is error-prone and cannot provide the automation needed for complex deployments"
        },
        examStrategy: "Cross-region deployment orchestration: Step Functions coordination + StackSets cross-region + EventBridge dependencies + automated rollback workflows."
    }
},
{
    id: 'dva_341',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A serverless application implements performance-based auto-scaling during deployment with predictive scaling, cost optimization, and automatic resource right-sizing based on deployment performance data.",
    question: "Which deployment auto-scaling approach provides PREDICTIVE scaling with cost optimization and automatic right-sizing?",
    options: [
        "Use Application Auto Scaling with predictive scaling policies, AWS Compute Optimizer for right-sizing recommendations, Cost Explorer for optimization, and Lambda for deployment coordination",
        "Implement basic auto-scaling with manual performance monitoring and periodic resource adjustments",
        "Use CloudWatch alarms with simple scaling policies and manual cost optimization procedures",
        "Deploy with fixed resource allocation and manual scaling based on deployment results"
    ],
    correct: 0,
    explanation: {
        correct: "Application Auto Scaling with predictive policies anticipates deployment load. Compute Optimizer provides right-sizing recommendations. Cost Explorer enables optimization. Lambda coordinates deployment-aware scaling.",
        whyWrong: {
            1: "Basic auto-scaling with manual monitoring cannot provide predictive capabilities and cost optimization automation",
            2: "Simple CloudWatch scaling lacks predictive capabilities and doesn't integrate cost optimization with deployment performance",
            3: "Fixed resource allocation doesn't provide the dynamic scaling and optimization needed for efficient deployments"
        },
        examStrategy: "Deployment auto-scaling: Application Auto Scaling predictive + Compute Optimizer right-sizing + Cost Explorer optimization + Lambda coordination."
    }
},
{
    id: 'dva_342',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function deployment requires container image optimization with layer caching, security scanning, and automated vulnerability patching integrated into the deployment pipeline.",
    question: "What container optimization approach provides LAYER caching with automated security scanning and vulnerability patching?",
    options: [
        "Use ECR with enhanced scanning, CodeBuild with layer caching, AWS Inspector for vulnerability assessment, and automated patching workflows with Systems Manager",
        "Implement basic container builds with manual security scanning and periodic vulnerability updates",
        "Use third-party container registry with standard scanning and manual patch management",
        "Deploy custom container optimization with basic caching and manual security procedures"
    ],
    correct: 0,
    explanation: {
        correct: "ECR enhanced scanning provides comprehensive vulnerability detection. CodeBuild layer caching optimizes build performance. Inspector provides continuous vulnerability assessment. Systems Manager automates patching workflows.",
        whyWrong: {
            1: "Basic container builds with manual scanning don't provide the automation and comprehensive security needed",
            2: "Third-party registries may not integrate well with AWS services and lack comprehensive vulnerability management",
            3: "Custom container optimization lacks the proven capabilities and automation of AWS container services"
        },
        examStrategy: "Container optimization: ECR enhanced scanning + CodeBuild layer caching + Inspector vulnerability assessment + Systems Manager automated patching."
    }
},
{
    id: 'dva_343',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global financial platform implements compliance-aware deployment automation with regulatory validation, data residency enforcement, and automated compliance reporting across multiple jurisdictions.",
    question: "Which compliance-aware deployment provides REGULATORY validation with data residency enforcement and automated reporting?",
    options: [
        "Use AWS Config for compliance monitoring, AWS Control Tower for governance, Step Functions for compliance workflows, and automated reporting with QuickSight dashboards",
        "Implement manual compliance checking with basic regulatory validation and periodic compliance reports",
        "Use third-party compliance platform with API integration and standard regulatory workflows",
        "Deploy custom compliance system with basic validation and manual reporting procedures"
    ],
    correct: 0,
    explanation: {
        correct: "AWS Config provides continuous compliance monitoring across jurisdictions. Control Tower enforces governance and data residency. Step Functions automate complex compliance workflows. QuickSight provides automated compliance reporting.",
        whyWrong: {
            1: "Manual compliance checking cannot handle complex multi-jurisdiction requirements and lacks automation for continuous monitoring",
            2: "Third-party platforms may not provide deep AWS integration and comprehensive regulatory validation capabilities",
            3: "Custom compliance systems lack the proven capabilities and comprehensive automation of AWS compliance services"
        },
        examStrategy: "Compliance-aware deployment: Config monitoring + Control Tower governance + Step Functions workflows + QuickSight automated reporting."
    }
},
{
    id: 'dva_344',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A serverless application implements deployment observability with distributed tracing, performance metrics collection, and automated performance regression detection during deployment phases.",
    question: "What deployment observability approach provides DISTRIBUTED tracing with automated performance regression detection?",
    options: [
        "Use X-Ray for distributed tracing, CloudWatch Insights for metrics analysis, automated baseline comparison with Lambda, and EventBridge for regression alerts",
        "Implement basic logging with manual performance monitoring and periodic regression analysis",
        "Use third-party observability platform with API integration and standard performance monitoring",
        "Deploy custom monitoring solution with basic tracing and manual regression detection"
    ],
    correct: 0,
    explanation: {
        correct: "X-Ray provides comprehensive distributed tracing across deployment phases. CloudWatch Insights analyzes performance metrics. Lambda automates baseline comparison for regression detection. EventBridge provides immediate regression alerts.",
        whyWrong: {
            1: "Basic logging with manual monitoring cannot provide distributed tracing and automated regression detection",
            2: "Third-party platforms may not integrate well with AWS services and lack comprehensive distributed tracing capabilities",
            3: "Custom monitoring solutions lack the proven capabilities and automation of AWS observability services"
        },
        examStrategy: "Deployment observability: X-Ray distributed tracing + CloudWatch Insights analysis + Lambda baseline comparison + EventBridge regression alerts."
    }
},
{
    id: 'dva_345',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A high-frequency trading platform requires atomic deployment updates with microsecond-level consistency, distributed transaction support, and immediate rollback capabilities for market-critical applications.",
    question: "Which atomic deployment architecture provides MICROSECOND consistency with distributed transaction support and immediate rollback?",
    options: [
        "Use Lambda with atomic configuration updates, DynamoDB transactions for state consistency, Redis for microsecond data access, and immediate rollback triggers",
        "Implement database-backed deployment with manual consistency checking and standard rollback procedures",
        "Use ECS with blue/green deployment and basic consistency validation",
        "Deploy with infrastructure updates and manual atomic operations"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda atomic configuration updates ensure consistency. DynamoDB transactions provide distributed transaction support. Redis enables microsecond-level data access. Immediate rollback triggers respond to consistency violations.",
        whyWrong: {
            1: "Database-backed deployment with manual checking cannot achieve microsecond-level consistency requirements",
            2: "ECS blue/green deployment doesn't provide the microsecond-level atomic operations needed for trading platforms",
            3: "Manual atomic operations cannot provide the microsecond-level consistency and automated rollback needed"
        },
        examStrategy: "Atomic microsecond deployment: Lambda atomic updates + DynamoDB transactions + Redis microsecond access + immediate rollback triggers."
    }
},
{
    id: 'dva_346',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A multi-tenant application requires tenant-aware deployment strategies with isolated testing, gradual tenant migration, and automated tenant-specific validation during deployments.",
    question: "What tenant-aware deployment provides ISOLATED testing with gradual migration and automated validation?",
    options: [
        "Use AWS Organizations for tenant isolation, CodePipeline with tenant-specific stages, automated testing with tenant validation, and Step Functions for migration orchestration",
        "Implement shared deployment pipeline with manual tenant testing and basic migration procedures",
        "Use separate deployments per tenant with manual coordination and basic validation",
        "Deploy with tenant flags and manual testing procedures for each tenant"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations provide tenant isolation for testing. CodePipeline tenant-specific stages enable customization. Automated testing validates tenant-specific requirements. Step Functions orchestrate gradual migration.",
        whyWrong: {
            1: "Shared deployment with manual testing doesn't provide the isolation and automation needed for multi-tenant validation",
            2: "Separate deployments per tenant create operational overhead and don't enable gradual migration capabilities",
            3: "Tenant flags with manual testing don't provide the isolation and automated validation needed for complex multi-tenant applications"
        },
        examStrategy: "Tenant-aware deployment: Organizations isolation + CodePipeline tenant stages + automated validation + Step Functions migration orchestration."
    }
},
{
    id: 'dva_347',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A global application implements intelligent deployment routing with latency-aware traffic distribution, performance-based region selection, and automated failover during deployment updates.",
    question: "Which intelligent routing deployment provides LATENCY-AWARE distribution with performance-based selection and automated failover?",
    options: [
        "Use Route 53 latency-based routing with health checks, CloudFront with Lambda@Edge for intelligent routing, real-time performance monitoring, and automated failover triggers",
        "Implement manual deployment routing with basic geographic distribution and manual failover procedures",
        "Use standard load balancing with fixed routing and basic health monitoring",
        "Deploy with regional endpoints and manual traffic management during updates"
    ],
    correct: 0,
    explanation: {
        correct: "Route 53 latency-based routing optimizes traffic distribution. Lambda@Edge enables intelligent routing decisions. Real-time monitoring tracks performance. Automated failover triggers respond to deployment issues.",
        whyWrong: {
            1: "Manual deployment routing cannot provide real-time latency optimization and automated failover capabilities",
            2: "Standard load balancing with fixed routing doesn't adapt to deployment performance and lacks intelligent routing",
            3: "Manual traffic management during updates is error-prone and cannot provide the automation needed for global deployments"
        },
        examStrategy: "Intelligent deployment routing: Route 53 latency routing + Lambda@Edge intelligence + real-time monitoring + automated failover triggers."
    }
},
{
    id: 'dva_348',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A serverless application implements deployment security validation with runtime security testing, penetration testing automation, and security regression detection integrated into CI/CD pipelines.",
    question: "What deployment security validation provides RUNTIME testing with automated penetration testing and regression detection?",
    options: [
        "Use AWS Security Hub for security aggregation, CodeGuru for runtime analysis, automated penetration testing with Lambda, and security regression detection",
        "Implement manual security testing with basic penetration testing and periodic security reviews",
        "Use third-party security testing tools with manual integration and standard regression checking",
        "Deploy custom security validation with basic runtime testing and manual penetration testing"
    ],
    correct: 0,
    explanation: {
        correct: "Security Hub aggregates security findings across deployment phases. CodeGuru provides runtime security analysis. Lambda automates penetration testing workflows. Automated regression detection identifies security degradation.",
        whyWrong: {
            1: "Manual security testing cannot provide the comprehensive automation and regression detection needed for CI/CD integration",
            2: "Third-party tools may not integrate well with AWS services and lack comprehensive runtime security analysis",
            3: "Custom security validation lacks the proven capabilities and automation of AWS security services"
        },
        examStrategy: "Deployment security validation: Security Hub aggregation + CodeGuru runtime analysis + Lambda automated testing + regression detection."
    }
},
{
    id: 'dva_349',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A financial services platform implements deployment chaos engineering with automated failure injection, resilience testing, and real-time system behavior analysis during deployment phases.",
    question: "Which chaos engineering deployment provides AUTOMATED failure injection with resilience testing and real-time behavior analysis?",
    options: [
        "Use AWS Fault Injection Simulator for failure scenarios, Lambda for automated testing, X-Ray for behavior analysis, and Step Functions for chaos orchestration",
        "Implement manual failure testing with basic resilience validation and periodic chaos experiments",
        "Use third-party chaos engineering tools with manual integration and standard testing procedures",
        "Deploy custom chaos testing with basic failure injection and manual behavior analysis"
    ],
    correct: 0,
    explanation: {
        correct: "Fault Injection Simulator provides controlled failure scenarios. Lambda automates resilience testing workflows. X-Ray analyzes system behavior during chaos experiments. Step Functions orchestrate complex chaos engineering workflows.",
        whyWrong: {
            1: "Manual failure testing cannot provide the comprehensive automation and real-time analysis needed for deployment chaos engineering",
            2: "Third-party tools may not integrate well with AWS services and lack comprehensive behavior analysis capabilities",
            3: "Custom chaos testing lacks the proven capabilities and automation of AWS fault injection services"
        },
        examStrategy: "Deployment chaos engineering: Fault Injection Simulator + Lambda automated testing + X-Ray behavior analysis + Step Functions orchestration."
    }
},
{
    id: 'dva_350',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A global application requires deployment performance optimization with caching strategies, CDN integration, and automated performance tuning based on real-world usage patterns.",
    question: "What deployment performance optimization provides CACHING strategies with CDN integration and automated tuning?",
    options: [
        "Use CloudFront with Lambda@Edge for caching optimization, ElastiCache for data caching, automated performance tuning with CloudWatch metrics, and deployment-aware cache warming",
        "Implement basic caching with manual performance tuning and standard CDN configuration",
        "Use standard caching strategies with manual optimization and basic CDN setup",
        "Deploy with fixed caching configuration and periodic performance reviews"
    ],
    correct: 0,
    explanation: {
        correct: "CloudFront with Lambda@Edge provides intelligent caching at edge locations. ElastiCache optimizes data access. CloudWatch metrics enable automated performance tuning. Deployment-aware cache warming optimizes post-deployment performance.",
        whyWrong: {
            1: "Basic caching with manual tuning cannot provide the automation and optimization needed for global deployment performance",
            2: "Standard caching strategies lack the intelligent optimization and deployment integration needed",
            3: "Fixed caching configuration doesn't adapt to deployment changes and usage patterns"
        },
        examStrategy: "Deployment performance optimization: CloudFront Lambda@Edge caching + ElastiCache data optimization + automated tuning + cache warming."
    }
},
{
    id: 'dva_351',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A microservices platform implements service dependency deployment with dependency graph analysis, service mesh configuration, and automated dependency validation during deployment orchestration.",
    question: "Which service dependency deployment provides DEPENDENCY graph analysis with service mesh configuration and automated validation?",
    options: [
        "Use AWS App Mesh for service mesh management, X-Ray for dependency analysis, Step Functions for deployment orchestration, and automated dependency validation workflows",
        "Implement manual service deployment with basic dependency checking and manual mesh configuration",
        "Use service discovery with manual dependency management and basic validation procedures",
        "Deploy services independently with manual dependency coordination and basic mesh setup"
    ],
    correct: 0,
    explanation: {
        correct: "App Mesh provides comprehensive service mesh configuration. X-Ray analyzes service dependencies and interactions. Step Functions orchestrate complex dependency-aware deployments. Automated validation ensures dependency compatibility.",
        whyWrong: {
            1: "Manual service deployment cannot handle complex dependency graphs and lacks automated mesh configuration",
            2: "Service discovery alone doesn't provide dependency analysis and automated validation capabilities",
            3: "Independent service deployment doesn't ensure dependency compatibility and lacks orchestration"
        },
        examStrategy: "Service dependency deployment: App Mesh configuration + X-Ray dependency analysis + Step Functions orchestration + automated validation."
    }
},
{
    id: 'dva_352',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A serverless application implements deployment cost optimization with resource right-sizing, usage-based scaling, and automated cost monitoring during deployment phases.",
    question: "What deployment cost optimization provides RESOURCE right-sizing with usage-based scaling and automated cost monitoring?",
    options: [
        "Use AWS Compute Optimizer for right-sizing recommendations, Application Auto Scaling for usage-based scaling, Cost Explorer for monitoring, and Lambda for cost optimization automation",
        "Implement manual resource sizing with basic scaling policies and periodic cost reviews",
        "Use fixed resource allocation with manual cost monitoring and basic optimization procedures",
        "Deploy with standard sizing and manual cost management based on utilization reports"
    ],
    correct: 0,
    explanation: {
        correct: "Compute Optimizer provides right-sizing recommendations based on actual usage. Application Auto Scaling adapts to usage patterns. Cost Explorer monitors deployment costs. Lambda automates cost optimization workflows.",
        whyWrong: {
            1: "Manual resource sizing cannot provide the optimization and automation needed for deployment cost management",
            2: "Fixed resource allocation doesn't adapt to deployment usage patterns and lacks cost optimization",
            3: "Standard sizing with manual management doesn't provide the right-sizing and automation needed for cost optimization"
        },
        examStrategy: "Deployment cost optimization: Compute Optimizer right-sizing + Application Auto Scaling usage-based + Cost Explorer monitoring + Lambda automation."
    }
},
{
    id: 'dva_353',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A global trading platform implements deployment latency optimization with edge computing, request routing optimization, and real-time latency monitoring across multiple geographic regions.",
    question: "Which deployment latency optimization provides EDGE computing with request routing optimization and real-time latency monitoring?",
    options: [
        "Use Lambda@Edge for edge computing, Route 53 with latency-based routing, CloudWatch RUM for real-time monitoring, and automated latency optimization workflows",
        "Implement regional deployment with basic routing and manual latency monitoring",
        "Use standard global deployment with fixed routing and periodic latency analysis",
        "Deploy with geographic distribution and manual routing optimization based on performance reports"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda@Edge enables computation at edge locations for minimal latency. Route 53 latency-based routing optimizes request distribution. CloudWatch RUM provides real-time user monitoring. Automated workflows optimize latency continuously.",
        whyWrong: {
            1: "Regional deployment with basic routing cannot provide edge computing and real-time latency optimization",
            2: "Standard deployment with fixed routing doesn't adapt to latency patterns and lacks edge computing benefits",
            3: "Manual routing optimization based on reports is too slow for real-time latency requirements"
        },
        examStrategy: "Deployment latency optimization: Lambda@Edge computing + Route 53 latency routing + CloudWatch RUM monitoring + automated optimization."
    }
},
{
    id: 'dva_354',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A serverless application implements deployment artifact management with version control, integrity verification, and automated artifact promotion through deployment stages.",
    question: "What artifact management approach provides VERSION control with integrity verification and automated promotion?",
    options: [
        "Use CodeArtifact for version control, AWS Signer for integrity verification, CodePipeline for automated promotion, and Lambda for artifact validation workflows",
        "Implement manual artifact management with basic version control and manual promotion procedures",
        "Use third-party artifact repository with standard versioning and manual promotion workflows",
        "Deploy custom artifact system with basic versioning and manual integrity checking"
    ],
    correct: 0,
    explanation: {
        correct: "CodeArtifact provides comprehensive version control for deployment artifacts. AWS Signer ensures cryptographic integrity verification. CodePipeline automates promotion through stages. Lambda validates artifacts during promotion.",
        whyWrong: {
            1: "Manual artifact management lacks the automation and comprehensive integrity verification needed",
            2: "Third-party repositories may not integrate well with AWS services and lack comprehensive promotion automation",
            3: "Custom artifact systems lack the proven capabilities and automation of AWS artifact management services"
        },
        examStrategy: "Deployment artifact management: CodeArtifact versioning + AWS Signer integrity + CodePipeline promotion + Lambda validation."
    }
},
{
    id: 'dva_355',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A financial application implements deployment compliance automation with regulatory approval workflows, audit trail generation, and automated compliance reporting for SOX, PCI DSS, and GDPR requirements.",
    question: "Which deployment compliance automation provides REGULATORY approval workflows with audit trails and automated reporting?",
    options: [
        "Use AWS Control Tower for governance, Step Functions for approval workflows, CloudTrail for audit trails, and automated compliance reporting with QuickSight dashboards",
        "Implement manual compliance workflows with basic approval procedures and periodic compliance reports",
        "Use third-party compliance platform with manual integration and standard regulatory workflows",
        "Deploy custom compliance automation with basic approval workflows and manual reporting"
    ],
    correct: 0,
    explanation: {
        correct: "Control Tower provides governance framework for regulatory compliance. Step Functions automate complex approval workflows. CloudTrail provides comprehensive audit trails. QuickSight enables automated compliance reporting.",
        whyWrong: {
            1: "Manual compliance workflows cannot handle complex regulatory requirements and lack automation for continuous compliance",
            2: "Third-party platforms may not provide deep AWS integration and comprehensive regulatory workflow capabilities",
            3: "Custom compliance automation lacks the proven capabilities and comprehensive automation of AWS compliance services"
        },
        examStrategy: "Deployment compliance automation: Control Tower governance + Step Functions approval workflows + CloudTrail audit + QuickSight reporting."
    }
},
{
    id: 'dva_356',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A global application requires deployment health monitoring with synthetic testing, real user monitoring, and automated health scoring during deployment phases.",
    question: "What deployment health monitoring provides SYNTHETIC testing with real user monitoring and automated health scoring?",
    options: [
        "Use CloudWatch Synthetics for automated testing, CloudWatch RUM for real user monitoring, health scoring with Lambda analytics, and EventBridge for health alerts",
        "Implement manual health checking with basic synthetic testing and periodic user monitoring",
        "Use third-party monitoring platform with API integration and standard health validation",
        "Deploy custom health monitoring with basic testing and manual health assessment"
    ],
    correct: 0,
    explanation: {
        correct: "CloudWatch Synthetics provides automated synthetic testing across deployment phases. CloudWatch RUM monitors real user experience. Lambda analytics generate health scores. EventBridge provides immediate health alerts.",
        whyWrong: {
            1: "Manual health checking cannot provide comprehensive synthetic testing and real user monitoring automation",
            2: "Third-party platforms may not integrate well with AWS services and lack comprehensive health scoring capabilities",
            3: "Custom health monitoring lacks the proven capabilities and automation of AWS monitoring services"
        },
        examStrategy: "Deployment health monitoring: CloudWatch Synthetics testing + RUM real user monitoring + Lambda health scoring + EventBridge alerts."
    }
},
{
    id: 'dva_357',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A trading platform implements deployment coordination with distributed locking, atomic updates across services, and consistency verification for mission-critical financial operations.",
    question: "Which deployment coordination provides DISTRIBUTED locking with atomic updates and consistency verification?",
    options: [
        "Use DynamoDB with conditional writes for distributed locking, Step Functions for atomic coordination, Lambda for consistency verification, and EventBridge for coordination events",
        "Implement manual deployment coordination with basic locking and manual consistency checking",
        "Use database-based locking with standard deployment procedures and basic consistency validation",
        "Deploy with service-specific locking and manual atomic operations"
    ],
    correct: 0,
    explanation: {
        correct: "DynamoDB conditional writes provide distributed locking for coordination. Step Functions ensure atomic updates across services. Lambda verifies consistency across deployments. EventBridge coordinates complex deployment events.",
        whyWrong: {
            1: "Manual deployment coordination cannot provide distributed locking and atomic updates needed for mission-critical operations",
            2: "Database-based locking doesn't scale to distributed systems and lacks atomic coordination capabilities",
            3: "Service-specific locking doesn't provide distributed coordination and atomic operations across services"
        },
        examStrategy: "Deployment coordination: DynamoDB distributed locking + Step Functions atomic updates + Lambda consistency verification + EventBridge coordination."
    }
},
{
    id: 'dva_358',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A serverless application implements deployment rollback automation with state preservation, data consistency maintenance, and automated rollback decision-making based on multiple failure criteria.",
    question: "What deployment rollback automation provides STATE preservation with data consistency and automated decision-making?",
    options: [
        "Use AWS Backup for state preservation, DynamoDB transactions for data consistency, Lambda for rollback decision logic, and Step Functions for rollback orchestration",
        "Implement manual rollback procedures with basic state backup and manual decision-making",
        "Use standard backup and restore with manual rollback triggers and basic consistency checking",
        "Deploy with snapshot-based rollback and manual state management"
    ],
    correct: 0,
    explanation: {
        correct: "AWS Backup preserves application state automatically. DynamoDB transactions ensure data consistency during rollback. Lambda analyzes multiple failure criteria for decisions. Step Functions orchestrate complex rollback workflows.",
        whyWrong: {
            1: "Manual rollback procedures cannot provide the automation and consistency needed for complex deployment rollback",
            2: "Standard backup with manual triggers lacks the automation and decision-making capabilities needed",
            3: "Snapshot-based rollback with manual management doesn't provide comprehensive state preservation and consistency"
        },
        examStrategy: "Deployment rollback automation: AWS Backup state preservation + DynamoDB consistency + Lambda decision logic + Step Functions orchestration."
    }
},
{
    id: 'dva_359',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A global financial platform implements deployment security orchestration with threat modeling, security testing automation, and compliance validation integrated into deployment workflows.",
    question: "Which deployment security orchestration provides THREAT modeling with automated security testing and compliance validation?",
    options: [
        "Use AWS Security Hub for threat aggregation, CodeGuru for automated security testing, AWS Config for compliance validation, and Step Functions for security orchestration workflows",
        "Implement manual security testing with basic threat assessment and periodic compliance checking",
        "Use third-party security platform with manual integration and standard security workflows",
        "Deploy custom security orchestration with basic threat detection and manual compliance validation"
    ],
    correct: 0,
    explanation: {
        correct: "Security Hub aggregates threat intelligence and security findings. CodeGuru provides automated security testing throughout deployment. Config ensures compliance validation. Step Functions orchestrate complex security workflows.",
        whyWrong: {
            1: "Manual security testing cannot provide comprehensive threat modeling and automated compliance validation",
            2: "Third-party platforms may not integrate well with AWS services and lack comprehensive security orchestration",
            3: "Custom security orchestration lacks the proven capabilities and automation of AWS security services"
        },
        examStrategy: "Deployment security orchestration: Security Hub threat aggregation + CodeGuru automated testing + Config compliance + Step Functions workflows."
    }
},
{
    id: 'dva_360',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A serverless application requires deployment testing automation with end-to-end testing, performance validation, and automated test result analysis integrated into CI/CD pipelines.",
    question: "What deployment testing automation provides END-TO-END testing with performance validation and automated result analysis?",
    options: [
        "Use CodeBuild for test execution, CloudWatch for performance metrics, Lambda for result analysis, and Step Functions for test orchestration workflows",
        "Implement manual testing procedures with basic performance monitoring and manual result evaluation",
        "Use third-party testing platform with API integration and standard test automation",
        "Deploy custom testing automation with basic end-to-end testing and manual analysis"
    ],
    correct: 0,
    explanation: {
        correct: "CodeBuild executes comprehensive end-to-end tests. CloudWatch collects performance metrics during testing. Lambda analyzes test results automatically. Step Functions orchestrate complex testing workflows.",
        whyWrong: {
            1: "Manual testing procedures cannot provide the automation and comprehensive analysis needed for CI/CD integration",
            2: "Third-party platforms may not integrate well with AWS services and lack comprehensive result analysis",
            3: "Custom testing automation lacks the proven capabilities and automation of AWS testing services"
        },
        examStrategy: "Deployment testing automation: CodeBuild test execution + CloudWatch performance metrics + Lambda result analysis + Step Functions orchestration."
    }
},
{
    id: 'dva_361',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A trading platform implements deployment performance validation with latency benchmarking, throughput testing, and automated performance regression detection for microsecond-sensitive trading operations.",
    question: "Which deployment performance validation provides LATENCY benchmarking with throughput testing and automated regression detection?",
    options: [
        "Use X-Ray for latency tracing, CloudWatch custom metrics for throughput monitoring, Lambda for regression analysis, and automated performance validation workflows",
        "Implement manual performance testing with basic latency monitoring and periodic regression analysis",
        "Use standard performance monitoring with fixed benchmarks and manual validation procedures",
        "Deploy with basic performance testing and manual regression detection"
    ],
    correct: 0,
    explanation: {
        correct: "X-Ray provides detailed latency tracing for microsecond-level analysis. CloudWatch custom metrics monitor throughput accurately. Lambda analyzes performance regression automatically. Automated workflows validate performance continuously.",
        whyWrong: {
            1: "Manual performance testing cannot provide microsecond-level latency validation and automated regression detection",
            2: "Standard monitoring with fixed benchmarks doesn't adapt to performance changes and lacks automated analysis",
            3: "Basic performance testing doesn't provide the precision and automation needed for trading platform validation"
        },
        examStrategy: "Deployment performance validation: X-Ray latency tracing + CloudWatch throughput metrics + Lambda regression analysis + automated validation."
    }
},
{
    id: 'dva_362',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A global application implements deployment monitoring integration with alerting, escalation procedures, and automated incident response for deployment-related issues.",
    question: "What deployment monitoring integration provides ALERTING with escalation procedures and automated incident response?",
    options: [
        "Use CloudWatch for monitoring, SNS for alerting, Step Functions for escalation workflows, and Lambda for automated incident response",
        "Implement basic monitoring with manual alerting and standard escalation procedures",
        "Use third-party monitoring platform with API integration and manual incident response",
        "Deploy custom monitoring with basic alerting and manual escalation management"
    ],
    correct: 0,
    explanation: {
        correct: "CloudWatch provides comprehensive deployment monitoring. SNS enables multi-channel alerting. Step Functions automate escalation workflows. Lambda responds to incidents automatically based on deployment context.",
        whyWrong: {
            1: "Basic monitoring with manual alerting cannot provide the automation and escalation needed for deployment monitoring",
            2: "Third-party platforms may not integrate well with AWS services and lack deployment-specific incident response",
            3: "Custom monitoring lacks the proven capabilities and automation of AWS monitoring services"
        },
        examStrategy: "Deployment monitoring integration: CloudWatch monitoring + SNS alerting + Step Functions escalation + Lambda automated incident response."
    }
},
{
    id: 'dva_363',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A financial services platform implements deployment governance with policy enforcement, compliance auditing, and automated governance reporting across multiple business units and regulatory frameworks.",
    question: "Which deployment governance provides POLICY enforcement with compliance auditing and automated governance reporting?",
    options: [
        "Use AWS Organizations with SCPs for policy enforcement, AWS Config for compliance auditing, CloudTrail for governance tracking, and automated reporting with QuickSight",
        "Implement manual governance procedures with basic policy checking and periodic compliance reports",
        "Use third-party governance platform with manual integration and standard compliance workflows",
        "Deploy custom governance system with basic policy enforcement and manual reporting"
    ],
    correct: 0,
    explanation: {
        correct: "Organizations SCPs enforce governance policies across business units. Config provides continuous compliance auditing. CloudTrail tracks all governance activities. QuickSight enables automated governance reporting.",
        whyWrong: {
            1: "Manual governance procedures cannot handle complex multi-unit policy enforcement and automated compliance auditing",
            2: "Third-party platforms may not provide deep AWS integration and comprehensive governance capabilities",
            3: "Custom governance systems lack the proven capabilities and comprehensive automation of AWS governance services"
        },
        examStrategy: "Deployment governance: Organizations SCP enforcement + Config compliance auditing + CloudTrail tracking + QuickSight automated reporting."
    }
},
{
    id: 'dva_364',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A serverless application requires deployment optimization with resource utilization analysis, cost-performance optimization, and automated recommendation generation for deployment improvements.",
    question: "What deployment optimization provides RESOURCE analysis with cost-performance optimization and automated recommendations?",
    options: [
        "Use AWS Compute Optimizer for resource analysis, Cost Explorer for cost-performance metrics, Lambda for optimization algorithms, and automated recommendation workflows",
        "Implement manual resource analysis with basic cost monitoring and periodic optimization reviews",
        "Use standard monitoring with manual optimization and basic recommendation procedures",
        "Deploy with fixed optimization and manual resource management"
    ],
    correct: 0,
    explanation: {
        correct: "Compute Optimizer analyzes resource utilization patterns. Cost Explorer provides cost-performance correlation. Lambda implements optimization algorithms. Automated workflows generate actionable recommendations.",
        whyWrong: {
            1: "Manual resource analysis cannot provide comprehensive optimization and automated recommendation generation",
            2: "Standard monitoring with manual optimization lacks the advanced analytics needed for deployment optimization",
            3: "Fixed optimization doesn't adapt to deployment changes and usage patterns"
        },
        examStrategy: "Deployment optimization: Compute Optimizer analysis + Cost Explorer metrics + Lambda algorithms + automated recommendation workflows."
    }
},
{
    id: 'dva_365',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A high-frequency trading platform implements deployment circuit breakers with automated failure detection, progressive deployment halting, and intelligent recovery mechanisms for mission-critical trading operations.",
    question: "Which deployment circuit breaker provides AUTOMATED failure detection with progressive halting and intelligent recovery?",
    options: [
        "Use CloudWatch alarms with custom metrics, Lambda for circuit breaker logic, Step Functions for progressive halting, and EventBridge for recovery coordination",
        "Implement manual deployment monitoring with basic failure detection and manual halting procedures",
        "Use standard health checks with manual circuit breaker controls and basic recovery procedures",
        "Deploy with fixed failure thresholds and manual recovery management"
    ],
    correct: 0,
    explanation: {
        correct: "CloudWatch custom metrics detect failures in real-time. Lambda implements intelligent circuit breaker logic. Step Functions enable progressive deployment halting. EventBridge coordinates intelligent recovery workflows.",
        whyWrong: {
            1: "Manual monitoring cannot provide the real-time failure detection and automation needed for trading platforms",
            2: "Standard health checks lack the sophistication and automation needed for circuit breaker patterns",
            3: "Fixed thresholds don't adapt to deployment conditions and manual recovery is too slow for trading operations"
        },
        examStrategy: "Deployment circuit breakers: CloudWatch failure detection + Lambda circuit logic + Step Functions progressive halting + EventBridge recovery."
    }
},
{
    id: 'dva_366',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A global application implements deployment scheduling with time zone coordination, maintenance window management, and automated scheduling conflict resolution across regions.",
    question: "What deployment scheduling provides TIME ZONE coordination with maintenance window management and conflict resolution?",
    options: [
        "Use EventBridge scheduled rules with time zone awareness, Systems Manager maintenance windows, conflict detection with Lambda, and automated resolution workflows",
        "Implement manual deployment scheduling with basic time zone handling and manual conflict resolution",
        "Use standard cron scheduling with manual time zone conversion and basic conflict checking",
        "Deploy with fixed scheduling and manual maintenance window coordination"
    ],
    correct: 0,
    explanation: {
        correct: "EventBridge scheduled rules handle time zone coordination automatically. Systems Manager maintenance windows provide structured scheduling. Lambda detects scheduling conflicts. Automated workflows resolve conflicts intelligently.",
        whyWrong: {
            1: "Manual scheduling cannot handle complex time zone coordination and conflict resolution at global scale",
            2: "Standard cron with manual conversion is error-prone and lacks automated conflict resolution",
            3: "Fixed scheduling doesn't adapt to regional requirements and maintenance needs"
        },
        examStrategy: "Deployment scheduling: EventBridge time zone rules + Systems Manager windows + Lambda conflict detection + automated resolution."
    }
},
{
    id: 'dva_367',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A financial platform implements deployment data consistency with cross-service transaction coordination, distributed state management, and automated consistency validation during deployments.",
    question: "Which deployment data consistency provides CROSS-SERVICE coordination with distributed state management and automated validation?",
    options: [
        "Use DynamoDB transactions for coordination, Step Functions for state management, Lambda for consistency validation, and EventBridge for cross-service communication",
        "Implement manual data coordination with basic state management and manual consistency checking",
        "Use database-level transactions with manual coordination and basic validation procedures",
        "Deploy with service-level consistency and manual cross-service coordination"
    ],
    correct: 0,
    explanation: {
        correct: "DynamoDB transactions provide ACID properties for coordination. Step Functions manage complex distributed state. Lambda validates consistency automatically. EventBridge enables reliable cross-service communication.",
        whyWrong: {
            1: "Manual coordination cannot handle complex cross-service consistency and lacks automated validation",
            2: "Database-level transactions don't span services and lack distributed state management",
            3: "Service-level consistency doesn't provide cross-service coordination and automated validation"
        },
        examStrategy: "Deployment data consistency: DynamoDB transactions + Step Functions state management + Lambda validation + EventBridge communication."
    }
},
{
    id: 'dva_368',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A serverless application implements deployment resource management with dynamic allocation, usage optimization, and automated resource cleanup after deployment completion.",
    question: "What deployment resource management provides DYNAMIC allocation with usage optimization and automated cleanup?",
    options: [
        "Use Application Auto Scaling for dynamic allocation, AWS Compute Optimizer for usage optimization, Lambda for cleanup automation, and CloudFormation for resource lifecycle management",
        "Implement manual resource allocation with basic optimization and manual cleanup procedures",
        "Use fixed resource allocation with standard optimization and periodic cleanup",
        "Deploy with static resources and manual lifecycle management"
    ],
    correct: 0,
    explanation: {
        correct: "Application Auto Scaling provides dynamic resource allocation based on deployment needs. Compute Optimizer optimizes usage patterns. Lambda automates cleanup workflows. CloudFormation manages complete resource lifecycle.",
        whyWrong: {
            1: "Manual resource allocation cannot provide dynamic optimization and automated cleanup needed for efficient deployment",
            2: "Fixed allocation doesn't adapt to deployment requirements and lacks automated optimization",
            3: "Static resources with manual management don't provide the efficiency and automation needed"
        },
        examStrategy: "Deployment resource management: Auto Scaling dynamic allocation + Compute Optimizer usage + Lambda cleanup + CloudFormation lifecycle."
    }
},
{
    id: 'dva_369',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A trading platform implements deployment validation with real-time trading simulation, market impact assessment, and automated deployment approval based on simulation results.",
    question: "Which deployment validation provides REAL-TIME simulation with market impact assessment and automated approval?",
    options: [
        "Use Lambda for trading simulation, CloudWatch for market metrics, machine learning models for impact assessment, and Step Functions for approval workflows",
        "Implement manual trading validation with basic simulation and manual approval procedures",
        "Use standard testing with manual market assessment and basic approval workflows",
        "Deploy with basic validation and manual trading impact evaluation"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda enables real-time trading simulation during deployment. CloudWatch captures market metrics for analysis. ML models assess market impact automatically. Step Functions automate approval based on simulation results.",
        whyWrong: {
            1: "Manual validation cannot provide real-time simulation and automated market impact assessment needed for trading",
            2: "Standard testing lacks the trading-specific simulation and market impact capabilities needed",
            3: "Basic validation doesn't provide the sophisticated simulation and automated approval needed for trading platforms"
        },
        examStrategy: "Deployment trading validation: Lambda real-time simulation + CloudWatch market metrics + ML impact assessment + Step Functions approval."
    }
},
{
    id: 'dva_370',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A global application requires deployment notification management with stakeholder alerting, status broadcasting, and automated communication workflows for deployment events.",
    question: "What deployment notification management provides STAKEHOLDER alerting with status broadcasting and automated communication workflows?",
    options: [
        "Use SNS for multi-channel alerting, EventBridge for status broadcasting, Step Functions for communication workflows, and Lambda for notification customization",
        "Implement manual notification with basic email alerts and manual status updates",
        "Use standard alerting with manual communication and basic notification procedures",
        "Deploy with fixed notifications and manual stakeholder management"
    ],
    correct: 0,
    explanation: {
        correct: "SNS provides multi-channel alerting to different stakeholder groups. EventBridge broadcasts status to multiple systems. Step Functions automate complex communication workflows. Lambda customizes notifications based on deployment context.",
        whyWrong: {
            1: "Manual notification cannot provide comprehensive stakeholder alerting and automated communication workflows",
            2: "Standard alerting lacks the customization and automation needed for complex deployment communication",
            3: "Fixed notifications don't adapt to deployment events and stakeholder requirements"
        },
        examStrategy: "Deployment notification management: SNS multi-channel alerting + EventBridge broadcasting + Step Functions workflows + Lambda customization."
    }
},
{
    id: 'dva_371',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A financial services platform implements deployment risk assessment with automated risk scoring, compliance validation, and risk-based deployment approval for regulatory compliance.",
    question: "Which deployment risk assessment provides AUTOMATED scoring with compliance validation and risk-based approval?",
    options: [
        "Use Lambda for risk scoring algorithms, AWS Config for compliance validation, machine learning models for risk prediction, and Step Functions for approval workflows",
        "Implement manual risk assessment with basic scoring and manual compliance checking",
        "Use standard risk evaluation with manual scoring and basic approval procedures",
        "Deploy with fixed risk criteria and manual assessment procedures"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda implements sophisticated risk scoring algorithms. Config validates compliance continuously. ML models predict deployment risks. Step Functions automate risk-based approval workflows.",
        whyWrong: {
            1: "Manual risk assessment cannot provide comprehensive scoring and automated compliance validation needed",
            2: "Standard evaluation lacks the automation and sophisticated risk analysis needed for financial services",
            3: "Fixed criteria don't adapt to deployment conditions and regulatory requirements"
        },
        examStrategy: "Deployment risk assessment: Lambda risk scoring + Config compliance validation + ML risk prediction + Step Functions approval workflows."
    }
},
{
    id: 'dva_372',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A serverless application implements deployment tracking with version management, dependency tracking, and automated deployment history analysis for troubleshooting and optimization.",
    question: "What deployment tracking provides VERSION management with dependency tracking and automated history analysis?",
    options: [
        "Use CodeCommit for version control, X-Ray for dependency tracking, CloudTrail for deployment history, and Lambda for automated analysis workflows",
        "Implement manual version tracking with basic dependency management and manual history analysis",
        "Use standard version control with manual dependency tracking and basic history logging",
        "Deploy with basic tracking and manual version management"
    ],
    correct: 0,
    explanation: {
        correct: "CodeCommit provides comprehensive version management. X-Ray tracks service dependencies automatically. CloudTrail captures complete deployment history. Lambda analyzes patterns for optimization.",
        whyWrong: {
            1: "Manual tracking cannot provide comprehensive dependency analysis and automated history analysis",
            2: "Standard version control lacks the dependency tracking and automated analysis needed",
            3: "Basic tracking doesn't provide the comprehensive management and analysis needed for optimization"
        },
        examStrategy: "Deployment tracking: CodeCommit version management + X-Ray dependency tracking + CloudTrail history + Lambda automated analysis."
    }
},
{
    id: 'dva_373',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global trading platform implements deployment performance isolation with tenant-specific performance testing, isolated performance validation, and automated performance SLA enforcement.",
    question: "Which deployment performance isolation provides TENANT-SPECIFIC testing with isolated validation and automated SLA enforcement?",
    options: [
        "Use separate test environments per tenant, Lambda for performance validation, CloudWatch custom metrics for SLA monitoring, and automated enforcement workflows",
        "Implement shared performance testing with basic tenant separation and manual SLA monitoring",
        "Use standard performance testing with manual tenant isolation and basic SLA checking",
        "Deploy with generic performance testing and manual SLA management"
    ],
    correct: 0,
    explanation: {
        correct: "Separate test environments ensure tenant isolation. Lambda validates performance against tenant-specific requirements. CloudWatch custom metrics monitor SLA compliance. Automated workflows enforce SLA violations.",
        whyWrong: {
            1: "Shared testing with basic separation cannot provide the isolation needed for tenant-specific performance requirements",
            2: "Manual isolation doesn't scale and lacks automated SLA enforcement capabilities",
            3: "Generic testing doesn't address tenant-specific requirements and SLA differences"
        },
        examStrategy: "Deployment performance isolation: Tenant-specific environments + Lambda validation + CloudWatch SLA monitoring + automated enforcement."
    }
},
{
    id: 'dva_374',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A serverless application requires deployment integration testing with cross-service validation, API contract testing, and automated integration verification during deployment phases.",
    question: "What deployment integration testing provides CROSS-SERVICE validation with API contract testing and automated verification?",
    options: [
        "Use CodeBuild for test execution, API Gateway for contract testing, Lambda for cross-service validation, and Step Functions for integration test orchestration",
        "Implement manual integration testing with basic cross-service validation and manual contract checking",
        "Use standard testing with manual integration validation and basic contract verification",
        "Deploy with basic integration testing and manual cross-service coordination"
    ],
    correct: 0,
    explanation: {
        correct: "CodeBuild executes comprehensive integration tests. API Gateway enables contract testing. Lambda validates cross-service interactions. Step Functions orchestrate complex integration testing workflows.",
        whyWrong: {
            1: "Manual integration testing cannot provide comprehensive cross-service validation and automated contract testing",
            2: "Standard testing lacks the automation and contract validation needed for complex integrations",
            3: "Basic testing doesn't provide the comprehensive validation and coordination needed"
        },
        examStrategy: "Deployment integration testing: CodeBuild execution + API Gateway contract testing + Lambda cross-service validation + Step Functions orchestration."
    }
},
{
    id: 'dva_375',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A financial platform implements deployment capacity planning with predictive scaling, resource forecasting, and automated capacity optimization based on deployment patterns and usage trends.",
    question: "Which deployment capacity planning provides PREDICTIVE scaling with resource forecasting and automated optimization?",
    options: [
        "Use machine learning models for capacity prediction, Application Auto Scaling with predictive policies, AWS Compute Optimizer for forecasting, and Lambda for optimization automation",
        "Implement manual capacity planning with basic scaling and manual resource forecasting",
        "Use standard capacity management with manual scaling and basic forecasting procedures",
        "Deploy with fixed capacity and manual resource management"
    ],
    correct: 0,
    explanation: {
        correct: "ML models predict capacity needs based on deployment patterns. Auto Scaling predictive policies anticipate load. Compute Optimizer provides resource forecasting. Lambda automates capacity optimization.",
        whyWrong: {
            1: "Manual capacity planning cannot provide predictive capabilities and automated optimization needed",
            2: "Standard management lacks predictive capabilities and automated forecasting needed for deployment optimization",
            3: "Fixed capacity doesn't adapt to deployment patterns and usage trends"
        },
        examStrategy: "Deployment capacity planning: ML predictive models + Auto Scaling predictive policies + Compute Optimizer forecasting + Lambda optimization."
    }
},
{
    id: 'dva_376',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A global application implements deployment quality gates with automated quality checking, performance benchmarking, and quality-based deployment progression control.",
    question: "What deployment quality gates provide AUTOMATED checking with performance benchmarking and progression control?",
    options: [
        "Use CodeGuru for quality analysis, CloudWatch for performance benchmarking, Lambda for quality scoring, and Step Functions for progression control workflows",
        "Implement manual quality gates with basic checking and manual progression control",
        "Use standard quality processes with manual benchmarking and basic gate controls",
        "Deploy with fixed quality criteria and manual progression management"
    ],
    correct: 0,
    explanation: {
        correct: "CodeGuru provides automated code quality analysis. CloudWatch benchmarks performance against baselines. Lambda calculates quality scores. Step Functions control deployment progression based on quality gates.",
        whyWrong: {
            1: "Manual quality gates cannot provide comprehensive automated checking and progression control needed",
            2: "Standard processes lack the automation and benchmarking needed for quality-based deployment control",
            3: "Fixed criteria don't adapt to deployment conditions and quality requirements"
        },
        examStrategy: "Deployment quality gates: CodeGuru quality analysis + CloudWatch benchmarking + Lambda scoring + Step Functions progression control."
    }
},
{
    id: 'dva_377',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A trading platform implements deployment state synchronization with distributed state management, consistency verification, and automated state recovery during deployment failures.",
    question: "Which deployment state synchronization provides DISTRIBUTED management with consistency verification and automated recovery?",
    options: [
        "Use DynamoDB Global Tables for distributed state, Lambda for consistency verification, automated backup and restore with AWS Backup, and Step Functions for recovery orchestration",
        "Implement manual state management with basic consistency checking and manual recovery procedures",
        "Use standard state synchronization with manual verification and basic recovery mechanisms",
        "Deploy with local state management and manual synchronization procedures"
    ],
    correct: 0,
    explanation: {
        correct: "DynamoDB Global Tables provide distributed state management with automatic replication. Lambda verifies consistency across regions. AWS Backup enables automated recovery. Step Functions orchestrate complex recovery workflows.",
        whyWrong: {
            1: "Manual state management cannot provide distributed consistency and automated recovery needed for trading platforms",
            2: "Standard synchronization lacks the distributed capabilities and automated verification needed",
            3: "Local state management doesn't provide the distributed synchronization and recovery needed"
        },
        examStrategy: "Deployment state synchronization: DynamoDB Global Tables + Lambda consistency verification + AWS Backup recovery + Step Functions orchestration."
    }
},
{
    id: 'dva_378',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A serverless application requires deployment environment management with environment provisioning, configuration management, and automated environment cleanup after deployment testing.",
    question: "What deployment environment management provides PROVISIONING with configuration management and automated cleanup?",
    options: [
        "Use CloudFormation for environment provisioning, AWS AppConfig for configuration management, Lambda for cleanup automation, and Systems Manager for environment coordination",
        "Implement manual environment setup with basic configuration and manual cleanup procedures",
        "Use standard environment management with manual provisioning and basic cleanup",
        "Deploy with static environments and manual configuration management"
    ],
    correct: 0,
    explanation: {
        correct: "CloudFormation automates environment provisioning consistently. AppConfig manages environment-specific configurations. Lambda automates cleanup workflows. Systems Manager coordinates environment operations.",
        whyWrong: {
            1: "Manual environment setup cannot provide the consistency and automation needed for deployment environment management",
            2: "Standard management lacks the automation and configuration capabilities needed",
            3: "Static environments don't provide the flexibility and automated management needed"
        },
        examStrategy: "Deployment environment management: CloudFormation provisioning + AppConfig configuration + Lambda cleanup + Systems Manager coordination."
    }
},
{
    id: 'dva_379',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A financial platform implements deployment audit automation with comprehensive audit trail generation, compliance verification, and automated audit reporting for regulatory requirements.",
    question: "Which deployment audit automation provides COMPREHENSIVE trails with compliance verification and automated reporting?",
    options: [
        "Use CloudTrail for audit logging, AWS Config for compliance verification, Lambda for audit analysis, and automated reporting with QuickSight dashboards",
        "Implement manual audit procedures with basic logging and manual compliance checking",
        "Use standard audit logging with manual verification and basic reporting procedures",
        "Deploy with basic audit trails and manual compliance management"
    ],
    correct: 0,
    explanation: {
        correct: "CloudTrail provides comprehensive audit logging for all deployment activities. Config verifies compliance continuously. Lambda analyzes audit data. QuickSight generates automated compliance reports.",
        whyWrong: {
            1: "Manual audit procedures cannot provide comprehensive trail generation and automated compliance verification",
            2: "Standard logging lacks the comprehensive verification and automated reporting needed for regulatory compliance",
            3: "Basic audit trails don't provide the compliance verification and automated reporting needed"
        },
        examStrategy: "Deployment audit automation: CloudTrail comprehensive logging + Config compliance verification + Lambda analysis + QuickSight reporting."
    }
},
{
    id: 'dva_380',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A global application implements deployment coordination with time zone awareness, regional deployment sequencing, and automated coordination across multiple geographic regions.",
    question: "What deployment coordination provides TIME ZONE awareness with regional sequencing and automated coordination?",
    options: [
        "Use EventBridge with scheduled rules for time zone coordination, Step Functions for regional sequencing, Lambda for coordination logic, and CloudWatch for monitoring",
        "Implement manual deployment coordination with basic time zone handling and manual sequencing",
        "Use standard scheduling with manual regional coordination and basic sequencing procedures",
        "Deploy with fixed timing and manual regional management"
    ],
    correct: 0,
    explanation: {
        correct: "EventBridge scheduled rules handle time zone coordination automatically. Step Functions sequence regional deployments intelligently. Lambda implements coordination logic. CloudWatch monitors coordination across regions.",
        whyWrong: {
            1: "Manual coordination cannot handle complex time zone requirements and regional sequencing at global scale",
            2: "Standard scheduling lacks the automation and regional coordination needed for global deployments",
            3: "Fixed timing doesn't adapt to regional requirements and coordination needs"
        },
        examStrategy: "Deployment coordination: EventBridge time zone rules + Step Functions regional sequencing + Lambda coordination logic + CloudWatch monitoring."
    }
},
{
    id: 'dva_381',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A trading platform implements deployment performance profiling with microsecond-level performance analysis, bottleneck identification, and automated performance optimization during deployment validation.",
    question: "Which deployment performance profiling provides MICROSECOND analysis with bottleneck identification and automated optimization?",
    options: [
        "Use X-Ray for detailed tracing, custom CloudWatch metrics for microsecond measurements, Lambda for bottleneck analysis, and automated optimization workflows",
        "Implement basic performance profiling with manual analysis and standard optimization procedures",
        "Use standard monitoring with manual profiling and basic bottleneck identification",
        "Deploy with basic performance measurement and manual optimization"
    ],
    correct: 0,
    explanation: {
        correct: "X-Ray provides detailed distributed tracing for performance analysis. Custom CloudWatch metrics capture microsecond-level measurements. Lambda analyzes bottlenecks automatically. Automated workflows optimize performance continuously.",
        whyWrong: {
            1: "Basic profiling cannot provide microsecond-level analysis and automated optimization needed for trading platforms",
            2: "Standard monitoring lacks the precision and automated analysis needed for microsecond-level profiling",
            3: "Basic measurement doesn't provide the detailed analysis and optimization needed for trading performance"
        },
        examStrategy: "Deployment performance profiling: X-Ray detailed tracing + custom microsecond metrics + Lambda bottleneck analysis + automated optimization."
    }
},
{
    id: 'dva_382',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "A serverless application requires deployment documentation automation with automated documentation generation, deployment guide creation, and documentation version management.",
    question: "What deployment documentation automation provides AUTOMATED generation with guide creation and version management?",
    options: [
        "Use CodeBuild for documentation generation, S3 for storage with versioning, Lambda for guide creation, and automated documentation workflows",
        "Implement manual documentation with basic generation and manual version control",
        "Use standard documentation tools with manual generation and basic versioning",
        "Deploy with static documentation and manual maintenance procedures"
    ],
    correct: 0,
    explanation: {
        correct: "CodeBuild automates documentation generation from deployment artifacts. S3 versioning manages documentation versions. Lambda creates deployment guides automatically. Automated workflows ensure documentation currency.",
        whyWrong: {
            1: "Manual documentation cannot provide the automation and version management needed for deployment documentation",
            2: "Standard tools lack the automation and integration needed for deployment-specific documentation",
            3: "Static documentation doesn't adapt to deployment changes and lacks automated maintenance"
        },
        examStrategy: "Deployment documentation automation: CodeBuild generation + S3 versioned storage + Lambda guide creation + automated workflows."
    }
},
{
    id: 'dva_383',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A financial services platform implements deployment secret management with dynamic secret rotation, encrypted secret distribution, and automated secret lifecycle management during deployments.",
    question: "Which deployment secret management provides DYNAMIC rotation with encrypted distribution and automated lifecycle management?",
    options: [
        "Use AWS Secrets Manager for rotation, KMS for encryption, Lambda for distribution logic, and Step Functions for lifecycle management workflows",
        "Implement manual secret management with basic rotation and manual distribution procedures",
        "Use standard secret storage with manual rotation and basic lifecycle management",
        "Deploy with static secrets and manual lifecycle procedures"
    ],
    correct: 0,
    explanation: {
        correct: "Secrets Manager provides automated secret rotation. KMS encrypts secrets at rest and in transit. Lambda implements intelligent distribution logic. Step Functions manage complex secret lifecycle workflows.",
        whyWrong: {
            1: "Manual secret management cannot provide the automation and security needed for deployment secret management",
            2: "Standard storage lacks the dynamic rotation and automated lifecycle management needed",
            3: "Static secrets don't provide the security and lifecycle management needed for financial services"
        },
        examStrategy: "Deployment secret management: Secrets Manager rotation + KMS encryption + Lambda distribution + Step Functions lifecycle management."
    }
},
{
    id: 'dva_384',
    domain: "Domain 3: Deployment",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A global application implements deployment rollout strategies with geographic rollout control, market-specific deployment timing, and automated rollout progression based on regional success metrics.",
    question: "What deployment rollout strategy provides GEOGRAPHIC control with market-specific timing and automated progression?",
    options: [
        "Use Step Functions for rollout orchestration, EventBridge for geographic coordination, CloudWatch for success metrics, and Lambda for progression logic",
        "Implement manual rollout with basic geographic control and manual progression decisions",
        "Use standard deployment with manual geographic coordination and basic progression control",
        "Deploy with fixed rollout timing and manual geographic management"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions orchestrate complex geographic rollouts. EventBridge coordinates market-specific timing. CloudWatch tracks regional success metrics. Lambda automates progression decisions based on metrics.",
        whyWrong: {
            1: "Manual rollout cannot provide the coordination and automation needed for global geographic rollouts",
            2: "Standard deployment lacks the geographic coordination and automated progression needed",
            3: "Fixed timing doesn't adapt to market-specific requirements and success metrics"
        },
        examStrategy: "Deployment rollout strategies: Step Functions orchestration + EventBridge geographic coordination + CloudWatch metrics + Lambda progression logic."
    }
},

// BATCH 7: 60 Master-level Troubleshooting & Optimization Questions - Domain 4: Troubleshooting (21%)
// Complex distributed system debugging, advanced performance analysis, and expert troubleshooting

{
    id: 'dva_385',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A distributed financial trading application experiences intermittent latency spikes where 99th percentile response times jump from 50ms to 500ms for 30-60 seconds, affecting critical trading operations. X-Ray shows all services are healthy, CloudWatch metrics appear normal, but customers report significant trading delays during these periods.",
    question: "Which advanced troubleshooting approach will MOST effectively identify the root cause of these intermittent latency spikes?",
    options: [
        "Use CloudWatch Logs Insights with correlation analysis across all services, custom metrics for queue depths, and Lambda memory utilization analysis during spike periods",
        "Implement distributed tracing with custom segments, detailed timing analysis, and correlation with external market data feeds",
        "Enable VPC Flow Logs analysis, network packet capture during spikes, and correlation with DynamoDB throttling events",
        "Use AWS X-Ray service map analysis with custom annotations, database connection pool monitoring, and garbage collection analysis"
    ],
    correct: 1,
    explanation: {
        correct: "Distributed tracing with custom segments provides detailed timing visibility into each component. External market data correlation is critical for trading apps as spikes often correlate with market volatility affecting data feed processing.",
        whyWrong: {
            0: "CloudWatch Logs correlation is good but may miss external dependencies and real-time market data impacts",
            2: "Network analysis focuses on infrastructure but trading latency spikes are often application-level issues related to data processing",
            3: "X-Ray service maps show high-level flow but custom segments with timing analysis provide more granular troubleshooting data"
        },
        examStrategy: "Intermittent latency troubleshooting: Custom distributed tracing + external dependency correlation + detailed timing analysis for trading systems."
    }
},
{
    id: 'dva_386',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A Lambda function processing large CSV files from S3 randomly fails with 'Memory limit exceeded' errors despite being allocated 3GB of memory. The function processes files ranging from 100MB to 1GB, and failures occur across different file sizes with no apparent pattern.",
    question: "What troubleshooting technique will identify the specific cause of random memory limit failures?",
    options: [
        "Implement detailed memory profiling with garbage collection logging, memory snapshots during processing, and correlation with file content characteristics",
        "Increase Lambda memory to maximum 10GB and monitor for continued failures",
        "Use CloudWatch Lambda Insights to track memory usage patterns and identify peak memory consumption",
        "Implement file streaming with chunk processing to reduce memory footprint"
    ],
    correct: 0,
    explanation: {
        correct: "Memory profiling with GC logging reveals memory leaks, object accumulation patterns, and correlation with file content (e.g., number of columns, data types) that cause random memory spikes.",
        whyWrong: {
            1: "Increasing memory masks the problem without identifying the root cause of memory usage patterns",
            2: "Lambda Insights provides aggregated metrics but not the detailed profiling needed for random failure analysis",
            3: "Streaming helps but doesn't identify why specific files cause memory issues - profiling reveals the pattern"
        },
        examStrategy: "Random Lambda memory failures: Detailed memory profiling + GC analysis + file content correlation to identify patterns."
    }
},
{
    id: 'dva_387',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A multi-region DynamoDB Global Tables setup experiences data inconsistencies where writes in one region occasionally don't appear in other regions for 5-10 minutes, despite DynamoDB Global Tables typically providing sub-second replication. The application uses strongly consistent reads and conditional writes.",
    question: "Which diagnostic approach will identify the cause of delayed Global Tables replication?",
    options: [
        "Analyze DynamoDB Streams processing latency, cross-region network connectivity, and replication metrics for each table",
        "Use CloudWatch Contributor Insights to identify hot partition keys causing replication delays and implement key sharding",
        "Monitor DynamoDB consumed capacity, throttling events, and implement exponential backoff for conditional writes",
        "Enable VPC Flow Logs analysis for cross-region traffic and correlation with DynamoDB replication timestamps"
    ],
    correct: 0,
    explanation: {
        correct: "DynamoDB Streams power Global Tables replication. Stream processing delays, network issues, or cross-region connectivity problems directly impact replication timing. Replication metrics show the actual delay sources.",
        whyWrong: {
            1: "Hot partitions affect performance but Global Tables replication delays are typically due to stream processing or network issues, not partition distribution",
            2: "Consumed capacity and throttling affect local performance but don't typically cause 5-10 minute replication delays",
            3: "VPC Flow Logs show network traffic but Global Tables replication uses AWS backbone network, not customer VPC traffic"
        },
        examStrategy: "Global Tables replication delays: DynamoDB Streams analysis + cross-region network monitoring + replication metrics correlation."
    }
},
{
    id: 'dva_388',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "API Gateway REST API returns 504 Gateway Timeout errors for 2% of requests during peak traffic, while Lambda function metrics show average execution time of 200ms and no errors. CloudWatch shows API Gateway integration timeout is set to 29 seconds.",
    question: "What is the MOST likely cause of 504 errors with normal Lambda performance metrics?",
    options: [
        "Lambda cold starts causing initialization delays exceeding API Gateway timeout limits",
        "Lambda response format errors causing API Gateway to wait for valid responses until timeout",
        "API Gateway throttling limits being exceeded during peak traffic periods",
        "VPC configuration causing network delays between API Gateway and Lambda"
    ],
    correct: 1,
    explanation: {
        correct: "504 errors with successful Lambda execution typically indicate response format issues. Lambda must return proper JSON structure with statusCode, headers, and body. Malformed responses cause API Gateway to timeout waiting for valid format.",
        whyWrong: {
            0: "Cold starts would show in Lambda duration metrics and wouldn't be limited to 2% of requests during peak traffic",
            2: "API Gateway throttling returns 429 errors, not 504 Gateway Timeout errors",
            3: "VPC delays would appear in Lambda duration metrics, but metrics show normal 200ms execution times"
        },
        examStrategy: "API Gateway 504 with normal Lambda metrics: Check Lambda response format structure (statusCode, headers, body) compliance."
    }
},
{
    id: 'dva_389',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A real-time analytics application using Kinesis Data Streams with Lambda consumers experiences periodic data loss where approximately 1% of records are not processed during high-throughput periods (100,000 records/second). Lambda error rates remain below 0.1%.",
    question: "Which investigation approach will identify the cause of data loss with low error rates?",
    options: [
        "Analyze Kinesis shard iterator expiration, Lambda concurrency limits, and dead letter queue configuration for unprocessed records",
        "Monitor Kinesis GetRecords throttling, Lambda function timeout settings, and implement enhanced fan-out for better throughput",
        "Use X-Ray tracing to track record processing flow, identify bottlenecks in Lambda execution, and analyze DynamoDB checkpoint patterns",
        "Implement custom CloudWatch metrics for record tracking, correlation with Lambda invocation patterns, and analysis of processing delays"
    ],
    correct: 0,
    explanation: {
        correct: "Data loss with low error rates often indicates shard iterator expiration (5-minute limit) during high throughput. Concurrency limits can cause processing delays. DLQ configuration reveals if records are being dropped silently.",
        whyWrong: {
            1: "GetRecords throttling would show in CloudWatch metrics, and enhanced fan-out doesn't solve iterator expiration issues",
            2: "X-Ray tracing is good for performance but data loss requires checking iterator expiration and concurrency handling",
            3: "Custom metrics help track the problem but don't address the root causes of iterator expiration and concurrency limits"
        },
        examStrategy: "Kinesis data loss troubleshooting: Shard iterator expiration analysis + Lambda concurrency limits + DLQ configuration review."
    }
},
{
    id: 'dva_390',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Lambda function connects to RDS Aurora and experiences connection timeout errors during peak hours, despite connection pooling implementation. RDS metrics show low CPU utilization and available connections, but Lambda shows intermittent database connection failures.",
    question: "What troubleshooting approach will identify the connection timeout root cause?",
    options: [
        "Analyze VPC security group rules, subnet routing tables, and NAT Gateway performance during peak hours",
        "Monitor RDS Proxy connection metrics, Lambda VPC ENI creation patterns, and database connection lifecycle",
        "Use Enhanced Monitoring for RDS to track connection states, Lambda memory allocation for connection objects, and connection pool configuration",
        "Implement detailed logging of database connection attempts, DNS resolution timing, and VPC endpoint configuration"
    ],
    correct: 1,
    explanation: {
        correct: "RDS Proxy metrics show connection pooling effectiveness. Lambda VPC ENI creation can cause delays during scaling. Database connection lifecycle analysis reveals if connections are being properly managed and reused.",
        whyWrong: {
            0: "Security groups and routing are basic network issues that would cause consistent failures, not intermittent peak-hour issues",
            2: "Enhanced Monitoring helps but the key issue is often Lambda VPC networking and connection management during scaling",
            3: "DNS and VPC endpoints are helpful but don't address the core Lambda VPC networking and connection pooling issues"
        },
        examStrategy: "Lambda RDS connection timeouts: RDS Proxy metrics + Lambda VPC ENI analysis + connection lifecycle monitoring."
    }
},
{
    id: 'dva_391',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A serverless application shows degraded performance across multiple Lambda functions simultaneously, with increased execution duration but no errors. The issue affects functions in different VPCs, different regions, and using different runtime versions, suggesting a systemic problem.",
    question: "Which systematic troubleshooting approach will identify cross-function performance degradation?",
    options: [
        "Analyze AWS Service Health Dashboard, Lambda service quotas, and account-level throttling patterns across regions",
        "Implement comprehensive X-Ray tracing across all functions, correlation with external service dependencies, and analysis of shared resource bottlenecks",
        "Monitor Lambda concurrent execution trends, cold start patterns, and correlation with deployment activities across the application",
        "Use CloudWatch Logs correlation analysis, custom performance metrics aggregation, and comparison with historical performance baselines"
    ],
    correct: 1,
    explanation: {
        correct: "Cross-function performance issues often stem from shared external dependencies (databases, APIs, third-party services). X-Ray tracing reveals these dependencies. Shared resource bottlenecks like database connections or external API limits affect multiple functions.",
        whyWrong: {
            0: "Service Health Dashboard and quotas are important but systemic performance degradation often relates to shared dependencies, not service limits",
            2: "Concurrency and cold starts are function-specific issues, not typically causing simultaneous degradation across different VPCs and regions",
            3: "Logs correlation is helpful but X-Ray distributed tracing provides better visibility into shared dependencies causing systemic issues"
        },
        examStrategy: "Systemic Lambda performance degradation: X-Ray dependency tracing + shared resource bottleneck analysis + external service correlation."
    }
},
{
    id: 'dva_392',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "A DynamoDB table experiences throttling errors despite having sufficient provisioned read/write capacity. CloudWatch shows even distribution of requests across partition keys, but throttling occurs during specific time periods with predictable patterns.",
    question: "What analysis will identify the cause of throttling with sufficient capacity and even key distribution?",
    options: [
        "Analyze partition-level metrics using CloudWatch Contributor Insights to identify sub-partition hotspots and temporal access patterns",
        "Monitor DynamoDB burst capacity consumption, request pattern analysis, and correlation with auto-scaling events",
        "Use DynamoDB Streams analysis to identify write amplification, Global Secondary Index throttling, and projection impacts",
        "Implement custom monitoring of request types (read vs write), item size distribution, and eventually consistent vs strongly consistent reads"
    ],
    correct: 1,
    explanation: {
        correct: "DynamoDB uses burst capacity for handling temporary spikes. Even with sufficient provisioned capacity, rapid consumption of burst capacity during predictable patterns (like batch jobs) can cause throttling. Auto-scaling events indicate capacity adjustments.",
        whyWrong: {
            0: "Contributor Insights helps with hot keys, but the scenario states even distribution, indicating burst capacity or auto-scaling timing issues",
            2: "Streams and GSI analysis are relevant but burst capacity consumption is more likely with predictable temporal patterns",
            3: "Request type monitoring is useful but doesn't address the core issue of burst capacity management during predictable traffic patterns"
        },
        examStrategy: "DynamoDB throttling with sufficient capacity: Burst capacity consumption analysis + auto-scaling timing correlation + predictable pattern analysis."
    }
},
{
    id: 'dva_393',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A multi-service serverless application experiences cascading failures where one service timeout causes multiple downstream services to fail, creating a chain reaction that impacts the entire application. Standard circuit breaker patterns are implemented but failures still propagate.",
    question: "Which advanced troubleshooting technique will identify why circuit breakers are not preventing cascading failures?",
    options: [
        "Analyze circuit breaker configuration parameters, failure threshold settings, and timeout values across the entire service dependency chain",
        "Use distributed tracing to map failure propagation paths, identify shared resource dependencies, and analyze bulkhead isolation effectiveness",
        "Monitor service mesh traffic patterns, implement chaos engineering to simulate failures, and analyze recovery time objectives",
        "Implement comprehensive logging of all service interactions, correlation of failure timestamps, and analysis of retry logic implementation"
    ],
    correct: 1,
    explanation: {
        correct: "Distributed tracing reveals actual failure propagation paths. Shared resources (databases, caches) can bypass circuit breakers. Bulkhead analysis shows if services are properly isolated or sharing resources that create hidden dependencies.",
        whyWrong: {
            0: "Circuit breaker configuration is important but doesn't reveal shared resource dependencies that bypass circuit protection",
            2: "Service mesh and chaos engineering are good but don't provide the visibility into actual failure paths that distributed tracing offers",
            3: "Comprehensive logging helps but distributed tracing provides better visualization of failure propagation and shared dependencies"
        },
        examStrategy: "Cascading failure troubleshooting: Distributed tracing failure paths + shared resource dependency analysis + bulkhead isolation review."
    }
},
{
    id: 'dva_394',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "Lambda function execution times vary significantly (100ms to 5000ms) for identical inputs and code paths. The variability occurs across different execution contexts and doesn't correlate with memory allocation or cold starts.",
    question: "What investigation approach will identify the cause of execution time variability with identical inputs?",
    options: [
        "Implement fine-grained timing instrumentation within function code, analyze external service response times, and correlate with Lambda execution environment characteristics",
        "Monitor Lambda concurrency metrics, analyze function versioning impacts, and review deployment artifact consistency",
        "Use X-Ray subsegment analysis for individual operations, monitor CPU utilization patterns, and analyze memory garbage collection timing",
        "Track Lambda container reuse patterns, analyze initialization time variations, and monitor function configuration changes"
    ],
    correct: 0,
    explanation: {
        correct: "Identical inputs with variable execution times often indicate external service variability (databases, APIs, network) or execution environment differences. Fine-grained internal timing isolates the variable components.",
        whyWrong: {
            1: "Concurrency and versioning issues would show patterns related to specific deployments or load, not random variability with identical inputs",
            2: "X-Ray subsegments are helpful but internal timing instrumentation provides more granular analysis of execution environment variability",
            3: "Container reuse patterns affect cold starts but the scenario excludes cold start correlation, indicating external dependency variability"
        },
        examStrategy: "Lambda execution time variability: Internal timing instrumentation + external service analysis + execution environment correlation."
    }
},
{
    id: 'dva_395',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A real-time notification system using EventBridge experiences message delivery delays where urgent alerts arrive 30-45 seconds late during peak hours, despite EventBridge targets showing normal processing times and no throttling errors.",
    question: "Which diagnostic approach will identify EventBridge delivery delay root causes?",
    options: [
        "Analyze EventBridge rule evaluation latency, custom bus processing delays, and correlation with cross-region replication timing",
        "Monitor target service queue depths, dead letter queue processing, and implement custom event timestamp tracking throughout the delivery pipeline",
        "Use CloudWatch Logs Insights to correlate event production timestamps with delivery timestamps and identify processing bottlenecks",
        "Implement distributed tracing for event flow, analyze EventBridge internal processing metrics, and monitor account-level service limits"
    ],
    correct: 1,
    explanation: {
        correct: "Event delivery delays often occur in target service processing. Queue depths reveal backlog buildup. Custom timestamp tracking through the entire pipeline identifies where delays accumulate. DLQ processing can introduce delays if not properly configured.",
        whyWrong: {
            0: "EventBridge rule evaluation is typically fast; delays are more often in target processing or queue management",
            2: "CloudWatch Logs correlation helps but doesn't provide the granular pipeline tracking needed for delivery delay analysis",
            3: "Distributed tracing is good but target service queue analysis and custom timestamps provide more specific delay identification"
        },
        examStrategy: "EventBridge delivery delays: Target service queue analysis + custom timestamp tracking + DLQ processing review."
    }
},
{
    id: 'dva_396',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "A Step Functions workflow randomly fails at different steps with 'States.TaskFailed' errors, but when individual Lambda functions are tested in isolation, they execute successfully. The failure rate increases during concurrent workflow executions.",
    question: "What Step Functions troubleshooting technique will identify the cause of random task failures?",
    options: [
        "Analyze Step Functions execution history for failure patterns, monitor Lambda concurrent execution limits, and review shared resource access patterns",
        "Implement comprehensive error handling with retry policies, increase Lambda timeout values, and enable Step Functions logging",
        "Use X-Ray integration to trace workflow execution paths, analyze task input/output data, and monitor state transition timing",
        "Review IAM role permissions for Step Functions execution, monitor service quotas, and analyze workflow definition syntax"
    ],
    correct: 0,
    explanation: {
        correct: "Random failures increasing with concurrency often indicate shared resource contention (databases, external APIs) or Lambda concurrency limits. Step Functions execution history shows patterns. Individual functions work but fail under concurrent load.",
        whyWrong: {
            1: "Error handling and timeouts are good practices but don't identify the root cause of concurrent execution failures",
            2: "X-Ray tracing helps but doesn't specifically address shared resource contention that causes concurrent workflow failures",
            3: "IAM and quotas would cause consistent failures, not random failures that correlate with concurrency levels"
        },
        examStrategy: "Step Functions random failures: Execution history pattern analysis + concurrent execution limits + shared resource contention analysis."
    }
},
{
    id: 'dva_397',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global application experiences inconsistent behavior where user actions in one region sometimes don't reflect in other regions for 10-15 minutes, despite using DynamoDB Global Tables and EventBridge cross-region replication. Regional health checks show all services as healthy.",
    question: "Which comprehensive troubleshooting approach will diagnose cross-region consistency issues?",
    options: [
        "Analyze DynamoDB Global Tables replication metrics, EventBridge cross-region delivery latency, and correlation with regional service dependencies",
        "Implement end-to-end distributed tracing across regions, monitor cross-region network latency, and analyze regional load balancer behavior",
        "Use custom application-level timestamp tracking, correlation analysis of user action timing, and regional cache invalidation patterns",
        "Monitor regional API Gateway performance, analyze cross-region DNS resolution timing, and review regional deployment consistency"
    ],
    correct: 2,
    explanation: {
        correct: "Cross-region consistency issues require application-level tracking. Custom timestamps reveal where delays occur. User action timing correlation identifies if it's data propagation or cache invalidation causing inconsistency.",
        whyWrong: {
            0: "DynamoDB and EventBridge metrics are important but don't reveal application-level consistency issues that may involve caching or business logic",
            1: "Distributed tracing and network analysis are good but miss application-specific consistency requirements and cache invalidation issues",
            3: "API Gateway and DNS are infrastructure-level but cross-region consistency issues often involve application-level data and cache management"
        },
        examStrategy: "Cross-region consistency troubleshooting: Application-level timestamp tracking + user action correlation + cache invalidation analysis."
    }
},
{
    id: 'dva_398',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "CloudWatch custom metrics from multiple Lambda functions show missing data points during peak traffic hours, even though Lambda execution logs confirm successful metric publication attempts. Standard CloudWatch metrics continue to work normally.",
    question: "What approach will identify why custom metrics are missing during peak traffic?",
    options: [
        "Analyze CloudWatch PutMetricData API throttling limits, implement exponential backoff for metric publishing, and review metric namespace usage patterns",
        "Monitor Lambda execution environment resource constraints, implement batched metric publishing, and analyze metric timestamp accuracy",
        "Use CloudWatch Logs to correlate metric publication attempts with CloudWatch API responses and analyze account-level service limits",
        "Implement custom metric validation, review metric dimension cardinality, and analyze CloudWatch service health during peak periods"
    ],
    correct: 0,
    explanation: {
        correct: "CloudWatch PutMetricData has rate limits that can be exceeded during peak traffic when multiple Lambda functions publish simultaneously. Exponential backoff and namespace analysis help identify and resolve throttling issues.",
        whyWrong: {
            1: "Lambda resource constraints would affect function execution, but logs show successful publication attempts - this is an API throttling issue",
            2: "CloudWatch Logs correlation is helpful but doesn't address the core throttling issue causing missing metrics during peak periods",
            3: "Metric validation and cardinality are important but the issue is likely API rate limiting during peak traffic, not metric format problems"
        },
        examStrategy: "Missing CloudWatch custom metrics: PutMetricData throttling analysis + exponential backoff implementation + namespace usage review."
    }
},
{
    id: 'dva_399',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A serverless data processing pipeline experiences data corruption where approximately 0.1% of records are processed incorrectly, with no error logs or failure indicators. The corruption is detected downstream and affects critical business calculations.",
    question: "Which investigation strategy will identify subtle data corruption in a serverless pipeline?",
    options: [
        "Implement data integrity checksums at each processing stage, correlation analysis of corrupted record patterns, and comparison with successful processing traces",
        "Use comprehensive X-Ray tracing with custom segments for each data transformation, analyze concurrent processing impacts, and review error handling logic",
        "Monitor Lambda function version consistency, analyze deployment timing correlation with corruption incidents, and review data serialization processes",
        "Implement detailed data validation logging, analyze external service response variations, and review Lambda execution environment differences"
    ],
    correct: 0,
    explanation: {
        correct: "Data corruption without errors requires integrity checking at each stage. Checksums identify where corruption occurs. Pattern analysis reveals if it's random or systematic. Successful trace comparison identifies processing differences.",
        whyWrong: {
            1: "X-Ray tracing shows execution flow but doesn't validate data integrity at each processing step, which is crucial for corruption detection",
            2: "Version consistency and deployment correlation are important but don't provide the data validation needed to identify corruption points",
            3: "Validation logging helps but checksums provide more definitive corruption detection and pattern analysis reveals systematic issues"
        },
        examStrategy: "Data corruption troubleshooting: Integrity checksums at each stage + corruption pattern analysis + successful processing comparison."
    }
},
{
    id: 'dva_400',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "Lambda functions using container images experience slow cold start times (10-15 seconds) despite images being under 1GB. The delay seems to occur during image download/extraction, not function initialization.",
    question: "What troubleshooting approach will optimize container image cold start performance?",
    options: [
        "Analyze ECR image layer structure, implement multi-stage builds for layer optimization, and monitor image pull timing metrics",
        "Increase Lambda memory allocation to speed up image extraction, enable provisioned concurrency, and optimize container runtime configuration",
        "Use Lambda Layers to reduce image size, implement image caching strategies, and analyze VPC networking impact on image pulls",
        "Monitor Lambda execution environment capacity, analyze concurrent cold start patterns, and implement image pre-warming strategies"
    ],
    correct: 0,
    explanation: {
        correct: "Container image cold starts depend on layer structure and pulling efficiency. Layer optimization reduces download size. Multi-stage builds minimize unnecessary content. Image pull timing metrics identify specific bottlenecks in the download process.",
        whyWrong: {
            1: "Memory allocation doesn't significantly speed up image extraction, and provisioned concurrency bypasses rather than solves the cold start issue",
            2: "Lambda Layers don't work with container images, and VPC networking doesn't typically affect ECR image pulls",
            3: "Execution environment capacity and pre-warming don't address the core image layer optimization needed for faster pulls"
        },
        examStrategy: "Container Lambda cold start optimization: ECR layer structure analysis + multi-stage build optimization + image pull timing metrics."
    }
},
{
    id: 'dva_401',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A financial application using multiple AWS services experiences transaction processing failures that only occur during market opening hours (high volatility periods). Individual services show normal metrics, but end-to-end transaction success rate drops from 99.9% to 95%.",
    question: "Which diagnostic approach will identify transaction failures during high-volatility periods?",
    options: [
        "Implement distributed transaction tracing with correlation to external market data feeds, analyze service interaction patterns during volatility, and monitor shared resource contention",
        "Analyze API Gateway throttling patterns, Lambda concurrency limits during peak periods, and DynamoDB capacity utilization correlations",
        "Use chaos engineering to simulate market volatility conditions, implement comprehensive service health monitoring, and analyze failure propagation patterns",
        "Monitor external service dependencies, implement circuit breaker pattern analysis, and correlate failures with market data volume spikes"
    ],
    correct: 0,
    explanation: {
        correct: "Transaction failures during market volatility suggest correlation with external data processing. Distributed tracing reveals transaction flow. Market data correlation identifies timing relationships. Shared resource contention increases during high-volume periods.",
        whyWrong: {
            1: "Individual service metrics are normal, indicating the issue is in transaction flow or external dependencies, not basic service limits",
            2: "Chaos engineering is good for testing but doesn't identify the specific correlation with market volatility and external data processing",
            3: "External dependencies are important but distributed transaction tracing provides better visibility into the complete transaction flow"
        },
        examStrategy: "Market volatility transaction failures: Distributed transaction tracing + market data correlation + shared resource contention analysis."
    }
},
{
    id: 'dva_402',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "SQS message processing experiences increased latency where messages sit in the queue for 5-10 seconds before Lambda consumption, despite having adequate Lambda concurrency and SQS having no visibility timeout issues.",
    question: "What SQS troubleshooting technique will identify message processing latency causes?",
    options: [
        "Analyze SQS message receive rate patterns, Lambda polling configuration, and correlation with queue depth fluctuations",
        "Monitor Lambda function initialization timing, implement batched message processing, and review error handling for failed messages",
        "Use CloudWatch Insights to analyze message timestamp patterns, implement custom metrics for processing delays, and review SQS FIFO configuration",
        "Analyze Lambda event source mapping configuration, monitor concurrent execution scaling patterns, and review message visibility settings"
    ],
    correct: 0,
    explanation: {
        correct: "Message processing latency often relates to Lambda polling frequency and patterns. Queue depth fluctuations indicate if messages are accumulating. Receive rate patterns show if Lambda is efficiently polling the queue.",
        whyWrong: {
            1: "Lambda initialization wouldn't cause messages to sit in queue - they'd be pulled but process slowly, not remain queued",
            2: "Message timestamps help but don't address Lambda polling configuration which affects how quickly messages are pulled from queue",
            3: "Event source mapping configuration is key, but receive rate patterns and queue depth provide more specific latency insights"
        },
        examStrategy: "SQS message latency troubleshooting: Message receive rate analysis + Lambda polling configuration + queue depth correlation."
    }
},
{
    id: 'dva_403',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A machine learning inference API experiences model prediction accuracy degradation over time, where initial predictions are 95% accurate but degrade to 85% after several weeks of operation, despite using the same model and identical input preprocessing.",
    question: "Which troubleshooting approach will identify the cause of ML model accuracy degradation?",
    options: [
        "Analyze model inference request patterns, monitor feature drift in input data, and implement data quality monitoring with baseline comparisons",
        "Review Lambda function memory allocation impacts on model performance, analyze model loading patterns, and monitor inference timing variations",
        "Use comprehensive logging of prediction inputs and outputs, implement A/B testing with fresh model deployments, and analyze model caching behavior",
        "Monitor external data source changes, analyze preprocessing pipeline consistency, and implement model validation scoring against known test datasets"
    ],
    correct: 0,
    explanation: {
        correct: "ML accuracy degradation typically indicates data drift - input data characteristics changing over time. Feature drift monitoring identifies changes in data distribution. Data quality monitoring with baselines reveals if inputs are still within expected ranges.",
        whyWrong: {
            1: "Memory allocation and timing don't affect accuracy, they affect performance - accuracy degradation is a data quality issue",
            2: "Logging and A/B testing help but don't identify the root cause of why the same model performs differently over time",
            3: "External data sources and validation are important but feature drift monitoring provides more specific accuracy degradation insights"
        },
        examStrategy: "ML accuracy degradation: Feature drift monitoring + data quality baselines + input data distribution analysis."
    }
},
{
    id: 'dva_404',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "API Gateway WebSocket connections randomly disconnect during active conversations, affecting real-time chat functionality. Connection logs show no obvious errors, and disconnect patterns don't correlate with message volume or user behavior.",
    question: "What WebSocket troubleshooting approach will identify random disconnection causes?",
    options: [
        "Analyze API Gateway connection duration patterns, monitor Lambda function errors during message processing, and review WebSocket route configuration",
        "Implement heartbeat/ping mechanisms with timing analysis, monitor client-side connection states, and correlate disconnections with backend processing delays",
        "Use CloudWatch Logs to track connection lifecycle events, analyze connection management code for resource leaks, and monitor API Gateway quotas",
        "Review WebSocket authentication timeout settings, analyze connection idle timeout configurations, and monitor network connectivity patterns"
    ],
    correct: 1,
    explanation: {
        correct: "Random WebSocket disconnections often relate to missing heartbeat mechanisms or backend processing delays that exceed connection timeouts. Client-side monitoring reveals if disconnections are client or server initiated. Heartbeat analysis identifies timeout issues.",
        whyWrong: {
            0: "Connection duration and routes are important but don't address the core issue of maintaining active connections during conversations",
            2: "CloudWatch Logs help but heartbeat implementation is more critical for identifying why active connections randomly drop",
            3: "Authentication and idle timeouts are relevant but heartbeat mechanisms are more important for active conversation disconnections"
        },
        examStrategy: "WebSocket random disconnections: Heartbeat mechanism analysis + client-side monitoring + backend processing delay correlation."
    }
},
{
    id: 'dva_405',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A complex serverless workflow using Step Functions, Lambda, DynamoDB, and external APIs experiences state corruption where workflow executions occasionally produce inconsistent results despite identical inputs and successful completion status.",
    question: "Which advanced troubleshooting technique will identify workflow state corruption causes?",
    options: [
        "Implement comprehensive state validation at each workflow step, analyze concurrent execution impacts on shared state, and monitor external API response consistency",
        "Use Step Functions execution history analysis with X-Ray correlation, implement idempotency validation, and analyze retry logic implementation",
        "Monitor DynamoDB conditional write failures, analyze Lambda function concurrency impacts, and implement comprehensive audit logging",
        "Implement workflow input/output validation, analyze state machine definition consistency, and monitor service integration error patterns"
    ],
    correct: 0,
    explanation: {
        correct: "State corruption requires validation at each step to identify where inconsistency occurs. Concurrent executions can interfere with shared state. External API response variations can cause state inconsistency even with successful status.",
        whyWrong: {
            1: "Step Functions history and X-Ray show execution flow but don't validate state consistency at each step where corruption might occur",
            2: "DynamoDB and Lambda monitoring are important but comprehensive state validation identifies exactly where corruption happens",
            3: "Input/output validation helps but doesn't address concurrent execution and external API impacts on state consistency"
        },
        examStrategy: "Workflow state corruption: Step-by-step state validation + concurrent execution analysis + external API consistency monitoring."
    }
},
{
    id: 'dva_406',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "Lambda functions in a VPC experience intermittent DNS resolution failures when connecting to external services, despite VPC DNS settings being correctly configured. The failures occur unpredictably across different external domains.",
    question: "What VPC networking troubleshooting approach will identify DNS resolution issues?",
    options: [
        "Analyze VPC DNS resolver query patterns, monitor ENI creation timing during Lambda scaling, and implement custom DNS resolution logging",
        "Review VPC security group rules for DNS traffic, analyze NAT Gateway performance during peak periods, and monitor Route 53 resolver endpoints",
        "Use VPC Flow Logs to track DNS query routing, implement DNS query timing analysis, and review VPC endpoint configurations",
        "Monitor Lambda execution environment DNS caching, analyze DNS query timeout configurations, and implement retry logic for DNS failures"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda VPC DNS issues often relate to ENI creation delays during scaling which can cause temporary DNS unavailability. Custom DNS logging reveals specific failure patterns. DNS resolver query patterns identify if it's a resolver capacity issue.",
        whyWrong: {
            1: "Security groups and NAT Gateway are basic networking but DNS issues in Lambda VPC are often related to ENI scaling timing",
            2: "VPC Flow Logs show network traffic but don't provide the specific DNS resolution timing needed for Lambda scaling analysis",
            3: "DNS caching and timeouts are relevant but ENI creation timing during Lambda scaling is the most common cause of intermittent DNS failures"
        },
        examStrategy: "Lambda VPC DNS issues: ENI creation timing analysis + DNS resolver patterns + custom DNS resolution logging."
    }
},
{
    id: 'dva_407',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A distributed caching layer using ElastiCache Redis experiences cache hit rate degradation from 95% to 70% over time, despite stable application usage patterns and no changes to caching logic or key expiration policies.",
    question: "Which caching troubleshooting approach will identify hit rate degradation causes?",
    options: [
        "Analyze cache key distribution patterns, monitor memory usage and eviction policies, and implement cache key lifecycle tracking",
        "Review application caching logic consistency, monitor Redis cluster performance metrics, and analyze network latency impacts on cache access",
        "Use Redis MONITOR command analysis during peak periods, implement cache warming strategies, and analyze connection pooling effectiveness",
        "Monitor cache expiration timing patterns, analyze application deployment correlation with hit rate changes, and review cache serialization consistency"
    ],
    correct: 0,
    explanation: {
        correct: "Hit rate degradation often indicates memory pressure causing premature evictions or key distribution changes. Memory usage analysis reveals if eviction policies are removing cache entries early. Key lifecycle tracking identifies if patterns have changed.",
        whyWrong: {
            1: "Application logic consistency is important but memory pressure and eviction policies are more likely causes of hit rate degradation",
            2: "MONITOR command provides insights but memory analysis and eviction patterns are more critical for hit rate issues",
            3: "Expiration timing and deployment correlation help but don't address the core memory management issues affecting hit rates"
        },
        examStrategy: "Cache hit rate degradation: Memory usage analysis + eviction policy monitoring + cache key lifecycle tracking."
    }
},
{
    id: 'dva_408',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "CloudWatch dashboard queries experience slow loading times (30+ seconds) for dashboards that previously loaded in 2-3 seconds, despite no changes to the dashboard configuration or underlying metric volume.",
    question: "What CloudWatch performance troubleshooting approach will identify dashboard loading delays?",
    options: [
        "Analyze CloudWatch API rate limiting, metric query complexity, and correlation with account-level service usage patterns",
        "Review dashboard widget configuration efficiency, implement metric query optimization, and analyze time range impact on query performance",
        "Monitor CloudWatch service health status, analyze metric data retention periods, and review dashboard sharing configurations",
        "Use CloudWatch Logs Insights to analyze query execution timing, implement dashboard caching strategies, and optimize widget refresh intervals"
    ],
    correct: 1,
    explanation: {
        correct: "Dashboard loading delays often relate to inefficient widget queries or time range selection. Widget configuration optimization (reducing metric queries, optimizing time periods) directly impacts loading speed. Time range analysis identifies if longer periods are causing delays.",
        whyWrong: {
            0: "API rate limiting would cause errors, not just slow loading; query complexity analysis is part of widget optimization",
            2: "Service health and retention periods don't typically cause gradual performance degradation of existing dashboards",
            3: "CloudWatch Logs Insights doesn't analyze dashboard query performance; widget optimization is more direct for loading delays"
        },
        examStrategy: "CloudWatch dashboard performance: Widget configuration optimization + metric query efficiency + time range impact analysis."
    }
},
{
    id: 'dva_409',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A microservices architecture experiences intermittent 'Service Unavailable' errors that propagate across multiple services in unpredictable patterns, suggesting a shared dependency failure that's difficult to isolate using standard monitoring.",
    question: "Which dependency analysis approach will identify hidden shared dependencies causing service unavailability?",
    options: [
        "Implement comprehensive distributed tracing with dependency mapping, analyze shared resource utilization patterns, and perform chaos engineering experiments",
        "Use service mesh observability to track inter-service communication, monitor shared database connection pools, and analyze DNS resolution dependencies",
        "Analyze service deployment dependency graphs, monitor shared infrastructure components, and implement circuit breaker pattern analysis",
        "Review service discovery mechanisms, analyze load balancer health check patterns, and monitor shared security token validation services"
    ],
    correct: 0,
    explanation: {
        correct: "Hidden shared dependencies require comprehensive tracing to reveal actual service interactions. Dependency mapping shows unexpected connections. Shared resource analysis identifies bottlenecks. Chaos engineering systematically tests dependencies.",
        whyWrong: {
            1: "Service mesh and database connections are visible dependencies; hidden dependencies require more comprehensive analysis",
            2: "Deployment graphs show intended dependencies but not runtime hidden dependencies that distributed tracing reveals",
            3: "Service discovery and load balancers are infrastructure-level but hidden dependencies often involve shared business logic or data services"
        },
        examStrategy: "Hidden dependency analysis: Comprehensive distributed tracing + dependency mapping + chaos engineering + shared resource analysis."
    }
},
{
    id: 'dva_410',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "Lambda function logs show successful execution with normal duration metrics, but downstream systems report missing data, indicating that some Lambda executions are not properly completing their intended operations.",
    question: "What troubleshooting approach will identify why Lambda shows success but operations don't complete?",
    options: [
        "Implement end-to-end transaction tracking with unique identifiers, analyze external service response validation, and monitor Lambda execution environment reuse patterns",
        "Use detailed X-Ray tracing for all Lambda operations, implement comprehensive error handling, and analyze timeout configurations",
        "Monitor Lambda concurrent execution patterns, analyze memory allocation impacts, and review function versioning consistency",
        "Implement custom CloudWatch metrics for operation completion, analyze database transaction logs, and review Lambda retry mechanisms"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda showing success but operations not completing suggests execution environment issues or external service failures that aren't properly caught. End-to-end tracking identifies where operations fail. Response validation ensures external calls succeeded.",
        whyWrong: {
            1: "X-Ray tracing shows execution flow but doesn't validate that external operations actually completed successfully",
            2: "Concurrency and memory don't typically cause successful executions that don't complete operations - this is a validation issue",
            3: "Custom metrics help but end-to-end transaction tracking provides more specific operation completion validation"
        },
        examStrategy: "Lambda success without completion: End-to-end transaction tracking + external service response validation + execution environment analysis."
    }
},
{
    id: 'dva_411',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A real-time data processing system using Kinesis Data Streams shows normal throughput metrics but data consumers report processing lag that increases over time, suggesting a backlog that's not visible in standard CloudWatch metrics.",
    question: "Which advanced Kinesis troubleshooting technique will identify hidden processing backlogs?",
    options: [
        "Analyze shard-level iterator lag metrics, implement custom consumer lag monitoring, and correlate processing delays with producer burst patterns",
        "Monitor Kinesis Data Streams throughput utilization, analyze consumer application scaling patterns, and review checkpoint management",
        "Use Kinesis scaling utility analysis, implement enhanced fan-out monitoring, and analyze record aggregation impacts",
        "Review consumer group coordination, monitor shard assignment patterns, and analyze processing time per record metrics"
    ],
    correct: 0,
    explanation: {
        correct: "Processing lag that's not visible in standard metrics requires shard-level analysis. Iterator lag shows how far behind consumers are. Custom lag monitoring reveals processing delays. Producer burst patterns can cause temporary backlogs.",
        whyWrong: {
            1: "Throughput utilization and scaling patterns are important but don't reveal the specific shard-level lag that causes hidden backlogs",
            2: "Enhanced fan-out and aggregation analysis help but shard-level iterator lag provides more specific backlog identification",
            3: "Consumer group coordination is relevant but iterator lag metrics provide more direct measurement of processing backlog"
        },
        examStrategy: "Hidden Kinesis processing lag: Shard-level iterator lag analysis + custom consumer lag monitoring + producer burst correlation."
    }
},
{
    id: 'dva_412',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "API Gateway custom authorizers experience increased latency during peak hours, adding 2-3 seconds to request processing time, despite the authorizer Lambda function showing normal execution metrics.",
    question: "What API Gateway authorizer troubleshooting approach will identify latency increases?",
    options: [
        "Analyze authorizer result caching configuration, monitor Lambda cold start patterns during peak traffic, and review authorizer timeout settings",
        "Implement detailed authorizer execution logging, analyze token validation timing, and monitor external identity provider response times",
        "Review API Gateway throttling patterns, analyze concurrent authorizer execution limits, and monitor Lambda provisioned concurrency utilization",
        "Use X-Ray tracing for authorizer execution flow, implement custom timing metrics, and analyze database query performance for authorization data"
    ],
    correct: 1,
    explanation: {
        correct: "Authorizer latency often relates to external identity provider delays or token validation timing. Detailed execution logging reveals where time is spent. External provider response times directly impact authorizer performance.",
        whyWrong: {
            0: "Caching and cold starts are relevant but external identity provider delays are more common causes of authorizer latency",
            2: "Throttling and concurrency limits would show errors, not just increased latency with normal execution metrics",
            3: "X-Ray tracing helps but external identity provider analysis is more specific to authorizer latency issues"
        },
        examStrategy: "API Gateway authorizer latency: External identity provider timing + token validation analysis + detailed execution logging."
    }
},
{
    id: 'dva_413',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "DynamoDB queries show normal performance metrics but application users report slow response times for specific data access patterns that worked efficiently in the past, suggesting query optimization degradation.",
    question: "Which DynamoDB performance analysis will identify query optimization issues?",
    options: [
        "Analyze query execution plans with PartiQL query optimization, monitor GSI usage patterns, and implement query performance benchmarking",
        "Use DynamoDB Streams analysis to identify write amplification, monitor item size growth patterns, and analyze projection efficiency",
        "Implement comprehensive access pattern analysis, monitor hot partition development, and analyze query filter efficiency",
        "Review table schema evolution impacts, analyze item attribute growth, and monitor consistent read vs eventually consistent read usage patterns"
    ],
    correct: 2,
    explanation: {
        correct: "Query optimization degradation often relates to access pattern changes, hot partition development, or inefficient filters. Access pattern analysis reveals if queries are hitting different partitions. Filter efficiency shows if more data is being scanned.",
        whyWrong: {
            0: "PartiQL query optimization is useful but access pattern analysis is more fundamental for identifying why previously efficient queries are now slow",
            1: "Streams analysis focuses on writes but the issue is query performance degradation, which is more about read access patterns",
            3: "Schema evolution is relevant but hot partition and filter efficiency analysis are more direct causes of query performance degradation"
        },
        examStrategy: "DynamoDB query degradation: Access pattern analysis + hot partition monitoring + query filter efficiency evaluation."
    }
},
{
    id: 'dva_414',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "Lambda functions deployed with container images experience memory leaks that cause gradual performance degradation within the same execution context, eventually leading to out-of-memory errors after processing multiple events.",
    question: "What container Lambda memory troubleshooting approach will identify and resolve memory leaks?",
    options: [
        "Implement heap dump analysis at regular intervals, monitor garbage collection patterns, and analyze object retention in global variables",
        "Increase Lambda memory allocation to maximum capacity, implement execution context rotation, and monitor container restart patterns",
        "Use Lambda runtime API to monitor memory usage, implement request batching optimization, and analyze container image layer efficiency",
        "Monitor Lambda execution environment reuse, implement memory usage logging, and optimize container initialization processes"
    ],
    correct: 0,
    explanation: {
        correct: "Memory leaks require detailed heap analysis to identify which objects are accumulating. GC patterns show if memory is being properly released. Global variables often cause memory retention across invocations in container execution contexts.",
        whyWrong: {
            1: "Increasing memory masks the problem without fixing the leak; context rotation helps but doesn't identify the leak source",
            2: "Runtime API monitoring helps but heap dump analysis provides more specific memory leak identification",
            3: "Execution environment monitoring is useful but heap analysis and GC patterns are more specific for memory leak troubleshooting"
        },
        examStrategy: "Container Lambda memory leaks: Heap dump analysis + garbage collection monitoring + global variable retention analysis."
    }
},
{
    id: 'dva_415',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A financial trading application experiences order processing failures that only occur during market volatility spikes, with complex interactions between Lambda functions, DynamoDB, and external market data feeds causing race conditions.",
    question: "Which race condition troubleshooting approach will identify order processing failures during market volatility?",
    options: [
        "Implement distributed locking analysis, monitor DynamoDB conditional write failures, and analyze external data feed timing correlation with processing failures",
        "Use comprehensive distributed tracing with timing analysis, implement order processing sequence validation, and monitor concurrent execution patterns",
        "Analyze Lambda concurrency scaling patterns during volatility, monitor DynamoDB burst capacity consumption, and implement retry logic validation",
        "Review order state machine consistency, monitor external API response timing, and analyze data feed synchronization patterns"
    ],
    correct: 1,
    explanation: {
        correct: "Race conditions require timing analysis to see execution sequence. Distributed tracing reveals overlapping executions. Order sequence validation identifies when processing steps occur out of order. Concurrent patterns show execution conflicts.",
        whyWrong: {
            0: "Distributed locking helps but doesn't identify where race conditions occur; timing analysis is more fundamental",
            2: "Concurrency scaling and burst capacity are relevant but don't reveal the specific timing conflicts causing race conditions",
            3: "State machine consistency is important but distributed timing analysis provides better race condition identification"
        },
        examStrategy: "Trading race condition analysis: Distributed timing tracing + order sequence validation + concurrent execution pattern monitoring."
    }
},
{
    id: 'dva_416',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "EventBridge custom bus routing experiences rule evaluation delays where events are processed 10-20 seconds after they're published, despite targets showing normal processing times and no throttling indicators.",
    question: "What EventBridge troubleshooting technique will identify rule evaluation delays?",
    options: [
        "Analyze EventBridge rule complexity and pattern matching efficiency, monitor custom bus capacity utilization, and implement event timestamp tracking",
        "Review target service configuration for processing delays, monitor dead letter queue accumulation, and analyze retry policy impacts",
        "Use CloudWatch Logs to correlate event publication with rule execution timing, implement custom metrics for rule evaluation latency, and monitor cross-region replication",
        "Monitor EventBridge service limits, analyze event size impacts on processing, and review IAM role permission evaluation timing"
    ],
    correct: 0,
    explanation: {
        correct: "Rule evaluation delays often relate to complex pattern matching or custom bus capacity. Rule complexity analysis identifies inefficient patterns. Bus capacity utilization shows if there's a processing backlog. Event timestamp tracking measures actual delays.",
        whyWrong: {
            1: "Target service configuration is ruled out by normal processing times; the delay is in rule evaluation, not target processing",
            2: "CloudWatch Logs correlation helps but rule complexity and bus capacity analysis are more specific to evaluation delays",
            3: "Service limits and event size are relevant but rule pattern efficiency is more likely to cause evaluation delays"
        },
        examStrategy: "EventBridge rule evaluation delays: Rule complexity analysis + custom bus capacity monitoring + event timestamp tracking."
    }
},
{
    id: 'dva_417',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A serverless API experiences response time degradation where identical requests show increasing latency over time within the same Lambda execution context, suggesting performance degradation within long-running execution environments.",
    question: "Which execution context troubleshooting approach will identify performance degradation within Lambda containers?",
    options: [
        "Implement detailed per-request timing instrumentation, analyze memory usage growth patterns, and monitor execution context lifecycle metrics",
        "Use X-Ray subsegment analysis for request processing stages, monitor Lambda concurrent execution impacts, and analyze external service dependency timing",
        "Analyze Lambda function configuration changes, monitor provisioned concurrency utilization, and review deployment artifact consistency",
        "Implement comprehensive logging of execution environment variables, monitor container reuse patterns, and analyze initialization timing variations"
    ],
    correct: 0,
    explanation: {
        correct: "Performance degradation within execution context suggests memory accumulation or resource contention. Per-request timing identifies where degradation occurs. Memory growth patterns reveal leaks. Context lifecycle metrics show execution environment health.",
        whyWrong: {
            1: "X-Ray subsegments show request flow but don't identify why performance degrades within the same execution context over time",
            2: "Configuration changes and provisioned concurrency don't explain degradation within the same execution context",
            3: "Environment variables and initialization timing don't address performance degradation during context lifetime"
        },
        examStrategy: "Lambda execution context degradation: Per-request timing instrumentation + memory growth analysis + context lifecycle monitoring."
    }
},
{
    id: 'dva_418',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "CloudFormation stack updates occasionally fail with 'Resource in use' errors for resources that appear to be properly configured for deletion, suggesting hidden dependencies or resource locks.",
    question: "What CloudFormation troubleshooting approach will identify hidden resource dependencies?",
    options: [
        "Analyze CloudFormation stack events with detailed error messages, review resource dependency graphs, and implement manual resource deletion testing",
        "Use CloudFormation drift detection to identify configuration changes, monitor resource utilization during updates, and analyze template dependency declarations",
        "Implement comprehensive resource tagging analysis, review IAM permissions for resource modification, and analyze cross-stack references",
        "Monitor AWS service limits during stack updates, analyze resource creation timing, and review CloudFormation service role permissions"
    ],
    correct: 0,
    explanation: {
        correct: "Hidden resource dependencies require detailed event analysis to understand specific 'in use' errors. Resource dependency graphs reveal unexpected connections. Manual deletion testing identifies which resources are actually blocking deletion.",
        whyWrong: {
            1: "Drift detection shows configuration changes but doesn't reveal hidden dependencies causing 'in use' errors",
            2: "Resource tagging and cross-stack references are helpful but detailed event analysis is more specific for dependency issues",
            3: "Service limits and timing don't typically cause 'Resource in use' errors - these are dependency-related issues"
        },
        examStrategy: "CloudFormation resource dependency issues: Stack event analysis + dependency graph review + manual deletion testing."
    }
},
{
    id: 'dva_419',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A distributed cache invalidation system using EventBridge and Lambda experiences consistency issues where cache invalidation events are processed successfully but cached data remains stale in some application instances.",
    question: "Which cache consistency troubleshooting approach will identify invalidation failures?",
    options: [
        "Implement cache invalidation event tracking with unique identifiers, analyze cache key versioning consistency, and monitor application instance cache synchronization",
        "Use distributed tracing for cache invalidation workflows, monitor EventBridge delivery guarantees, and analyze Lambda function execution success rates",
        "Analyze cache eviction policy effectiveness, monitor cache cluster health across regions, and implement cache warming validation",
        "Review cache invalidation timing patterns, monitor network connectivity between cache layers, and analyze cache replication mechanisms"
    ],
    correct: 0,
    explanation: {
        correct: "Cache consistency issues require tracking specific invalidation events to see which instances received them. Cache key versioning identifies if invalidation is properly applied. Instance synchronization shows if all application instances are getting updates.",
        whyWrong: {
            1: "Distributed tracing shows workflow execution but doesn't verify that cache invalidation was actually applied on each instance",
            2: "Eviction policies and cluster health are important but don't address the specific invalidation event delivery and application",
            3: "Timing patterns and network connectivity help but event tracking with unique identifiers provides more specific invalidation verification"
        },
        examStrategy: "Cache invalidation consistency: Event tracking with unique IDs + cache key versioning + application instance synchronization."
    }
},
{
    id: 'dva_420',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "S3 event notifications to Lambda experience delays where file upload events trigger Lambda functions 30-60 seconds after the actual S3 upload, despite S3 event configuration appearing correct.",
    question: "What S3 event notification troubleshooting approach will identify processing delays?",
    options: [
        "Analyze S3 event notification delivery timing, monitor Lambda function initialization patterns, and review S3 bucket notification configuration consistency",
        "Monitor S3 request patterns during upload periods, analyze Lambda concurrent execution scaling, and review VPC configuration impacts on event processing",
        "Use CloudTrail to correlate S3 API calls with Lambda invocations, implement custom timing metrics, and analyze S3 event filtering accuracy",
        "Review S3 bucket replication settings, monitor cross-region transfer timing, and analyze eventual consistency impacts on event generation"
    ],
    correct: 0,
    explanation: {
        correct: "S3 event notification delays require analysis of delivery timing from S3 to Lambda. Initialization patterns show if Lambda scaling is causing delays. Configuration consistency ensures events are properly configured for immediate delivery.",
        whyWrong: {
            1: "Request patterns and VPC configuration don't typically cause 30-60 second delays in event notification delivery",
            2: "CloudTrail correlation helps but notification delivery timing analysis is more specific to event processing delays",
            3: "Replication settings and eventual consistency don't typically cause long delays in event notification generation"
        },
        examStrategy: "S3 event notification delays: Delivery timing analysis + Lambda initialization patterns + notification configuration review."
    }
},
{
    id: 'dva_421',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A complex data transformation pipeline experiences data quality issues where input validation passes but output data contains subtle inconsistencies that affect downstream analytics and reporting accuracy.",
    question: "Which data quality troubleshooting approach will identify subtle transformation inconsistencies?",
    options: [
        "Implement comprehensive data lineage tracking, analyze transformation logic with sample data validation, and monitor statistical distribution changes in output data",
        "Use detailed logging of transformation steps, implement data checksums at pipeline stages, and analyze concurrent processing impacts on data integrity",
        "Monitor transformation performance metrics, analyze memory usage patterns during processing, and review transformation algorithm consistency",
        "Implement end-to-end data validation, analyze input data quality trends, and monitor transformation error handling effectiveness"
    ],
    correct: 0,
    explanation: {
        correct: "Subtle data inconsistencies require comprehensive lineage tracking to trace data through transformations. Sample data validation with known inputs identifies transformation logic issues. Statistical distribution analysis reveals subtle changes in output patterns.",
        whyWrong: {
            1: "Logging and checksums help but don't identify subtle logic issues in transformations; lineage tracking is more comprehensive",
            2: "Performance metrics and memory patterns don't identify data quality issues; transformation logic analysis is needed",
            3: "End-to-end validation helps but statistical distribution analysis of transformations provides more specific inconsistency identification"
        },
        examStrategy: "Data transformation quality: Comprehensive lineage tracking + sample data validation + statistical distribution analysis."
    }
},
{
    id: 'dva_422',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "Lambda functions using external API integrations experience timeout errors that only occur during specific time periods, despite the external APIs reporting normal response times and availability.",
    question: "What external API integration troubleshooting approach will identify time-specific timeout patterns?",
    options: [
        "Analyze API response timing correlation with Lambda execution patterns, monitor DNS resolution timing during problem periods, and implement retry logic validation",
        "Use comprehensive logging of API request/response cycles, monitor Lambda VPC networking performance, and analyze connection pooling effectiveness",
        "Monitor external API rate limiting patterns, analyze Lambda concurrent execution correlation with timeouts, and review API authentication timing",
        "Implement custom metrics for API response times, analyze network connectivity patterns, and monitor Lambda function timeout configuration"
    ],
    correct: 0,
    explanation: {
        correct: "Time-specific timeouts often relate to DNS resolution delays, network congestion, or API response timing changes. DNS timing analysis during problem periods identifies resolution delays. Response timing correlation reveals if API responses are actually slower despite reported normal times.",
        whyWrong: {
            1: "VPC networking and connection pooling are relevant but DNS resolution timing is more specific to time-based timeout patterns",
            2: "Rate limiting and concurrent execution correlation help but DNS and response timing analysis are more fundamental",
            3: "Custom metrics help but DNS resolution timing during specific periods is more targeted for time-specific issues"
        },
        examStrategy: "Time-specific API timeouts: DNS resolution timing analysis + API response correlation + retry logic validation."
    }
},
{
    id: 'dva_423',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A real-time recommendation engine using machine learning models experiences recommendation quality degradation during peak traffic hours, despite model inference latency remaining within acceptable limits.",
    question: "Which ML recommendation troubleshooting approach will identify quality degradation during peak traffic?",
    options: [
        "Analyze feature engineering pipeline consistency during high load, monitor data freshness impacts on model accuracy, and implement recommendation diversity tracking",
        "Use model performance monitoring with A/B testing, analyze inference request batching impacts, and monitor model caching effectiveness",
        "Monitor model loading patterns during traffic spikes, analyze memory allocation impacts on inference quality, and review model versioning consistency",
        "Implement comprehensive recommendation logging, analyze user behavior pattern changes during peak hours, and monitor model drift detection"
    ],
    correct: 0,
    explanation: {
        correct: "Recommendation quality degradation during peak traffic often relates to feature pipeline delays, stale data, or reduced diversity. Feature consistency analysis identifies if real-time features are delayed. Data freshness impacts accuracy. Diversity tracking ensures recommendations don't become repetitive.",
        whyWrong: {
            1: "A/B testing and caching help but feature pipeline consistency is more fundamental to quality during high load",
            2: "Model loading and memory allocation affect performance, not quality; feature pipeline analysis is more relevant",
            3: "Recommendation logging helps but feature engineering consistency and data freshness are more specific to peak traffic quality issues"
        },
        examStrategy: "ML recommendation quality degradation: Feature pipeline consistency + data freshness monitoring + recommendation diversity tracking."
    }
},
{
    id: 'dva_424',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "Step Functions Express workflows experience execution failures that don't appear in CloudWatch Logs, making it difficult to debug the cause of workflow failures in the high-throughput asynchronous execution mode.",
    question: "What Step Functions Express workflow troubleshooting approach will provide visibility into execution failures?",
    options: [
        "Enable CloudWatch Logging for Express workflows, implement custom error tracking within workflow states, and analyze execution history sampling",
        "Use X-Ray integration for workflow tracing, implement comprehensive state input/output logging, and monitor Lambda function errors within workflows",
        "Analyze workflow definition syntax, monitor Step Functions service quotas, and implement retry logic validation for failed states",
        "Review IAM permissions for workflow execution, monitor Express workflow concurrency limits, and analyze state transition timing"
    ],
    correct: 0,
    explanation: {
        correct: "Express workflows have limited logging by default. Enabling CloudWatch Logging provides execution details. Custom error tracking in states captures specific failure information. Execution history sampling gives insights into failure patterns.",
        whyWrong: {
            1: "X-Ray integration helps but CloudWatch Logging specifically for Express workflows is more direct for failure visibility",
            2: "Workflow definition and quotas cause consistent issues, not intermittent failures that need debugging",
            3: "IAM permissions and concurrency limits would cause consistent failures, not debugging challenges with invisible failures"
        },
        examStrategy: "Step Functions Express troubleshooting: Enable CloudWatch Logging + custom error tracking + execution history sampling."
    }
},
{
    id: 'dva_425',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A distributed logging system using CloudWatch Logs experiences log message ordering issues where events from the same source arrive out of sequence, affecting log analysis and troubleshooting accuracy.",
    question: "Which CloudWatch Logs troubleshooting approach will resolve log message ordering issues?",
    options: [
        "Analyze log stream creation patterns, implement timestamp-based log ordering validation, and monitor log ingestion timing variations across multiple log streams",
        "Use CloudWatch Logs Insights for temporal analysis, implement custom log correlation, and analyze network latency impacts on log delivery",
        "Monitor log group retention settings, analyze log message batching patterns, and implement client-side log buffering optimization",
        "Review log agent configuration consistency, analyze clock synchronization across log sources, and implement log sequence validation"
    ],
    correct: 3,
    explanation: {
        correct: "Log ordering issues often stem from clock synchronization problems across sources or log agent configuration inconsistencies. Clock sync analysis identifies timing discrepancies. Agent configuration ensures consistent delivery. Sequence validation confirms ordering integrity.",
        whyWrong: {
            0: "Stream creation and ingestion timing help but clock synchronization is more fundamental to log ordering issues",
            1: "CloudWatch Logs Insights and correlation help but don't solve the root cause of ordering issues",
            2: "Retention settings and batching patterns don't directly affect message ordering within log streams"
        },
        examStrategy: "CloudWatch Logs ordering issues: Log agent configuration review + clock synchronization analysis + log sequence validation."
    }
},
{
    id: 'dva_426',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "API Gateway usage plans show throttling errors for certain API keys during peak periods, despite having sufficient throttling limits configured and normal API performance for other keys.",
    question: "What API Gateway usage plan troubleshooting approach will identify selective throttling issues?",
    options: [
        "Analyze per-API key usage patterns, monitor burst capacity consumption for specific keys, and review usage plan hierarchy and inheritance",
        "Monitor API Gateway overall throttling limits, analyze Lambda backend performance, and review request distribution patterns",
        "Use CloudWatch metrics for detailed API key analysis, implement custom logging for throttled requests, and analyze client request patterns",
        "Review API key quota configurations, monitor usage plan association accuracy, and analyze time-based usage pattern variations"
    ],
    correct: 0,
    explanation: {
        correct: "Selective throttling suggests per-API key issues. Usage pattern analysis identifies if specific keys are hitting individual limits. Burst capacity consumption shows if keys are exceeding temporary allowances. Usage plan hierarchy reveals if inheritance is causing conflicts.",
        whyWrong: {
            1: "Overall throttling limits and Lambda performance don't explain why specific API keys are throttled while others work normally",
            2: "CloudWatch metrics help but per-API key usage patterns and burst capacity are more specific to selective throttling",
            3: "Quota configurations are important but burst capacity analysis is more specific to peak period throttling issues"
        },
        examStrategy: "API Gateway selective throttling: Per-API key usage analysis + burst capacity monitoring + usage plan hierarchy review."
    }
},
{
    id: 'dva_427',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A serverless application with multiple AWS services experiences performance degradation that seems to cascade across services, suggesting a shared bottleneck that's not immediately visible in individual service metrics.",
    question: "Which system-wide troubleshooting approach will identify hidden shared bottlenecks causing cascading performance issues?",
    options: [
        "Implement comprehensive distributed tracing across all services, analyze shared resource utilization patterns, and perform dependency impact analysis",
        "Use CloudWatch cross-service correlation analysis, monitor account-level service quotas, and analyze inter-service communication patterns",
        "Monitor external service dependencies, analyze shared database connection patterns, and implement circuit breaker effectiveness analysis",
        "Analyze service mesh performance metrics, monitor shared infrastructure components, and implement chaos engineering for bottleneck identification"
    ],
    correct: 0,
    explanation: {
        correct: "Cascading performance issues require comprehensive distributed tracing to see actual service interactions and timing. Shared resource analysis identifies bottlenecks (databases, external APIs, cache layers). Dependency impact analysis shows how bottlenecks propagate.",
        whyWrong: {
            1: "Cross-service correlation helps but distributed tracing provides more detailed visibility into shared resource bottlenecks",
            2: "External dependencies and database connections are important but comprehensive tracing reveals all shared bottlenecks",
            3: "Service mesh metrics and chaos engineering help but distributed tracing with resource analysis is more systematic"
        },
        examStrategy: "Cascading performance troubleshooting: Comprehensive distributed tracing + shared resource analysis + dependency impact analysis."
    }
},
{
    id: 'dva_428',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "Lambda functions processing financial transactions experience subtle calculation errors that only affect a small percentage of transactions, making the errors difficult to detect and debug through standard monitoring.",
    question: "What financial transaction troubleshooting approach will identify subtle calculation errors?",
    options: [
        "Implement comprehensive transaction validation with checksum verification, analyze calculation logic with known test cases, and monitor numerical precision patterns",
        "Use detailed logging of all calculation steps, implement duplicate calculation verification, and analyze input data quality impacts",
        "Monitor Lambda execution environment consistency, analyze memory allocation impacts on calculations, and review floating-point precision handling",
        "Implement transaction replay capabilities, analyze calculation algorithm consistency, and monitor external data source accuracy"
    ],
    correct: 0,
    explanation: {
        correct: "Subtle calculation errors require comprehensive validation with checksums to detect discrepancies. Known test cases validate calculation logic. Numerical precision monitoring identifies floating-point or rounding issues that affect financial accuracy.",
        whyWrong: {
            1: "Detailed logging helps but checksum verification provides more systematic error detection for financial calculations",
            2: "Execution environment and memory allocation don't typically cause calculation errors; precision handling is more relevant",
            3: "Transaction replay helps but comprehensive validation with known test cases is more systematic for error identification"
        },
        examStrategy: "Financial calculation error detection: Comprehensive validation + checksum verification + numerical precision monitoring."
    }
},
{
    id: 'dva_429',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A multi-tenant application experiences tenant isolation failures where data from one tenant occasionally appears in another tenant's results, despite proper tenant ID validation and access controls being implemented.",
    question: "Which tenant isolation troubleshooting approach will identify data leakage causes?",
    options: [
        "Implement comprehensive tenant data tracking with unique identifiers, analyze cache isolation effectiveness, and monitor shared resource access patterns",
        "Use detailed audit logging for all data access, analyze database query execution plans, and monitor tenant-specific performance metrics",
        "Analyze application-level tenant validation logic, monitor database transaction isolation, and implement tenant data encryption validation",
        "Review IAM policy effectiveness for tenant separation, analyze Lambda execution context isolation, and monitor tenant-specific error patterns"
    ],
    correct: 0,
    explanation: {
        correct: "Tenant data leakage requires comprehensive tracking to identify where data crosses tenant boundaries. Cache isolation is often overlooked but critical. Shared resource access patterns reveal if resources are properly tenant-isolated.",
        whyWrong: {
            1: "Audit logging and query plans help but cache isolation and shared resource analysis are more specific to data leakage",
            2: "Validation logic and transaction isolation are important but cache isolation is often the overlooked cause of data leakage",
            3: "IAM policies and execution context help but comprehensive data tracking provides more specific leakage identification"
        },
        examStrategy: "Tenant isolation troubleshooting: Comprehensive data tracking + cache isolation analysis + shared resource access monitoring."
    }
},
{
    id: 'dva_430',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "DynamoDB Global Secondary Index queries experience performance degradation where previously fast queries now take 5-10 seconds, despite the GSI having adequate provisioned capacity and no throttling errors.",
    question: "What DynamoDB GSI troubleshooting approach will identify query performance degradation?",
    options: [
        "Analyze GSI key distribution patterns, monitor hot partition development, and review query filter efficiency on projected attributes",
        "Monitor GSI synchronization lag with base table, analyze item size growth impacts, and review GSI capacity allocation",
        "Use DynamoDB performance monitoring for GSI-specific metrics, analyze query execution plans, and monitor concurrent access patterns",
        "Review GSI projection configuration, analyze query pattern changes, and monitor base table write patterns affecting GSI updates"
    ],
    correct: 0,
    explanation: {
        correct: "GSI performance degradation often relates to hot partition development or inefficient filters. Key distribution analysis identifies if data is concentrating on specific partitions. Filter efficiency on projected attributes affects query performance.",
        whyWrong: {
            1: "Synchronization lag and item size help but hot partition development is more likely to cause sudden performance degradation",
            2: "Performance monitoring helps but specific analysis of key distribution and filter efficiency is more targeted",
            3: "Projection configuration and base table patterns are relevant but hot partition analysis is more specific to performance degradation"
        },
        examStrategy: "DynamoDB GSI performance degradation: Key distribution analysis + hot partition monitoring + query filter efficiency review."
    }
},
{
    id: 'dva_431',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A serverless image processing application experiences memory usage spikes that don't correlate with image size or processing complexity, suggesting memory management issues that are difficult to predict or reproduce.",
    question: "Which memory troubleshooting approach will identify unpredictable memory usage patterns in image processing?",
    options: [
        "Implement detailed memory profiling with object lifecycle tracking, analyze garbage collection patterns during image processing, and monitor memory allocation patterns for different image formats",
        "Use Lambda memory allocation optimization, monitor concurrent processing impacts, and analyze image processing library memory usage",
        "Analyze image metadata correlation with memory usage, implement memory usage prediction models, and monitor processing algorithm efficiency",
        "Monitor Lambda execution environment memory behavior, analyze image caching strategies, and implement memory usage alerting"
    ],
    correct: 0,
    explanation: {
        correct: "Unpredictable memory spikes require detailed profiling to track object lifecycles. GC pattern analysis reveals if memory is being properly released. Different image formats can have vastly different memory allocation patterns during processing.",
        whyWrong: {
            1: "Memory allocation optimization helps but doesn't identify why memory usage is unpredictable; profiling is needed",
            2: "Metadata correlation and prediction models help but object lifecycle tracking is more fundamental for memory spike analysis",
            3: "Environment monitoring and caching help but detailed profiling of object allocation patterns is more specific"
        },
        examStrategy: "Image processing memory spikes: Detailed memory profiling + object lifecycle tracking + format-specific allocation analysis."
    }
},
{
    id: 'dva_432',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "CloudWatch alarms experience false positives where alarms trigger despite actual application performance being normal, suggesting metric collection or evaluation issues.",
    question: "What CloudWatch alarm troubleshooting approach will identify false positive triggers?",
    options: [
        "Analyze alarm threshold configuration accuracy, monitor metric collection timing patterns, and review statistical evaluation methods",
        "Use detailed CloudWatch Logs correlation with alarm triggers, implement custom metric validation, and analyze alarm history patterns",
        "Monitor application performance baselines, analyze metric aggregation accuracy, and review alarm notification delivery timing",
        "Review alarm configuration syntax, analyze metric math expressions, and monitor CloudWatch service health during false positives"
    ],
    correct: 0,
    explanation: {
        correct: "False positive alarms often stem from incorrect thresholds, metric collection timing issues, or inappropriate statistical evaluation (e.g., using Average when Maximum is more appropriate). Threshold accuracy analysis identifies configuration issues.",
        whyWrong: {
            1: "CloudWatch Logs correlation helps but threshold configuration and statistical methods are more fundamental to false positives",
            2: "Performance baselines and aggregation help but statistical evaluation method analysis is more specific to false positive issues",
            3: "Configuration syntax and service health are basic issues; threshold and statistical method analysis are more relevant"
        },
        examStrategy: "CloudWatch alarm false positives: Threshold configuration analysis + metric timing patterns + statistical evaluation review."
    }
},
{
    id: 'dva_433',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A distributed microservices application experiences intermittent service mesh communication failures that don't follow predictable patterns, suggesting complex networking or service discovery issues.",
    question: "Which service mesh troubleshooting approach will identify intermittent communication failures?",
    options: [
        "Implement comprehensive network flow analysis, monitor service discovery timing patterns, and analyze load balancing distribution effectiveness",
        "Use distributed tracing for service-to-service communication patterns, monitor DNS resolution consistency, and analyze certificate rotation impacts",
        "Analyze service mesh configuration consistency, monitor proxy sidecar health patterns, and implement connection pooling validation",
        "Monitor service registration patterns, analyze health check timing, and review service mesh policy enforcement effectiveness"
    ],
    correct: 1,
    explanation: {
        correct: "Intermittent service mesh failures often relate to service discovery timing, DNS resolution inconsistencies, or certificate rotation issues. Distributed tracing reveals communication patterns. DNS consistency identifies resolution failures. Certificate rotation analysis identifies TLS issues.",
        whyWrong: {
            0: "Network flow analysis helps but service discovery timing and DNS resolution are more specific to intermittent failures",
            2: "Configuration consistency and proxy health are important but DNS resolution and certificate issues are more common for intermittent failures",
            3: "Registration patterns and health checks help but distributed tracing with DNS analysis provides better intermittent failure identification"
        },
        examStrategy: "Service mesh intermittent failures: Distributed tracing patterns + DNS resolution consistency + certificate rotation analysis."
    }
},
{
    id: 'dva_434',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "Lambda functions experience variable cold start times ranging from 500ms to 5+ seconds for identical function configurations, suggesting execution environment provisioning inconsistencies.",
    question: "What Lambda cold start troubleshooting approach will identify provisioning inconsistencies?",
    options: [
        "Analyze Lambda execution environment allocation patterns, monitor VPC ENI creation timing, and implement cold start timing instrumentation",
        "Use provisioned concurrency optimization, monitor concurrent execution scaling patterns, and analyze function deployment consistency",
        "Monitor Lambda service capacity in different AZs, analyze initialization code efficiency, and implement custom cold start metrics",
        "Review Lambda function configuration consistency, analyze package size impacts, and monitor execution environment reuse patterns"
    ],
    correct: 0,
    explanation: {
        correct: "Variable cold start times often relate to execution environment allocation differences or VPC ENI creation timing. Allocation pattern analysis identifies inconsistencies. ENI timing analysis reveals VPC-related delays. Cold start instrumentation measures specific timing components.",
        whyWrong: {
            1: "Provisioned concurrency bypasses cold starts but doesn't identify why cold start times vary; environment allocation analysis is needed",
            2: "Service capacity and initialization efficiency help but VPC ENI creation timing is more specific to variable cold start times",
            3: "Configuration consistency and package size affect absolute cold start time but don't explain variability in identical configurations"
        },
        examStrategy: "Variable Lambda cold starts: Execution environment allocation analysis + VPC ENI timing + cold start instrumentation."
    }
},
{
    id: 'dva_435',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A real-time analytics dashboard shows data inconsistencies where metrics from the same data source display different values across dashboard widgets, despite using identical queries and data sources.",
    question: "Which dashboard troubleshooting approach will identify data inconsistency causes across widgets?",
    options: [
        "Analyze query execution timing differences, monitor data source caching behavior, and implement data freshness validation across widgets",
        "Use comprehensive logging of dashboard query execution, monitor data source connection pooling, and analyze widget refresh timing patterns",
        "Monitor dashboard performance optimization, analyze query result caching effectiveness, and review data source configuration consistency",
        "Implement data lineage tracking for dashboard queries, analyze concurrent query execution impacts, and monitor data source load balancing"
    ],
    correct: 0,
    explanation: {
        correct: "Data inconsistencies across widgets often stem from timing differences in query execution or caching behavior. Timing analysis identifies if widgets query at different times. Caching behavior analysis reveals if some widgets use cached data while others use fresh data.",
        whyWrong: {
            1: "Logging and connection pooling help but query timing and caching analysis are more specific to cross-widget inconsistencies",
            2: "Performance optimization and caching effectiveness help but data freshness validation is more targeted for consistency issues",
            3: "Data lineage and concurrent execution help but query timing differences and caching behavior are more direct causes"
        },
        examStrategy: "Dashboard data inconsistency: Query execution timing analysis + data source caching behavior + freshness validation."
    }
},
{
    id: 'dva_436',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "API Gateway request/response logging shows successful API calls but client applications report receiving incomplete response data, suggesting response truncation or encoding issues.",
    question: "What API Gateway response troubleshooting approach will identify data truncation or encoding issues?",
    options: [
        "Analyze response payload size patterns, monitor content encoding configuration, and implement response integrity validation",
        "Use detailed API Gateway access logging, monitor Lambda function response formatting, and analyze client-side response parsing",
        "Monitor API Gateway response caching behavior, analyze response compression settings, and review content-type header consistency",
        "Implement custom response validation, analyze network transfer timing, and monitor API Gateway service limits for response size"
    ],
    correct: 0,
    explanation: {
        correct: "Response truncation often relates to payload size limits or encoding issues. Payload size analysis identifies if responses exceed limits. Content encoding configuration reveals compression or encoding problems. Response integrity validation confirms complete data transfer.",
        whyWrong: {
            1: "Access logging and response formatting help but payload size and encoding analysis are more specific to truncation issues",
            2: "Caching and compression settings help but response integrity validation provides more direct truncation detection",
            3: "Custom validation helps but payload size patterns and encoding configuration are more fundamental to truncation issues"
        },
        examStrategy: "API Gateway response truncation: Payload size analysis + content encoding configuration + response integrity validation."
    }
},
{
    id: 'dva_437',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A serverless batch processing system experiences job completion inconsistencies where some jobs report completion but their output files are missing or incomplete in S3, despite successful Lambda execution logs.",
    question: "Which batch processing troubleshooting approach will identify job completion inconsistencies?",
    options: [
        "Implement end-to-end job tracking with output validation, analyze S3 upload completion verification, and monitor Lambda execution context cleanup patterns",
        "Use comprehensive job state management, monitor batch processing queue integrity, and analyze concurrent job execution impacts",
        "Analyze job scheduling accuracy, monitor resource allocation consistency, and implement job retry logic validation",
        "Monitor job dependency resolution, analyze output file consistency, and review batch processing error handling effectiveness"
    ],
    correct: 0,
    explanation: {
        correct: "Job completion inconsistencies require end-to-end tracking to verify actual output creation. S3 upload completion verification ensures files are fully written. Lambda context cleanup analysis identifies if cleanup is interfering with output writing.",
        whyWrong: {
            1: "Job state management and queue integrity help but output validation is more specific to completion inconsistency issues",
            2: "Scheduling accuracy and resource allocation don't address output file creation verification",
            3: "Dependency resolution and error handling help but end-to-end tracking with output validation is more comprehensive"
        },
        examStrategy: "Batch job completion inconsistency: End-to-end tracking + S3 upload verification + Lambda context cleanup analysis."
    }
},
{
    id: 'dva_438',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "EventBridge rules experience selective matching failures where events that should match rule patterns occasionally don't trigger targets, despite the events appearing correctly formatted in logs.",
    question: "What EventBridge rule troubleshooting approach will identify selective matching failures?",
    options: [
        "Analyze event pattern complexity and matching logic, implement event format validation, and monitor rule evaluation timing patterns",
        "Use comprehensive EventBridge logging, monitor target service availability, and analyze event routing effectiveness",
        "Monitor EventBridge service capacity, analyze event size impacts on matching, and review rule priority configurations",
        "Implement custom event tracking, analyze rule syntax accuracy, and monitor cross-region event replication timing"
    ],
    correct: 0,
    explanation: {
        correct: "Selective matching failures often relate to complex event patterns or subtle format issues. Pattern complexity analysis identifies inefficient matching logic. Event format validation ensures events exactly match expected patterns. Timing patterns reveal if evaluation delays affect matching.",
        whyWrong: {
            1: "EventBridge logging and target availability help but pattern complexity analysis is more specific to matching failures",
            2: "Service capacity and event size help but event pattern analysis is more fundamental to matching issues",
            3: "Custom tracking and syntax accuracy help but pattern complexity and format validation are more targeted"
        },
        examStrategy: "EventBridge selective matching: Event pattern complexity analysis + format validation + rule evaluation timing."
    }
},
{
    id: 'dva_439',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A financial application using Step Functions for transaction processing experiences state machine execution drift where identical transactions occasionally follow different execution paths, affecting transaction integrity.",
    question: "Which Step Functions troubleshooting approach will identify execution path drift in transaction processing?",
    options: [
        "Implement comprehensive state transition logging, analyze choice state evaluation consistency, and monitor external service response impacts on execution flow",
        "Use detailed execution history analysis, monitor concurrent execution patterns, and analyze state machine definition consistency",
        "Analyze transaction input validation accuracy, monitor state machine version consistency, and implement execution path verification",
        "Monitor execution timing patterns, analyze resource allocation impacts, and review state machine error handling effectiveness"
    ],
    correct: 0,
    explanation: {
        correct: "Execution path drift requires analysis of state transitions and choice evaluations. Choice state consistency analysis identifies if external service responses are affecting decisions. External service impact monitoring reveals if API responses are causing different execution paths.",
        whyWrong: {
            1: "Execution history and definition consistency help but choice state evaluation analysis is more specific to path drift",
            2: "Input validation and version consistency are important but external service impact on choices is more likely to cause drift",
            3: "Timing patterns and resource allocation don't typically cause execution path changes; choice evaluation analysis is more relevant"
        },
        examStrategy: "Step Functions execution drift: State transition logging + choice evaluation consistency + external service impact analysis."
    }
},
{
    id: 'dva_440',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 140,
    scenario: "Lambda functions using custom runtime experience sporadic initialization failures where functions sometimes fail to start with unclear error messages, making debugging difficult.",
    question: "What custom runtime troubleshooting approach will identify sporadic initialization failures?",
    options: [
        "Implement detailed runtime bootstrap logging, analyze runtime API interaction patterns, and monitor execution environment initialization timing",
        "Use comprehensive error handling in runtime code, monitor Lambda service capacity, and analyze custom runtime dependency availability",
        "Analyze runtime implementation consistency, monitor memory allocation during initialization, and review custom runtime configuration",
        "Monitor runtime layer compatibility, analyze initialization script effectiveness, and implement runtime health validation"
    ],
    correct: 0,
    explanation: {
        correct: "Custom runtime initialization failures require detailed bootstrap logging to understand startup sequence. Runtime API interaction analysis identifies communication issues with Lambda service. Environment initialization timing reveals if timeouts are causing failures.",
        whyWrong: {
            1: "Error handling and service capacity help but bootstrap logging and API interaction analysis are more specific to custom runtime issues",
            2: "Implementation consistency and memory allocation help but runtime API interaction patterns are more fundamental",
            3: "Layer compatibility and health validation help but detailed bootstrap logging provides more specific initialization failure analysis"
        },
        examStrategy: "Custom runtime initialization failures: Bootstrap logging + runtime API interaction analysis + environment timing monitoring."
    }
},
{
    id: 'dva_441',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 170,
    scenario: "A distributed transaction system using multiple AWS services experiences transaction rollback failures where compensating actions don't execute properly, leaving the system in an inconsistent state.",
    question: "Which distributed transaction troubleshooting approach will identify rollback failure causes?",
    options: [
        "Implement comprehensive transaction state tracking, analyze compensating action execution patterns, and monitor service communication during rollback scenarios",
        "Use distributed tracing for transaction flow analysis, monitor individual service rollback capabilities, and analyze transaction timeout configurations",
        "Analyze transaction coordinator effectiveness, monitor resource locking patterns, and implement transaction integrity validation",
        "Monitor transaction isolation levels, analyze concurrent transaction impacts, and review distributed transaction protocol implementation"
    ],
    correct: 0,
    explanation: {
        correct: "Rollback failures require comprehensive state tracking to see where compensating actions fail. Execution pattern analysis identifies which compensating actions aren't running. Service communication monitoring during rollbacks reveals if services aren't receiving rollback requests.",
        whyWrong: {
            1: "Distributed tracing helps but comprehensive state tracking is more specific to rollback failure analysis",
            2: "Transaction coordinator and locking patterns help but compensating action execution analysis is more targeted",
            3: "Isolation levels and protocol implementation are important but state tracking during rollbacks is more specific"
        },
        examStrategy: "Distributed transaction rollback failures: Transaction state tracking + compensating action analysis + service communication monitoring."
    }
},
{
    id: 'dva_442',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 130,
    scenario: "CloudWatch custom metrics show data gaps during specific time periods, where metric data points are missing despite application logs confirming metric publication attempts during those periods.",
    question: "What CloudWatch metric troubleshooting approach will identify data gaps during specific periods?",
    options: [
        "Analyze CloudWatch API throttling during gap periods, monitor metric namespace capacity, and implement metric publication retry logic validation",
        "Use detailed timestamp analysis for metric publication, monitor CloudWatch service health, and analyze metric aggregation timing",
        "Monitor application scaling patterns during gap periods, analyze metric publication batching, and review CloudWatch quota limits",
        "Implement custom metric validation, analyze network connectivity during gaps, and monitor CloudWatch endpoint availability"
    ],
    correct: 0,
    explanation: {
        correct: "Metric data gaps during specific periods often indicate CloudWatch API throttling when multiple sources publish simultaneously. Namespace capacity analysis identifies if too many metrics are being published. Retry logic validation ensures failed publications are retried.",
        whyWrong: {
            1: "Timestamp analysis and service health help but API throttling during specific periods is more likely to cause data gaps",
            2: "Application scaling and batching help but CloudWatch API throttling is more specific to time-period gaps",
            3: "Custom validation and network connectivity help but API throttling analysis is more targeted for periodic data gaps"
        },
        examStrategy: "CloudWatch metric gaps: API throttling analysis + namespace capacity monitoring + retry logic validation."
    }
},
{
    id: 'dva_443',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 160,
    scenario: "A machine learning pipeline experiences model drift detection failures where the monitoring system doesn't identify obvious changes in model performance, suggesting issues with the drift detection mechanism itself.",
    question: "Which ML drift detection troubleshooting approach will identify monitoring system failures?",
    options: [
        "Analyze drift detection algorithm sensitivity, monitor baseline data quality, and implement manual validation of detected vs actual drift patterns",
        "Use comprehensive model performance logging, monitor feature distribution analysis, and analyze drift threshold configuration accuracy",
        "Monitor model inference consistency, analyze training data representation, and implement drift detection validation with known scenarios",
        "Analyze model version management, monitor data pipeline consistency, and review drift detection timing configuration"
    ],
    correct: 0,
    explanation: {
        correct: "Drift detection failures require algorithm sensitivity analysis to ensure detection thresholds are appropriate. Baseline data quality affects detection accuracy. Manual validation against actual drift patterns identifies if the detection system is working correctly.",
        whyWrong: {
            1: "Performance logging and feature distribution help but algorithm sensitivity analysis is more fundamental to detection failures",
            2: "Inference consistency and training data help but manual validation of detection accuracy is more specific",
            3: "Version management and timing configuration help but drift detection algorithm analysis is more targeted"
        },
        examStrategy: "ML drift detection failures: Algorithm sensitivity analysis + baseline data quality + manual validation against actual drift."
    }
},
{
    id: 'dva_444',
    domain: "Domain 4: Troubleshooting",
    difficulty: "medium",
    timeRecommendation: 120,
    scenario: "S3 bucket notifications experience delivery delays where events reach SQS queues 2-5 minutes after object operations, despite S3 event notification configuration appearing correct and no obvious bottlenecks.",
    question: "What S3 notification troubleshooting approach will identify delivery delay causes?",
    options: [
        "Analyze S3 event notification queue depth, monitor notification delivery timing patterns, and review S3 bucket region configuration consistency",
        "Use S3 CloudTrail correlation with notification delivery, monitor SQS queue configuration, and analyze S3 request rate impacts",
        "Monitor S3 service health during delay periods, analyze notification filtering accuracy, and review S3 bucket policy configurations",
        "Implement custom S3 event tracking, analyze cross-region replication impacts, and monitor S3 storage class transition timing"
    ],
    correct: 0,
    explanation: {
        correct: "S3 notification delays often relate to queue depth in the notification service or region configuration mismatches. Queue depth analysis identifies backlog issues. Delivery timing patterns reveal systematic delays. Region consistency ensures optimal routing.",
        whyWrong: {
            1: "CloudTrail correlation and SQS configuration help but S3 notification queue depth is more specific to delivery delays",
            2: "Service health and filtering accuracy help but queue depth analysis is more targeted for delay identification",
            3: "Custom tracking and replication impacts help but notification queue depth monitoring is more specific to delivery timing"
        },
        examStrategy: "S3 notification delays: Event notification queue depth + delivery timing patterns + region configuration consistency."
    }
},
        // BATCH 8: INTEGRATION & EDGE CASES (Questions 445-500)
// Domain 1: Development with AWS Services (445-462) - 18 questions - Complex Integrations

{
    id: 'dva_445',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A multi-tenant SaaS platform experiences Lambda cold starts causing 5-second delays for first requests after idle periods. The application uses VPC-connected Lambdas accessing RDS and ElastiCache. Each tenant has isolated data requiring separate database connections. Traffic is unpredictable with sudden spikes from any tenant.",
    question: "What architectural changes eliminate cold start impacts while maintaining tenant isolation?",
    options: [
        "Implement Lambda SnapStart with priming, RDS Proxy for connection pooling, provisioned concurrency with Application Load Balancer request routing based on tenant headers",
        "Remove VPC configuration, use DynamoDB instead of RDS, implement tenant isolation via partition keys, use API Gateway caching",
        "Convert to ECS Fargate with service auto-scaling, maintain persistent database connections, use Application Load Balancer with path-based routing",
        "Use Lambda reserved concurrency per tenant, implement custom connection pooling, add API Gateway with usage plans per tenant"
    ],
    correct: 0,
    explanation: {
        correct: "Lambda SnapStart eliminates cold starts for Java functions. RDS Proxy manages connection pooling efficiently. Provisioned concurrency keeps functions warm. ALB routing by headers maintains tenant isolation while optimizing resource usage.",
        whyWrong: {
            1: "Removing VPC and changing to DynamoDB requires complete application rewrite and may not suit relational data needs",
            2: "ECS Fargate increases operational overhead and cost compared to serverless; doesn't leverage Lambda's automatic scaling",
            3: "Reserved concurrency per tenant is inefficient and expensive; custom connection pooling adds complexity"
        },
        examStrategy: "Multi-tenant Lambda optimization: SnapStart + RDS Proxy + provisioned concurrency + intelligent request routing."
    }
},

{
    id: 'dva_446',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "An event-driven microservices architecture processes 1 million events/hour across 50 different event types. Services require exactly-once processing, event replay capability, and the ability to process events from specific time windows. Dead letter handling must capture context for debugging.",
    question: "Which event backbone architecture ensures reliability and debugging capabilities?",
    options: [
        "EventBridge with archive and replay, event buses per domain, Lambda destinations for DLQ with error context, DynamoDB for idempotency tracking",
        "Kinesis Data Streams with extended retention, Lambda parallelization factor, CloudWatch Logs Insights for debugging, S3 for replay storage",
        "SNS FIFO topics with content-based deduplication, SQS FIFO queues, Lambda with bisect batch on error, X-Ray for debugging",
        "MSK with log compaction, Kafka Connect for integrations, Lambda event source mapping, OpenSearch for event analysis"
    ],
    correct: 0,
    explanation: {
        correct: "EventBridge provides native archive/replay capabilities and time-window processing. Event buses enable domain separation. Lambda destinations capture full error context. DynamoDB conditional writes ensure idempotency.",
        whyWrong: {
            1: "Kinesis requires custom replay implementation; CloudWatch Logs Insights isn't designed for event context debugging",
            2: "SNS FIFO has throughput limitations for 1M events/hour; bisect batch doesn't provide time-window processing",
            3: "MSK adds operational overhead; log compaction doesn't provide time-based replay like EventBridge archives"
        },
        examStrategy: "Enterprise event backbone: EventBridge for replay + domain separation + Lambda destinations + idempotency tracking."
    }
},

{
    id: 'dva_447',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A real-time bidding platform must process bid requests in under 100ms, handle 500,000 requests/second during peak, implement complex pricing algorithms, and maintain bid history for 24 hours. The system must gracefully degrade under extreme load.",
    question: "What architecture meets these extreme performance requirements?",
    options: [
        "API Gateway HTTP APIs with Lambda@Edge for routing, DynamoDB with DAX for sub-millisecond reads, Kinesis Data Streams for bid history, auto-scaling policies with circuit breakers",
        "Application Load Balancer with gRPC support, ECS Fargate Spot for compute, ElastiCache Redis with Lua scripting, Time-series database for history",
        "CloudFront with Lambda@Edge for bid logic, DynamoDB Global Tables for multi-region, S3 for bid history with lifecycle policies, Route 53 weighted routing for load distribution",
        "API Gateway REST with caching, Lambda with provisioned concurrency, MemoryDB for state, Timestream for bid history, throttling policies for degradation"
    ],
    correct: 2,
    explanation: {
        correct: "CloudFront with Lambda@Edge executes logic at edge locations minimizing latency. DynamoDB Global Tables provide regional performance. S3 handles history cost-effectively. Route 53 weighted routing enables graceful degradation.",
        whyWrong: {
            0: "API Gateway adds latency; Kinesis isn't optimized for 24-hour history retrieval at this scale",
            1: "ECS Fargate Spot isn't suitable for consistent sub-100ms responses; might face interruptions",
            3: "API Gateway REST has higher latency than CloudFront; MemoryDB is overkill for this caching need"
        },
        examStrategy: "Ultra-low latency at scale: CloudFront + Lambda@Edge + DynamoDB Global Tables + intelligent routing."
    }
},

{
    id: 'dva_448',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A healthcare platform processes patient data requiring HIPAA compliance, end-to-end encryption, audit logging of all access, and real-time sync between mobile apps and web dashboard. Data must remain encrypted even during processing.",
    question: "Which architecture ensures compliance while maintaining real-time synchronization?",
    options: [
        "AppSync with Cognito authentication, DynamoDB encryption at rest, AWS KMS with CloudTrail logging, Lambda resolvers using AWS Encryption SDK for processing",
        "API Gateway with WAF, RDS encrypted instances, Secrets Manager for keys, CloudWatch Logs with encryption, WebSocket for real-time updates",
        "Application Load Balancer with ACM, ECS tasks with encrypted EBS, Parameter Store for secrets, X-Ray for tracing, Server-Sent Events for updates",
        "API Gateway with custom authorizers, S3 with SSE-C, Lambda with environment variable encryption, Kinesis for real-time streaming"
    ],
    correct: 0,
    explanation: {
        correct: "AppSync provides managed GraphQL with real-time subscriptions perfect for sync. AWS Encryption SDK enables processing encrypted data. KMS with CloudTrail provides complete audit trail. DynamoDB encryption ensures data security at rest.",
        whyWrong: {
            1: "WebSocket management adds complexity; RDS encryption doesn't support processing encrypted data like Encryption SDK",
            2: "Server-Sent Events are one-way; EBS encryption doesn't help with processing encrypted data",
            3: "SSE-C requires client-managed keys adding complexity; environment variables aren't suitable for sensitive data"
        },
        examStrategy: "HIPAA-compliant real-time sync: AppSync + AWS Encryption SDK + KMS auditing + end-to-end encryption."
    }
},

{
    id: 'dva_449',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A gaming platform needs to migrate 50TB of player data from on-premises MongoDB to DynamoDB while maintaining read/write operations. The migration must track progress, handle failures, and validate data integrity. Downtime must be under 5 minutes.",
    question: "What migration strategy minimizes downtime while ensuring data integrity?",
    options: [
        "AWS Database Migration Service with ongoing replication, custom transformation rules, validation tasks, and coordinated cutover with Lambda-based reconciliation",
        "AWS DataSync for initial bulk transfer, Lambda functions for CDC from MongoDB oplog, DynamoDB Streams for validation, blue-green deployment for cutover",
        "S3 Transfer Acceleration for data upload, AWS Glue for transformation, Step Functions for orchestration, dual-write pattern during migration",
        "Direct Connect for transfer, EC2-based migration tools, SQS for change tracking, canary deployment with percentage-based traffic shifting"
    ],
    correct: 0,
    explanation: {
        correct: "DMS supports MongoDB to DynamoDB migration with continuous replication minimizing cutover time. Transformation rules handle schema differences. Validation tasks ensure integrity. Lambda reconciliation handles any discrepancies.",
        whyWrong: {
            1: "DataSync doesn't support MongoDB; managing CDC from oplog is complex and error-prone",
            2: "S3 Transfer Acceleration doesn't help with database migration; dual-write pattern risks data inconsistency",
            3: "EC2-based tools require custom development; percentage-based traffic shifting doesn't work for database cutover"
        },
        examStrategy: "Large-scale database migration: DMS with ongoing replication + validation + coordinated cutover + reconciliation."
    }
},

{
    id: 'dva_450',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "An ML inference API must serve predictions from multiple models (TensorFlow, PyTorch, XGBoost) with sub-200ms latency. Models are updated daily and A/B tested. The system handles 100,000 requests/minute with cost optimization priority.",
    question: "Which architecture optimizes cost while meeting latency requirements?",
    options: [
        "SageMaker Multi-Model Endpoints with automatic scaling, S3 model registry, API Gateway caching for repeated predictions, CloudWatch Events for model updates",
        "Lambda with container images for each framework, EFS for model storage, API Gateway with usage plans, Step Functions for A/B testing orchestration",
        "ECS Fargate with GPU support, ECR for model versions, Application Load Balancer with weighted target groups, ElastiCache for prediction caching",
        "SageMaker Serverless Inference, Model Registry for versioning, Lambda for routing logic, DynamoDB for caching frequent predictions"
    ],
    correct: 0,
    explanation: {
        correct: "Multi-Model Endpoints serve multiple models from single endpoint optimizing resource usage. Automatic scaling handles load. API Gateway caching reduces inference calls. S3 registry enables easy model updates.",
        whyWrong: {
            1: "Lambda container images have cold start issues; EFS adds latency for model loading",
            2: "Fargate GPU is expensive for this scale; managing multiple container versions adds complexity",
            3: "Serverless Inference has cold starts affecting sub-200ms requirement; Lambda routing adds latency"
        },
        examStrategy: "ML inference optimization: SageMaker Multi-Model Endpoints + API caching + automatic scaling."
    }
},

{
    id: 'dva_451',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A document processing pipeline handles PDFs up to 500MB containing mixed content (text, images, tables). It must extract text with OCR, detect PII, redact sensitive data, and convert to searchable format. Processing must complete within 10 minutes.",
    question: "What service combination handles large document processing most efficiently?",
    options: [
        "S3 Batch Operations triggering Step Functions, Textract for OCR, Comprehend for PII detection, Lambda for redaction, OpenSearch for indexing",
        "Lambda with increased timeout, Rekognition for text extraction, Macie for PII scanning, S3 for storage, CloudSearch for search",
        "ECS Fargate tasks, open-source OCR libraries, Comprehend Medical for PII, DocumentDB for storage, ElasticSearch for search",
        "SageMaker Processing jobs, Textract asynchronous operations, custom PII models, DynamoDB for metadata, Kendra for intelligent search"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions orchestrate long-running workflows beyond Lambda limits. Textract handles OCR for large documents. Comprehend accurately detects PII. OpenSearch provides powerful search capabilities. Batch Operations efficiently trigger processing.",
        whyWrong: {
            1: "Lambda 15-minute timeout insufficient for 500MB PDFs; Rekognition isn't optimized for document text extraction",
            2: "Open-source OCR requires maintenance; Comprehend Medical is for healthcare data not general PII",
            3: "SageMaker Processing is overkill; Kendra is expensive for this use case compared to OpenSearch"
        },
        examStrategy: "Large document processing: Step Functions orchestration + Textract + Comprehend + OpenSearch."
    }
},

{
    id: 'dva_452',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A social media analytics platform ingests 10 million posts daily from various APIs, requiring sentiment analysis, trend detection, influence scoring, and real-time dashboards. Historical data must be queryable for 2 years with sub-second response times.",
    question: "Which architecture provides real-time analytics with historical query performance?",
    options: [
        "Kinesis Data Firehose for ingestion, Kinesis Analytics for real-time processing, Comprehend for sentiment, Redshift with materialized views for historical queries, QuickSight for dashboards",
        "EventBridge for API polling, Lambda for processing, Comprehend for analysis, DynamoDB for hot data, S3 with Athena for historical queries, AppSync for real-time updates",
        "Managed Kafka for streaming, Spark on EMR for processing, SageMaker for custom models, ElasticSearch for search, Grafana for visualization",
        "API Gateway for ingestion, Step Functions for orchestration, Textract for text extraction, RDS for storage, CloudWatch dashboards for visualization"
    ],
    correct: 0,
    explanation: {
        correct: "Kinesis Firehose handles high-volume ingestion reliably. Kinesis Analytics provides real-time stream processing. Redshift materialized views enable sub-second historical queries. QuickSight integrates natively for dashboards.",
        whyWrong: {
            1: "EventBridge polling isn't efficient for high-volume ingestion; Athena doesn't guarantee sub-second response times",
            2: "EMR requires cluster management; ElasticSearch isn't optimized for 2-year historical analytics at this scale",
            3: "RDS isn't suitable for 10M posts daily analytics; CloudWatch dashboards lack advanced analytics features"
        },
        examStrategy: "Real-time analytics platform: Kinesis suite for streaming + Redshift for historical + QuickSight for visualization."
    }
},

{
    id: 'dva_453',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A fintech application requires distributed transaction processing across multiple microservices with ACID guarantees. Each transaction involves updating user balance, transaction history, audit logs, and external payment gateway. Rollback must be supported if any step fails.",
    question: "How should distributed transactions be implemented with proper rollback capabilities?",
    options: [
        "Step Functions with Saga pattern, DynamoDB transactions for local consistency, SQS for compensation actions, Lambda for service coordination with idempotency",
        "API Gateway with X-Ray tracing, RDS with XA transactions, Lambda for orchestration, SNS for notifications",
        "EventBridge with event sourcing, DynamoDB Streams for event store, Lambda for projections, S3 for event backup",
        "AppSync with pipeline resolvers, Aurora Global Database for consistency, Lambda for business logic, CloudWatch Events for monitoring"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions excellently implements Saga pattern for distributed transactions. DynamoDB transactions ensure local ACID. SQS reliably handles compensation for rollbacks. Lambda with idempotency prevents duplicate processing.",
        whyWrong: {
            1: "XA transactions don't work well across different service types and add complexity",
            2: "Event sourcing doesn't directly provide ACID guarantees; requires complex implementation for rollbacks",
            3: "AppSync pipeline resolvers aren't designed for distributed transaction coordination"
        },
        examStrategy: "Distributed transactions: Step Functions Saga pattern + local transactions + compensation actions + idempotency."
    }
},

{
    id: 'dva_454',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A ride-sharing platform needs real-time driver location tracking for 100,000 active drivers, efficient passenger-driver matching within 2km radius, and surge pricing calculations based on supply-demand. Location updates occur every 5 seconds.",
    question: "What architecture handles real-time geospatial processing at scale?",
    options: [
        "IoT Core for location ingestion, DynamoDB with geospatial indexes using Z-order curves, Lambda for matching algorithm, ElastiCache for hot location data, Kinesis Analytics for surge pricing",
        "API Gateway WebSocket for updates, Aurora with PostGIS extension, Lambda for calculations, Redis Geospatial commands for proximity, EventBridge for pricing events",
        "Kinesis Data Streams for ingestion, OpenSearch with geo queries, Step Functions for matching, DynamoDB for driver state, SageMaker for pricing model",
        "AppSync subscriptions for real-time updates, Neptune for graph-based matching, Lambda for routing, MemoryDB for caching, Forecast for demand prediction"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Core efficiently handles massive device connections. DynamoDB with Z-order curves enables efficient geospatial queries. ElastiCache provides sub-millisecond location lookups. Kinesis Analytics processes streaming data for real-time surge calculations.",
        whyWrong: {
            1: "WebSocket management at 100k connections is complex; PostGIS queries slower than DynamoDB geospatial indexes",
            2: "OpenSearch geo queries have higher latency than DynamoDB with caching; Step Functions adds orchestration overhead",
            3: "Neptune graph database is overkill for proximity matching; Forecast is for time-series not real-time pricing"
        },
        examStrategy: "Real-time geospatial: IoT Core + DynamoDB geospatial + ElastiCache + streaming analytics."
    }
},

{
    id: 'dva_455',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "An e-commerce platform experiences 10x traffic spikes during flash sales lasting 1 hour. The checkout process involves inventory validation, payment processing, order creation, and email confirmation. System must prevent overselling while maintaining sub-second response times.",
    question: "Which architecture handles extreme traffic spikes while preventing overselling?",
    options: [
        "API Gateway with throttling and caching, Lambda with provisioned concurrency, DynamoDB with optimistic locking for inventory, SQS for order processing, Step Functions for checkout workflow",
        "CloudFront with custom origins, ALB with auto-scaling, ECS services, RDS with read replicas, Redis for session management, SNS for notifications",
        "API Gateway with usage plans, Lambda at scale, Aurora Serverless v2, ElastiCache for inventory cache, EventBridge for order events",
        "Direct Connect for dedicated bandwidth, EC2 auto-scaling groups, RDS Multi-AZ, SQS FIFO for order queue, Simple Email Service for confirmations"
    ],
    correct: 0,
    explanation: {
        correct: "API Gateway throttling prevents system overload. Provisioned concurrency eliminates cold starts. DynamoDB optimistic locking prevents overselling through conditional updates. SQS decouples order processing. Step Functions orchestrates complex checkout flow.",
        whyWrong: {
            1: "RDS even with read replicas can bottleneck during extreme spikes; managing ECS scaling is complex",
            2: "Aurora Serverless v2 has scaling limits; caching inventory risks overselling without proper locking",
            3: "Direct Connect doesn't help with application scaling; RDS Multi-AZ doesn't improve read performance"
        },
        examStrategy: "Flash sale architecture: API throttling + DynamoDB optimistic locking + queue decoupling + workflow orchestration."
    }
},

{
    id: 'dva_456',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A video conferencing platform needs to support 10,000 concurrent meetings with screen sharing, recording to S3, real-time transcription, and automatic meeting summaries. Latency must be under 150ms globally.",
    question: "What architecture supports large-scale video conferencing with AI features?",
    options: [
        "Amazon Chime SDK for WebRTC, Kinesis Video Streams for recording, Transcribe Streaming for real-time transcription, Bedrock for summarization, CloudFront for global distribution",
        "API Gateway WebSocket for signaling, EC2 GPU instances for video processing, S3 for recording, Comprehend for transcription, Lambda for summaries",
        "AppSync for real-time updates, MediaLive for streaming, MediaPackage for distribution, Transcribe for offline transcription, SageMaker for NLP",
        "Kinesis Data Streams for video, Elemental MediaConnect for transport, Lambda for processing, Textract for transcription, QuickSight for analytics"
    ],
    correct: 0,
    explanation: {
        correct: "Chime SDK provides managed WebRTC infrastructure for video conferencing. Kinesis Video Streams handles recording efficiently. Transcribe Streaming enables real-time transcription. Bedrock provides AI summarization. CloudFront ensures global low latency.",
        whyWrong: {
            1: "Managing WebSocket signaling and GPU instances for 10k meetings is extremely complex",
            2: "MediaLive is for broadcast not interactive conferencing; offline transcription doesn't meet real-time requirement",
            3: "Kinesis Data Streams isn't optimized for video; Textract is for document not audio transcription"
        },
        examStrategy: "Video conferencing scale: Chime SDK + Kinesis Video + Transcribe Streaming + Bedrock AI + CloudFront."
    }
},

{
    id: 'dva_457',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A blockchain analytics platform processes 1TB of daily transaction data requiring pattern detection, wallet clustering, risk scoring, and compliance reporting. Queries must return results within 5 seconds across 5 years of historical data.",
    question: "Which architecture provides performant blockchain analytics?",
    options: [
        "Managed Blockchain for data access, Glue for ETL, Redshift with clustering keys, SageMaker for pattern detection, QuickSight for reporting with SPICE",
        "Lambda for blockchain API calls, Kinesis for streaming, Neptune for graph analysis, Timestream for time-series data, Athena for ad-hoc queries",
        "Fargate for data collection, S3 data lake, EMR with Spark for processing, OpenSearch for full-text search, Tableau for visualization",
        "EC2 for blockchain nodes, DynamoDB for transaction storage, Kinesis Analytics for pattern detection, RDS for reporting data, CloudWatch dashboards"
    ],
    correct: 1,
    explanation: {
        correct: "Neptune excels at graph analysis essential for wallet clustering and transaction patterns. Timestream optimizes time-series queries across years of data. Kinesis handles high-volume streaming. Athena enables fast ad-hoc analysis.",
        whyWrong: {
            0: "Managed Blockchain is for building blockchain not analytics; Redshift less optimal for graph relationships",
            2: "EMR requires cluster management; OpenSearch not optimized for blockchain transaction analysis",
            3: "DynamoDB not suitable for complex analytical queries; CloudWatch dashboards lack advanced analytics"
        },
        examStrategy: "Blockchain analytics: Neptune for graph analysis + Timestream for time-series + streaming ingestion."
    }
},

{
    id: 'dva_458',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A autonomous vehicle platform processes 50GB of sensor data per vehicle per hour from 1,000 vehicles. Data requires real-time anomaly detection, predictive maintenance ML models, and 30-day hot storage with 5-year cold storage compliance.",
    question: "What architecture handles massive IoT sensor data with ML processing?",
    options: [
        "IoT FleetWise for vehicle data, Kinesis Data Firehose with transformation, S3 Intelligent-Tiering, SageMaker for ML pipelines, Lookout for Equipment for anomaly detection",
        "IoT Core with bulk registration, Kinesis Data Streams, Lambda for processing, DynamoDB for hot data, Glacier for archives, custom ML models",
        "IoT Greengrass for edge processing, IoT Analytics for cloud processing, S3 for data lake, EMR for ML training, CloudWatch for monitoring",
        "Direct API ingestion, Timestream for recent data, S3 for long-term, Spark on ECS for ML, ElasticSearch for search"
    ],
    correct: 0,
    explanation: {
        correct: "IoT FleetWise is purpose-built for vehicle data collection. Kinesis Firehose handles high-volume ingestion with transformations. S3 Intelligent-Tiering automatically optimizes storage costs. Lookout for Equipment specializes in sensor anomaly detection.",
        whyWrong: {
            1: "IoT Core not optimized for vehicle-specific protocols; managing hot/cold storage manually is complex",
            2: "Greengrass edge processing doesn't solve cloud-scale challenges; EMR requires cluster management",
            3: "Direct API ingestion won't scale to 50GB/hour/vehicle; Spark on ECS is complex to manage"
        },
        examStrategy: "Vehicle IoT platform: IoT FleetWise + Kinesis Firehose + S3 Intelligent-Tiering + purpose-built ML services."
    }
},

{
    id: 'dva_459',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A content moderation system processes user-generated images, videos, and text across 50 languages. It must detect inappropriate content, copyright violations, and deepfakes within 30 seconds of upload while maintaining 99.9% accuracy.",
    question: "Which service combination provides comprehensive content moderation?",
    options: [
        "Rekognition for image/video moderation, Comprehend for text analysis with custom classifiers, Textract for text in images, Step Functions for orchestration, human review with SageMaker Ground Truth",
        "Lambda with AI libraries, S3 for storage, DynamoDB for results, SNS for notifications, Mechanical Turk for human review",
        "API Gateway with Lambda, Transcribe for audio, Translate for languages, Macie for content classification, CloudWatch for monitoring",
        "Fargate with ML containers, ElastiCache for caching, RDS for metadata, SQS for queue management, custom review portal"
    ],
    correct: 0,
    explanation: {
        correct: "Rekognition provides pre-trained content moderation for images/videos. Comprehend with custom classifiers handles multi-language text. Textract extracts text from images. Step Functions orchestrates the workflow. Ground Truth enables human review for edge cases.",
        whyWrong: {
            1: "Lambda with libraries requires maintaining complex ML models; Mechanical Turk less integrated than Ground Truth",
            2: "Macie is for data discovery not content moderation; missing video processing capabilities",
            3: "Managing ML containers adds complexity; lacks purpose-built moderation features"
        },
        examStrategy: "Content moderation: Rekognition + Comprehend + orchestration + human-in-the-loop review."
    }
},

{
    id: 'dva_460',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A scientific computing platform runs complex simulations requiring 1000 vCPUs for 2-hour bursts, processes results with custom algorithms, stores 100TB of results, and provides Jupyter notebook access for researchers. Cost optimization is critical.",
    question: "What architecture optimizes cost for burst scientific computing?",
    options: [
        "AWS Batch with Spot instances, FSx for Lustre for high-performance storage, S3 for long-term storage, SageMaker Studio for notebook access, Step Functions for workflow orchestration",
        "EC2 Spot Fleet with auto-scaling, EBS volumes for storage, Lambda for result processing, Cloud9 for development, CloudFormation for infrastructure",
        "ECS with Fargate Spot, EFS for shared storage, Glue for processing, EMR Notebooks for access, CodePipeline for deployment",
        "ParallelCluster for HPC, instance store for temporary data, Glacier for archives, EC2 for notebooks, Simple Workflow Service for orchestration"
    ],
    correct: 0,
    explanation: {
        correct: "AWS Batch optimally manages Spot instances for burst computing. FSx for Lustre provides high-performance computing storage. S3 cost-effectively stores results. SageMaker Studio offers managed Jupyter notebooks. Step Functions orchestrates complex workflows.",
        whyWrong: {
            1: "Managing Spot Fleet directly is complex; EBS less performant than FSx for HPC; Cloud9 not optimized for scientific notebooks",
            2: "Fargate Spot has resource limits for HPC; EFS slower than FSx for Lustre for compute-intensive I/O",
            3: "ParallelCluster requires more management; instance store data is lost on termination; SWF is legacy"
        },
        examStrategy: "Scientific computing: AWS Batch with Spot + FSx for Lustre + SageMaker Studio + workflow orchestration."
    }
},

{
    id: 'dva_461',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A multi-region disaster recovery system requires RPO of 1 minute and RTO of 5 minutes for a critical application using RDS, DynamoDB, S3, and Lambda. The system must handle automatic failover and data consistency across regions.",
    question: "Which DR architecture meets the aggressive RPO/RTO requirements?",
    options: [
        "Aurora Global Database with write forwarding, DynamoDB Global Tables, S3 Cross-Region Replication, Lambda@Edge for routing, Route 53 health checks for automated failover",
        "RDS with read replicas promoted manually, DynamoDB on-demand backups, S3 lifecycle policies, Lambda with multi-region deployment, CloudWatch alarms for failover",
        "Database Migration Service for continuous replication, Point-in-time recovery for DynamoDB, S3 versioning, API Gateway with multiple origins, manual DNS updates",
        "Aurora with backtrack, DynamoDB Streams to secondary region, S3 Transfer Acceleration, Step Functions for failover orchestration, CloudFront for routing"
    ],
    correct: 0,
    explanation: {
        correct: "Aurora Global Database provides <1 second replication with automatic failover. DynamoDB Global Tables ensure multi-region consistency. S3 CRR maintains object replication. Lambda@Edge handles request routing. Route 53 automates failover within minutes.",
        whyWrong: {
            1: "Manual promotion and on-demand backups can't meet 5-minute RTO; lacks automation",
            2: "DMS adds complexity; point-in-time recovery doesn't meet 1-minute RPO; manual DNS updates delay RTO",
            3: "Backtrack is for Aurora rollback not DR; Streams replication requires custom code; lacks integrated failover"
        },
        examStrategy: "Aggressive DR requirements: Global databases + Cross-Region Replication + automated failover + edge routing."
    }
},

{
    id: 'dva_462',
    domain: "Domain 1: Development",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A recommendation engine processes 100 million user interactions daily, updates ML models hourly, serves personalized recommendations with 50ms latency, and requires A/B testing across 10 model variants. The system must explain recommendations for compliance.",
    question: "What architecture delivers explainable real-time recommendations at scale?",
    options: [
        "SageMaker Feature Store for features, SageMaker Model Registry for versions, SageMaker Endpoints with A/B testing, DynamoDB Accelerator for caching, SageMaker Clarify for explanations",
        "Kinesis for data ingestion, EMR for model training, ElastiCache for serving, Lambda for A/B logic, custom explanation service",
        "Personalize for recommendations, Kinesis Data Analytics for real-time features, API Gateway for serving, CloudWatch Evidently for A/B testing, Lambda for explanations",
        "Glue for feature engineering, S3 for model storage, ECS for serving, Route 53 weighted routing for A/B testing, Comprehend for explanations"
    ],
    correct: 0,
    explanation: {
        correct: "SageMaker Feature Store provides consistent features for training/inference. Model Registry manages versions. Endpoints support native A/B testing. DAX ensures sub-50ms latency. Clarify provides model explanations for compliance.",
        whyWrong: {
            1: "EMR batch training can't update hourly efficiently; custom explanation service requires complex development",
            2: "Personalize lacks custom model flexibility; doesn't provide detailed explanations required for compliance",
            3: "Manual model serving on ECS complex; Comprehend is for NLP not recommendation explanations"
        },
        examStrategy: "ML recommendation system: SageMaker suite + feature store + A/B testing + model explanations."
    }
},

// Domain 2: Security (463-477) - 15 questions - Security Edge Cases

{
    id: 'dva_463',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A financial services application experiences sophisticated DDoS attacks targeting the API layer with 50,000 requests/second from 10,000 distributed IPs. Attacks use valid JWT tokens from compromised accounts and mimic legitimate traffic patterns. The application must maintain availability for genuine users.",
    question: "Which security architecture effectively mitigates sophisticated application-layer DDoS attacks?",
    options: [
        "AWS WAF with rate-based rules and AWS Managed Rules, Shield Advanced with DDoS Response Team, API Gateway throttling per API key, Lambda authorizer with anomaly detection using CloudWatch Logs Insights",
        "CloudFront with AWS Shield Standard, Security Groups with restricted IPs, API Gateway usage plans, Cognito with MFA enforcement",
        "Network Load Balancer with flow logs, GuardDuty for threat detection, API Gateway with AWS_IAM auth, Lambda for token validation",
        "Application Load Balancer with WAF, Route 53 geolocation routing, API Gateway caching, Secrets Manager for key rotation"
    ],
    correct: 0,
    explanation: {
        correct: "WAF rate-based rules with Managed Rules provide layered defense. Shield Advanced offers DDoS Response Team support. API Gateway throttling limits per-key impact. Lambda authorizer with anomaly detection identifies compromised tokens through behavior analysis.",
        whyWrong: {
            1: "Shield Standard insufficient for application-layer attacks; Security Groups can't analyze application patterns",
            2: "NLB operates at Layer 4, missing application-layer attack patterns; IAM auth doesn't help with compromised valid tokens",
            3: "Geolocation routing doesn't stop distributed attacks; caching might serve stale content to legitimate users"
        },
        examStrategy: "Application-layer DDoS defense: WAF + Shield Advanced + intelligent rate limiting + behavioral analysis."
    }
},

{
    id: 'dva_464',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A healthcare platform must implement zero-trust architecture with encrypted data in transit and at rest, certificate-based authentication for services, secret rotation every 30 days, and complete audit trails. Lateral movement between services must be prevented.",
    question: "Which security implementation achieves zero-trust architecture requirements?",
    options: [
        "App Mesh with mTLS between services, Secrets Manager with automatic rotation, KMS for encryption, Certificate Manager Private CA, CloudTrail with CloudWatch Events for monitoring",
        "API Gateway with client certificates, Parameter Store for secrets, S3 encryption, ACM public certificates, X-Ray for tracing",
        "Service-to-service VPN connections, HashiCorp Vault on EC2, CloudHSM for keys, self-signed certificates, VPC Flow Logs",
        "PrivateLink endpoints, Lambda environment variables, DynamoDB encryption, IAM roles, Config for compliance"
    ],
    correct: 0,
    explanation: {
        correct: "App Mesh provides mTLS for zero-trust service communication. Secrets Manager handles automatic rotation. Private CA issues internal certificates. CloudTrail provides comprehensive audit trails. This prevents lateral movement through service isolation.",
        whyWrong: {
            1: "Client certificates at API Gateway don't secure internal communication; Parameter Store lacks automatic rotation",
            2: "Service-to-service VPNs are complex; self-signed certificates lack centralized management",
            3: "PrivateLink doesn't provide mTLS; environment variables aren't suitable for secrets"
        },
        examStrategy: "Zero-trust architecture: Service mesh with mTLS + automatic secret rotation + private PKI + comprehensive auditing."
    }
},

{
    id: 'dva_465',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A payment processing system handles PCI DSS compliance requiring network segmentation, encrypted card data, key rotation, intrusion detection, and file integrity monitoring. The system processes 1 million transactions daily with real-time fraud detection.",
    question: "What architecture ensures PCI DSS compliance with fraud detection?",
    options: [
        "Dedicated VPC with private subnets, KMS customer-managed keys with rotation, GuardDuty with custom threat lists, Systems Manager with Config rules, Macie for card data discovery, Fraud Detector for real-time analysis",
        "Shared VPC with NACLs, SSE-S3 encryption, manual key management, CloudWatch Logs, Inspector for vulnerabilities, Lambda for fraud rules",
        "Transit Gateway with segmentation, CloudHSM for key storage, third-party IDS on EC2, S3 object lock, custom fraud detection service",
        "VPC peering for isolation, Secrets Manager, WAF for protection, CloudTrail, Security Hub, Rekognition for fraud patterns"
    ],
    correct: 0,
    explanation: {
        correct: "Dedicated VPC ensures network isolation. KMS with CMK provides controlled key management with rotation. GuardDuty offers managed threat detection. Config rules ensure compliance. Macie discovers sensitive data. Fraud Detector provides purpose-built ML fraud detection.",
        whyWrong: {
            1: "Shared VPC risks compliance; SSE-S3 lacks key control; manual key management violates PCI requirements",
            2: "Third-party IDS adds complexity; CloudHSM expensive for this scale; custom fraud detection requires ML expertise",
            3: "VPC peering doesn't provide sufficient isolation; Rekognition is for image/video not transaction fraud"
        },
        examStrategy: "PCI DSS compliance: Network isolation + managed key rotation + threat detection + automated fraud analysis."
    }
},

{
    id: 'dva_466',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A multi-tenant SaaS platform requires complete data isolation between tenants, encryption with tenant-specific keys, SAML SSO integration for enterprise customers, and API access with fine-grained permissions. Each tenant averages 100GB of data.",
    question: "How should tenant isolation and security be implemented?",
    options: [
        "Cognito User Pools with custom attributes for tenant ID, KMS multi-tenant key policies, S3 with prefix-based IAM policies, API Gateway with Lambda authorizers validating tenant context, DynamoDB with tenant-prefixed partition keys",
        "Separate AWS accounts per tenant, AWS SSO for authentication, Cross-account roles for access, S3 buckets per tenant, API Gateway with IAM authentication",
        "Single Cognito identity pool, KMS grants for key access, RDS row-level security, API Gateway API keys per tenant, Lambda with tenant validation",
        "IAM users per tenant, Secrets Manager for tenant keys, EBS encrypted volumes, Direct Connect per tenant, custom authorization service"
    ],
    correct: 0,
    explanation: {
        correct: "Cognito handles SAML SSO with tenant attributes. KMS key policies enable tenant-specific encryption. S3 IAM policies provide prefix-based isolation. Lambda authorizers enforce tenant context throughout requests. DynamoDB partition keys ensure data separation.",
        whyWrong: {
            1: "Separate accounts per tenant creates management overhead and complex billing for 100GB average data",
            2: "Single identity pool lacks tenant isolation; RDS row-level security complex to maintain at scale",
            3: "IAM users don't scale for multi-tenant; Direct Connect per tenant extremely expensive"
        },
        examStrategy: "Multi-tenant security: Attribute-based access control + tenant-scoped encryption + contextual authorization."
    }
},

{
    id: 'dva_467',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "An application requires end-to-end encryption where data remains encrypted during processing, with hardware-based attestation, secure enclaves for key management, and compliance with FIPS 140-2 Level 3. The application processes financial transactions requiring non-repudiation.",
    question: "Which architecture provides hardware-based security with encrypted processing?",
    options: [
        "Nitro Enclaves for isolated compute, KMS with CloudHSM for key management, AWS Certificate Manager Private CA for PKI, CloudTrail with integrity validation, DynamoDB with client-side encryption using AWS Encryption SDK",
        "Lambda with environment encryption, Secrets Manager with CMK, ACM public certificates, X-Ray for tracing, RDS with TDE",
        "Fargate with encrypted tasks, Parameter Store with SecureString, self-managed PKI, CloudWatch Logs encryption, S3 with SSE-KMS",
        "EC2 with encrypted EBS, Systems Manager for secrets, IAM certificates, Config for compliance, ElastiCache with encryption"
    ],
    correct: 0,
    explanation: {
        correct: "Nitro Enclaves provide hardware-isolated compute for encrypted processing. CloudHSM meets FIPS 140-2 Level 3. Private CA enables PKI for non-repudiation. CloudTrail integrity validation ensures audit trail authenticity. Encryption SDK enables client-side encryption.",
        whyWrong: {
            1: "Lambda environment encryption doesn't protect during processing; lacks hardware security module",
            2: "Fargate doesn't provide secure enclaves; self-managed PKI lacks attestation",
            3: "Standard EC2 lacks isolated compute; doesn't meet FIPS Level 3 requirements"
        },
        examStrategy: "Hardware-based security: Nitro Enclaves + CloudHSM + Private PKI + encrypted processing."
    }
},

{
    id: 'dva_468',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A government application requires data residency within specific AWS regions, encryption with government-approved algorithms, continuous compliance monitoring, and automated remediation of security findings. Access must be restricted to cleared personnel only.",
    question: "What architecture ensures government compliance requirements?",
    options: [
        "AWS GovCloud deployment, FIPS-validated KMS, Security Hub with custom controls, Systems Manager automation for remediation, IAM Identity Center with PIV/CAC authentication, Config conformance packs",
        "Standard region with data residency controls, CloudHSM, GuardDuty, Lambda for remediation, Cognito with MFA, Trusted Advisor",
        "Outposts deployment, customer-managed HSM, Inspector, Step Functions for remediation, Directory Service, CloudWatch Events",
        "Local Zones deployment, Secrets Manager, Macie, manual remediation processes, IAM users with hardware MFA, AWS Audit Manager"
    ],
    correct: 0,
    explanation: {
        correct: "GovCloud ensures data residency and compliance. FIPS-validated KMS meets encryption requirements. Security Hub centralizes findings. Systems Manager automates remediation. PIV/CAC authentication supports cleared personnel. Config conformance packs ensure continuous compliance.",
        whyWrong: {
            1: "Standard regions may not meet government requirements; Cognito doesn't support PIV/CAC",
            2: "Outposts doesn't guarantee government compliance; customer HSM adds complexity",
            3: "Local Zones lack compliance certifications; manual remediation doesn't meet automation requirements"
        },
        examStrategy: "Government compliance: GovCloud + FIPS encryption + automated compliance + PIV/CAC authentication."
    }
},

{
    id: 'dva_469',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A cryptocurrency exchange requires cold wallet integration, multi-signature transaction approval, real-time fraud detection for suspicious transfers, and immutable audit logs. The system must prevent insider threats and maintain air-gapped security for cold storage.",
    question: "How should the exchange implement secure cryptocurrency operations?",
    options: [
        "Lambda in isolated VPC for hot wallet operations, Step Functions for multi-sig workflow, Fraud Detector for anomaly detection, QLDB for immutable audit logs, Nitro Enclaves for key operations, S3 Object Lock for cold wallet backups",
        "EC2 with dedicated hosts, custom blockchain implementation, CloudWatch anomaly detector, DynamoDB with encryption, HSM for all keys, Glacier Vault Lock",
        "Managed Blockchain for transactions, Lambda for approvals, GuardDuty for threats, CloudTrail, Secrets Manager, S3 with versioning",
        "Fargate for wallet services, SQS for approval queue, Macie for detection, RDS with audit tables, KMS, EBS snapshots"
    ],
    correct: 0,
    explanation: {
        correct: "Isolated Lambda prevents network access. Step Functions orchestrates multi-signature approvals. Fraud Detector identifies suspicious patterns. QLDB provides immutable audit logs. Nitro Enclaves secure key operations. Object Lock ensures backup immutability.",
        whyWrong: {
            1: "Custom blockchain adds complexity; CloudWatch anomaly detector not specialized for fraud",
            2: "Managed Blockchain is for building blockchain apps not exchange operations; CloudTrail logs are not immutable",
            3: "RDS audit tables can be modified; Macie is for data discovery not fraud detection"
        },
        examStrategy: "Cryptocurrency security: Isolated compute + multi-sig workflows + fraud detection + immutable audit logs."
    }
},

{
    id: 'dva_470',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "An application requires API authentication that works offline for 30 days, supports 100,000 concurrent mobile users, enables immediate revocation when devices are compromised, and provides usage analytics per API key.",
    question: "What authentication architecture meets offline and revocation requirements?",
    options: [
        "API Gateway with Lambda authorizers validating JWT tokens with 30-day expiration, DynamoDB for revocation list checked on each request, CloudWatch for usage analytics, Cognito for token refresh when online",
        "API Gateway with API keys, usage plans with quotas, CloudFront with signed URLs, Systems Manager for key distribution",
        "API Gateway with IAM authentication, STS tokens with 30-day validity, S3 for revocation list, X-Ray for analytics",
        "API Gateway with Cognito authorizers, offline token validation, Parameter Store for revoked tokens, custom analytics service"
    ],
    correct: 0,
    explanation: {
        correct: "JWT tokens with 30-day expiration work offline. Lambda authorizers check DynamoDB revocation list for immediate revocation. CloudWatch provides built-in usage analytics. Cognito handles token refresh when devices reconnect.",
        whyWrong: {
            1: "API keys don't support user-level revocation; signed URLs are for content not API authentication",
            2: "IAM credentials shouldn't be embedded in mobile apps; STS tokens max duration is 12 hours",
            3: "Cognito authorizers require online validation; Parameter Store not optimal for revocation list"
        },
        examStrategy: "Offline API auth: JWT with long expiration + revocation list + Lambda authorizers + usage analytics."
    }
},

{
    id: 'dva_471',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A development team needs temporary access to production databases for debugging, with automatic credential rotation, session recording for audit, and prevention of data exfiltration. Access must be granted through an approval workflow.",
    question: "Which solution provides secure temporary database access with audit trails?",
    options: [
        "Systems Manager Session Manager with port forwarding disabled, Step Functions for approval workflow, RDS IAM authentication with temporary tokens, CloudTrail for audit, VPC endpoints to prevent internet routing",
        "Bastion hosts with session recording, Lambda for approvals, database passwords in Secrets Manager, CloudWatch Logs, Security Groups",
        "Direct Connect with MFA, manual approval process, RDS master credentials, X-Ray tracing, NACLs for network control",
        "VPN with certificate auth, ServiceNow for approvals, read replicas for debugging, Config for audit, WAF for data protection"
    ],
    correct: 0,
    explanation: {
        correct: "Session Manager provides secure access with session recording. Port forwarding disabled prevents data exfiltration. Step Functions automates approval workflow. RDS IAM auth generates temporary credentials. VPC endpoints ensure traffic stays within AWS.",
        whyWrong: {
            1: "Bastion hosts add infrastructure overhead; database passwords even in Secrets Manager are less secure than IAM auth",
            2: "Direct Connect for temporary access is overkill; master credentials shouldn't be shared",
            3: "Read replicas may contain sensitive data; WAF doesn't prevent database exfiltration"
        },
        examStrategy: "Temporary production access: Session Manager + approval automation + IAM database auth + audit recording."
    }
},

{
    id: 'dva_472',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A mobile banking app requires biometric authentication, device fingerprinting, location-based fraud detection, and adaptive MFA that increases security requirements for high-value transactions. The system must handle 50 million users.",
    question: "How should adaptive authentication be implemented at scale?",
    options: [
        "Cognito with custom authentication flow, Lambda triggers for risk assessment using device and location data, DynamoDB for device fingerprints, Step-up authentication with Cognito MFA, EventBridge for fraud alerts",
        "API Gateway with custom authorizer, ElastiCache for session management, third-party biometric SDK, SNS for MFA codes, CloudWatch for monitoring",
        "Federated authentication with SAML, RDS for user data, Lambda for biometric validation, SES for OTP delivery, GuardDuty for threat detection",
        "OAuth 2.0 with Auth0 integration, S3 for device profiles, Kinesis for location streaming, Pinpoint for push notifications, Macie for data protection"
    ],
    correct: 0,
    explanation: {
        correct: "Cognito scales to millions of users with custom authentication flows. Lambda triggers enable risk-based assessment. DynamoDB efficiently stores device fingerprints. Step-up authentication dynamically adjusts security. EventBridge enables real-time fraud response.",
        whyWrong: {
            1: "Custom session management in ElastiCache doesn't scale to 50M users; lacks integrated risk assessment",
            2: "SAML federation not suitable for mobile; RDS less efficient for device fingerprints at scale",
            3: "Third-party auth adds complexity; S3 not optimal for frequently accessed device profiles"
        },
        examStrategy: "Adaptive mobile auth: Cognito custom flows + risk-based Lambda triggers + step-up authentication."
    }
},

{
    id: 'dva_473',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "An organization needs to prevent data exfiltration from compromised Lambda functions that process sensitive data. Functions require internet access for third-party APIs but must be prevented from sending data to unauthorized destinations.",
    question: "What architecture prevents Lambda data exfiltration while maintaining functionality?",
    options: [
        "Lambda in VPC with NAT Gateway, VPC Flow Logs for monitoring, AWS Network Firewall with domain filtering, CloudWatch Logs Insights for anomaly detection, API Gateway for controlled egress",
        "Lambda with resource policies, Internet Gateway with Security Groups, GuardDuty for threat detection, S3 bucket policies, CloudTrail for audit",
        "Lambda with execution role restrictions, CloudFront for API access, WAF for filtering, Config rules, Direct Connect for third-party APIs",
        "Lambda layers with security controls, Transit Gateway with route tables, Shield for protection, Systems Manager for configuration, PrivateLink for APIs"
    ],
    correct: 0,
    explanation: {
        correct: "VPC isolation with NAT Gateway controls network flow. Network Firewall enables domain-based filtering for authorized destinations. VPC Flow Logs provide visibility. CloudWatch Logs Insights detects anomalous data patterns. API Gateway proxies approved external calls.",
        whyWrong: {
            1: "Security Groups don't inspect application-layer traffic; can't prevent data exfiltration to allowed IPs",
            2: "CloudFront doesn't provide egress control; execution roles don't prevent network exfiltration",
            3: "Transit Gateway doesn't filter traffic; PrivateLink only works for AWS services"
        },
        examStrategy: "Lambda data exfiltration prevention: VPC isolation + Network Firewall + domain filtering + egress proxy."
    }
},

{
    id: 'dva_474',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A blockchain application stores private keys for user wallets requiring threshold cryptography where 3 of 5 key shares must combine to sign transactions. Keys must never exist in complete form, even in memory. The system processes 10,000 transactions per hour.",
    question: "How should distributed key management be implemented?",
    options: [
        "Nitro Enclaves for secure multiparty computation, DynamoDB for encrypted key shares, Step Functions for threshold signing orchestration, Lambda for share combination in isolated environment, KMS for share encryption",
        "CloudHSM cluster with key shares, Lambda for signing logic, Secrets Manager for share storage, EventBridge for orchestration, S3 for backup",
        "EC2 with SGX enclaves, EBS encrypted volumes for shares, SQS for signing queue, custom MPC implementation, Glacier for key backup",
        "Fargate with encrypted environment, Parameter Store for shares, API Gateway for signing API, CloudFormation for deployment, RDS for key metadata"
    ],
    correct: 0,
    explanation: {
        correct: "Nitro Enclaves provide hardware-isolated secure computation for MPC. DynamoDB stores encrypted shares separately. Step Functions orchestrates threshold signing. Lambda in enclaves ensures keys never exist completely. KMS protects shares at rest.",
        whyWrong: {
            1: "CloudHSM doesn't support threshold cryptography natively; Secrets Manager risks exposing complete keys",
            2: "SGX enclaves require specific hardware; custom MPC implementation is complex and risky",
            3: "Fargate lacks secure enclaves; Parameter Store not designed for cryptographic material distribution"
        },
        examStrategy: "Threshold cryptography: Nitro Enclaves for MPC + distributed key shares + secure orchestration."
    }
},

{
    id: 'dva_475',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A legal document platform requires tamper-proof document storage with cryptographic proof of document integrity, time-stamping from trusted authority, long-term signature validation, and chain of custody tracking for court admissibility.",
    question: "Which architecture provides legally admissible document integrity?",
    options: [
        "QLDB for immutable ledger with cryptographic verification, S3 Object Lock for document storage, ACM Private CA for document signing, Time Stamp Authority integration via Lambda, CloudTrail with integrity validation for audit",
        "Blockchain on EC2 for immutability, EBS volumes with encryption, self-signed certificates, CloudWatch Events for timestamps, Config for tracking",
        "DynamoDB with versioning, S3 with MFA delete, public SSL certificates, Systems Manager for time sync, X-Ray for tracing",
        "RDS with audit triggers, Glacier with vault lock, IAM signing, NTP for time, CloudWatch Logs for chain of custody"
    ],
    correct: 0,
    explanation: {
        correct: "QLDB provides cryptographically verifiable immutable ledger perfect for legal requirements. Object Lock ensures document immutability. Private CA enables trusted document signing. TSA integration provides legal timestamps. CloudTrail integrity validation ensures audit authenticity.",
        whyWrong: {
            1: "Custom blockchain lacks legal framework; self-signed certificates lack trust; CloudWatch timestamps not legally recognized",
            2: "DynamoDB versioning can be modified; public SSL certs not for document signing; lacks cryptographic verification",
            3: "RDS triggers can be bypassed; IAM not for document signing; lacks comprehensive integrity proof"
        },
        examStrategy: "Legal document integrity: QLDB immutable ledger + Object Lock + trusted PKI + TSA timestamps."
    }
},

{
    id: 'dva_476',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "An IoT security system with 1 million devices requires mutual TLS authentication, automatic certificate renewal before expiry, immediate revocation for compromised devices, and must handle 100,000 concurrent connections.",
    question: "What architecture handles IoT device authentication at scale?",
    options: [
        "IoT Core with just-in-time provisioning, IoT Device Management for fleet operations, ACM Private CA with short-lived certificates, Lambda for certificate renewal, DynamoDB for device registry with revocation status",
        "API Gateway with client certificates, Certificate Manager public CA, S3 for CRL distribution, Lambda for validation, RDS for device database",
        "Application Load Balancer with mTLS, self-managed CA on EC2, ElastiCache for CRL, Step Functions for renewal, DocumentDB for registry",
        "Network Load Balancer with TLS termination, Secrets Manager for certificates, CloudFront for CRL, EventBridge for renewal, Neptune for device graph"
    ],
    correct: 0,
    explanation: {
        correct: "IoT Core handles millions of devices with built-in mTLS. Just-in-time provisioning simplifies onboarding. Private CA with short-lived certificates reduces revocation complexity. IoT Device Management handles fleet-wide operations. DynamoDB scales for device registry.",
        whyWrong: {
            1: "API Gateway client certificates don't scale to 1M devices; public CA expensive for IoT; S3 CRL has latency",
            2: "Self-managed CA requires significant operational overhead; ALB has connection limits",
            3: "NLB doesn't provide application-layer certificate validation; Secrets Manager not designed for 1M certificates"
        },
        examStrategy: "IoT authentication scale: IoT Core with mTLS + Private CA + just-in-time provisioning + fleet management."
    }
},

{
    id: 'dva_477',
    domain: "Domain 2: Security",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A research platform shares datasets with external collaborators requiring attribute-based encryption where data access depends on user attributes, time-based access windows, watermarking for traceability, and usage analytics. Datasets range from 1GB to 1TB.",
    question: "How should secure data sharing with fine-grained access control be implemented?",
    options: [
        "S3 Access Points with Lambda authorizers for attribute evaluation, S3 Object Lambda for dynamic watermarking, CloudWatch Events for time-based access control, S3 request metrics for usage analytics, KMS grants for encryption",
        "S3 with bucket policies, Cognito for authentication, CloudFront signed URLs, Textract for watermarking, Athena for analytics",
        "EFS with access points, IAM roles with tags, Lambda for watermarking, EventBridge for scheduling, CloudTrail for analytics",
        "FSx with Active Directory, SAML assertions, Fargate for processing, Step Functions for workflows, QuickSight for analytics"
    ],
    correct: 0,
    explanation: {
        correct: "S3 Access Points enable attribute-based access control. Lambda authorizers evaluate complex attribute rules. S3 Object Lambda adds watermarks dynamically without storing copies. CloudWatch Events manages time windows. S3 metrics provide detailed usage analytics.",
        whyWrong: {
            1: "Bucket policies lack attribute-based flexibility; signed URLs don't support complex attributes; Textract is for text extraction not watermarking",
            2: "EFS access points don't support attribute-based encryption; less suitable for large dataset sharing",
            3: "FSx lacks native attribute-based access; requires Windows infrastructure adding complexity"
        },
        examStrategy: "Secure data sharing: S3 Access Points + Object Lambda + attribute-based authorization + dynamic watermarking."
    }
},

// Domain 3: Deployment (478-490) - 13 questions - Deployment Edge Cases

{
    id: 'dva_478',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A global application requires zero-downtime deployment across 6 regions with database schema migrations, feature flags for gradual rollout, automatic rollback on 1% error rate increase, and compliance validation before production. Each region handles 50,000 requests/second.",
    question: "Which deployment strategy ensures zero-downtime with automatic rollback?",
    options: [
        "CodeDeploy with blue/green deployment and traffic shifting, RDS Blue/Green deployments for schema changes, AppConfig for feature flags, CloudWatch alarms triggering automatic rollback, Step Functions for multi-region orchestration",
        "CodePipeline with manual approval gates, Lambda for database migrations, Parameter Store for feature flags, X-Ray for error tracking, CloudFormation StackSets",
        "Jenkins on EC2 with custom scripts, Flyway for migrations, S3 for configuration, CloudWatch Logs for monitoring, Terraform for infrastructure",
        "GitLab CI/CD with runners, DMS for schema replication, Secrets Manager for flags, Application Load Balancer health checks, CDK for deployment"
    ],
    correct: 0,
    explanation: {
        correct: "CodeDeploy blue/green with traffic shifting enables zero-downtime. RDS Blue/Green handles schema migrations safely. AppConfig provides managed feature flags with validation. CloudWatch alarms trigger automatic rollback on error rate increase. Step Functions orchestrates complex multi-region workflows.",
        whyWrong: {
            1: "Manual approval gates delay deployment; Parameter Store lacks feature flag capabilities like validation and gradual rollout",
            2: "Jenkins requires infrastructure management; custom scripts increase failure risk at this scale",
            3: "GitLab runners add complexity; DMS not designed for schema migrations; Secrets Manager not for feature flags"
        },
        examStrategy: "Zero-downtime global deployment: Blue/green + traffic shifting + feature flags + automatic rollback + orchestration."
    }
},

{
    id: 'dva_479',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A microservices platform with 200 services requires canary deployments with automatic rollback, distributed tracing, service mesh for traffic management, and progressive delivery based on business metrics. Services are written in multiple languages.",
    question: "What deployment architecture supports progressive delivery for hundreds of microservices?",
    options: [
        "EKS with Flagger for progressive delivery, App Mesh for service mesh, X-Ray for distributed tracing, CloudWatch metrics for business KPIs, Flux for GitOps deployment, Prometheus for monitoring",
        "ECS with CodeDeploy, ALB weighted target groups, CloudMap for service discovery, CloudWatch Logs Insights, CodePipeline for CI/CD",
        "Lambda with aliases and weighted routing, API Gateway for mesh, X-Ray for tracing, EventBridge for orchestration, SAM for deployment",
        "EC2 with Auto Scaling groups, Route 53 weighted routing, ElastiCache for service registry, CloudTrail for tracing, Systems Manager for deployment"
    ],
    correct: 0,
    explanation: {
        correct: "EKS with Flagger automates progressive delivery with canary analysis. App Mesh provides language-agnostic service mesh. X-Ray enables distributed tracing. CloudWatch tracks business metrics for deployment decisions. Flux enables GitOps for 200 services.",
        whyWrong: {
            1: "ECS with CodeDeploy lacks advanced progressive delivery features; ALB doesn't provide service mesh capabilities",
            2: "Lambda doesn't suit all microservice patterns; API Gateway isn't a service mesh",
            3: "EC2 Auto Scaling lacks container orchestration; Route 53 not designed for service mesh traffic management"
        },
        examStrategy: "Microservices progressive delivery: Kubernetes + Flagger + service mesh + GitOps + business metrics."
    }
},

{
    id: 'dva_480',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A machine learning platform requires deploying models across edge locations, A/B testing with statistical significance, shadow deployments for validation, model versioning with instant rollback, and real-time performance monitoring. Models range from 50MB to 5GB.",
    question: "How should ML model deployment with edge distribution be architected?",
    options: [
        "SageMaker Multi-Model Endpoints with production variants for A/B testing, SageMaker Edge Manager for edge deployment, Model Registry for versioning, CloudWatch Evidently for experiment management, Data Capture for shadow validation",
        "Lambda@Edge with container images, CloudFront for distribution, DynamoDB for experiment tracking, S3 for model storage, CloudWatch for monitoring",
        "ECS on Outposts, ECR for model images, Route 53 for traffic splitting, RDS for experiment data, X-Ray for performance",
        "IoT Greengrass for edge, S3 for models, API Gateway for routing, ElastiCache for metrics, Step Functions for deployment"
    ],
    correct: 0,
    explanation: {
        correct: "SageMaker Multi-Model Endpoints efficiently serve multiple models with production variants for A/B testing. Edge Manager handles edge deployment and monitoring. Model Registry provides versioning and instant rollback. Evidently manages experiments with statistical rigor. Data Capture enables shadow deployments.",
        whyWrong: {
            1: "Lambda@Edge has size limits for containers; lacks ML-specific deployment features",
            2: "ECS on Outposts requires significant infrastructure; lacks ML experiment management",
            3: "IoT Greengrass not optimized for large ML models; manual experiment management lacks statistical analysis"
        },
        examStrategy: "ML edge deployment: SageMaker endpoints + Edge Manager + Model Registry + statistical experiments."
    }
},

{
    id: 'dva_481',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A financial system requires deployment validation including security scanning, compliance checks, performance testing under load, and integration tests across 20 dependent services. Deployments must complete within 30-minute maintenance windows.",
    question: "Which deployment pipeline ensures comprehensive validation within time constraints?",
    options: [
        "CodePipeline with parallel actions, CodeBuild for security scanning with Trivy, Lambda for compliance checks against Config rules, CodeBuild with Locust for load testing, Step Functions for integration test orchestration",
        "Jenkins with sequential stages, Inspector for scanning, manual compliance review, JMeter for load testing, custom integration scripts",
        "GitHub Actions with matrix builds, GuardDuty for security, Trusted Advisor for compliance, CloudWatch Synthetics for testing, EventBridge for orchestration",
        "CircleCI with workflows, Macie for scanning, AWS Audit Manager for compliance, Device Farm for testing, Lambda for integration"
    ],
    correct: 0,
    explanation: {
        correct: "CodePipeline parallel actions maximize speed within 30-minute window. CodeBuild with Trivy provides fast container scanning. Lambda quickly validates Config rules. Locust in CodeBuild enables scalable load testing. Step Functions efficiently orchestrates complex integration tests.",
        whyWrong: {
            1: "Sequential stages in Jenkins won't meet 30-minute window; manual compliance review adds delays",
            2: "GuardDuty is for threat detection not deployment scanning; Synthetics for monitoring not load testing",
            3: "Macie scans data not code; Device Farm is for mobile app testing not system load testing"
        },
        examStrategy: "Fast comprehensive validation: Parallel pipeline actions + automated security/compliance + orchestrated testing."
    }
},

{
    id: 'dva_482',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "An application requires deployment across hybrid cloud environments (AWS and on-premises), with unified CI/CD pipeline, consistent configuration management, secrets synchronization, and centralized monitoring. On-premises has intermittent connectivity.",
    question: "What deployment strategy works for hybrid cloud with connectivity issues?",
    options: [
        "Systems Manager with hybrid activations, CodeDeploy on-premises agents with S3 pull mode, Systems Manager Parameter Store with local caching, CloudWatch agent for unified monitoring, AWS Outposts for consistent infrastructure",
        "CodePipeline with custom actions, Ansible for configuration, HashiCorp Vault for secrets, Datadog for monitoring, Direct Connect for networking",
        "GitHub Actions self-hosted runners, Terraform for both environments, Kubernetes secrets, Prometheus federation, Site-to-Site VPN",
        "Jenkins with distributed agents, CloudFormation and ARM templates, Secrets Manager with replication, X-Ray with OpenTelemetry, Transit Gateway"
    ],
    correct: 0,
    explanation: {
        correct: "Systems Manager hybrid activations work with intermittent connectivity. CodeDeploy S3 pull mode handles disconnections gracefully. Parameter Store local caching ensures secrets availability. CloudWatch agent provides unified monitoring. Outposts extends AWS infrastructure on-premises.",
        whyWrong: {
            1: "CodePipeline requires constant connectivity; HashiCorp Vault adds third-party complexity",
            2: "GitHub runners need reliable internet; Kubernetes secrets don't sync easily across hybrid environments",
            3: "CloudFormation doesn't work on-premises; X-Ray requires connectivity for trace submission"
        },
        examStrategy: "Hybrid deployment with unreliable connectivity: Systems Manager + pull-based deployment + local caching."
    }
},

{
    id: 'dva_483',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A SaaS platform requires tenant-specific customizations during deployment, with different feature sets, resource limits, and configurations per tenant. Deployments must maintain tenant isolation while sharing core infrastructure. The platform serves 500 tenants.",
    question: "How should multi-tenant deployments with customization be managed?",
    options: [
        "CodePipeline with dynamic stage generation, CloudFormation with nested stacks and conditions, AppConfig for tenant configurations, Lambda for tenant-specific customization, Service Catalog for tenant resource provisioning",
        "Single pipeline with all tenant configurations, Terraform workspaces, S3 for configurations, EC2 user data for customization, manual resource allocation",
        "Jenkins with per-tenant jobs, CDK with environment variables, Parameter Store for all settings, ECS task definitions, Resource Groups for organization",
        "GitLab CI with includes, SAM templates, Secrets Manager for configs, Fargate profiles, Cost Explorer for limits"
    ],
    correct: 0,
    explanation: {
        correct: "Dynamic stage generation scales to 500 tenants. Nested stacks with conditions enable customization while sharing core templates. AppConfig manages tenant configurations with validation. Lambda applies tenant-specific logic. Service Catalog ensures consistent tenant provisioning.",
        whyWrong: {
            1: "Single pipeline with all configurations becomes unwieldy at 500 tenants; manual resource allocation doesn't scale",
            2: "Per-tenant Jenkins jobs create maintenance overhead; environment variables not suitable for complex configs",
            3: "GitLab includes become complex; Secrets Manager not designed for application configuration"
        },
        examStrategy: "Multi-tenant deployment: Dynamic pipelines + conditional templates + managed configurations + Service Catalog."
    }
},

{
    id: 'dva_484',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A regulated industry application requires deployment artifacts to be signed, with chain of custody tracking, immutable audit logs, and automated compliance reporting. Deployments must be reproducible for audit purposes years later.",
    question: "Which deployment architecture ensures long-term audit compliance?",
    options: [
        "CodePipeline with CodeArtifact for dependency management, CodeSigning for artifacts, QLDB for immutable audit trail, S3 with Object Lock for artifact retention, Config with conformance packs for compliance reporting",
        "Jenkins with Artifactory, GPG signing, CloudTrail for audit, Glacier for storage, manual compliance reports",
        "GitHub Actions with packages, Sigstore for signing, DynamoDB for audit, S3 versioning, Trusted Advisor for compliance",
        "CircleCI with artifacts, self-signed certificates, RDS for audit logs, EBS snapshots, Security Hub for reporting"
    ],
    correct: 0,
    explanation: {
        correct: "CodeArtifact ensures dependency reproducibility. Code Signing provides AWS-native artifact signing. QLDB offers immutable audit trail with cryptographic verification. Object Lock ensures long-term artifact retention. Config conformance packs automate compliance reporting.",
        whyWrong: {
            1: "Artifactory requires management; CloudTrail logs can be deleted; manual compliance reporting is error-prone",
            2: "GitHub packages may not meet retention requirements; DynamoDB not immutable; Trusted Advisor lacks detailed compliance",
            3: "Self-signed certificates lack trust chain; RDS audit logs can be modified; EBS snapshots can be deleted"
        },
        examStrategy: "Audit-compliant deployment: Artifact signing + immutable audit logs + long-term retention + automated compliance."
    }
},

{
    id: 'dva_485',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A mobile app requires coordinated backend and frontend releases across iOS, Android, and web platforms. Backend API changes must be backward compatible during rollout. The app has 10 million active users with phased geographic rollout.",
    question: "How should coordinated multi-platform deployment be orchestrated?",
    options: [
        "API Gateway with stage variables and canary deployments, AppConfig for feature flags across platforms, Route 53 geolocation routing for phased rollout, CloudFront for web app versioning, Lambda@Edge for API version routing",
        "Multiple CodePipelines with manual coordination, S3 for mobile app hosting, CloudFront for distribution, Lambda for API versioning, Parameter Store for flags",
        "Single monolithic pipeline, Device Farm for testing, API Gateway with versions, DynamoDB for feature flags, Direct Connect for app stores",
        "Jenkins with parallel jobs, ECR for containers, ALB with path routing, ElastiCache for flags, CloudWatch Events for coordination"
    ],
    correct: 0,
    explanation: {
        correct: "API Gateway canary deployments ensure backward compatibility. AppConfig provides centralized feature flags for all platforms. Route 53 geolocation enables geographic phased rollout. CloudFront versioning supports web rollback. Lambda@Edge intelligently routes API versions.",
        whyWrong: {
            1: "Manual coordination prone to errors; S3 not suitable for mobile app distribution to stores",
            2: "Monolithic pipeline lacks flexibility; Direct Connect unnecessary for app store submission",
            3: "Jenkins parallel jobs require complex coordination; ALB path routing doesn't solve mobile versioning"
        },
        examStrategy: "Multi-platform deployment: API versioning + centralized feature flags + geographic rollout + edge routing."
    }
},

{
    id: 'dva_486',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A data pipeline requires deploying Spark jobs, ML models, and streaming applications with dependency management, resource optimization, and cost allocation per team. Jobs run on EMR, Glue, and Kinesis Analytics. Each team has different resource quotas.",
    question: "What deployment strategy manages diverse data workloads with team quotas?",
    options: [
        "Step Functions for workflow orchestration, Service Catalog for team resource provisioning, AWS Budgets with actions for quota enforcement, CodeArtifact for dependency management, Cost and Usage Reports with tags for allocation",
        "Airflow on EC2 for orchestration, manual EMR cluster management, S3 for dependencies, CloudWatch billing alarms, spreadsheet for cost tracking",
        "Lambda for job submission, CloudFormation for all resources, pip/maven in S3, Cost Explorer for monitoring, IAM policies for quotas",
        "Jenkins for deployment, Terraform for infrastructure, Docker images for dependencies, Trusted Advisor for cost, Resource Groups for organization"
    ],
    correct: 0,
    explanation: {
        correct: "Step Functions orchestrates diverse workload types. Service Catalog ensures consistent team provisioning with quotas. Budgets with actions automatically enforce limits. CodeArtifact centralizes dependency management. Cost allocation tags enable accurate team billing.",
        whyWrong: {
            1: "Self-managed Airflow adds overhead; manual EMR management doesn't scale; spreadsheet tracking is error-prone",
            2: "Lambda not suitable for long-running Spark jobs; IAM doesn't enforce cost quotas",
            3: "Jenkins not optimized for data workflows; Trusted Advisor doesn't provide detailed cost allocation"
        },
        examStrategy: "Data workload deployment: Orchestration + Service Catalog + automated quotas + dependency management + cost allocation."
    }
},

{
    id: 'dva_487',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A disaster recovery deployment requires automated failover with data consistency validation, DNS updates, application stack deployment in secondary region, and rollback capability if validation fails. RTO is 15 minutes with zero data loss.",
    question: "Which DR deployment architecture meets aggressive RTO with validation?",
    options: [
        "Route 53 Application Recovery Controller for failover orchestration, CloudFormation StackSets for multi-region deployment, DMS with validation for data consistency, Lambda for post-deployment validation, Step Functions for rollback workflow",
        "Manual Route 53 updates, separate CloudFormation stacks, S3 cross-region replication, CloudWatch alarms, custom scripts for validation",
        "Route 53 simple routing, Terraform with workspaces, database snapshots, X-Ray for validation, EventBridge for automation",
        "Route 53 failover routing, CDK deployment, Aurora backtrack, Config for validation, SNS for notifications"
    ],
    correct: 0,
    explanation: {
        correct: "Application Recovery Controller provides coordinated failover with readiness checks. StackSets ensure consistent multi-region deployment. DMS validation confirms data consistency. Lambda validates application health. Step Functions orchestrates complex rollback if needed.",
        whyWrong: {
            1: "Manual updates can't meet 15-minute RTO; lacks automated validation and rollback",
            2: "Simple routing lacks health checks; snapshots don't provide zero data loss; X-Ray not for deployment validation",
            3: "Backtrack is for Aurora rollback not DR; Config validates compliance not application health"
        },
        examStrategy: "Automated DR deployment: Application Recovery Controller + StackSets + data validation + orchestrated rollback."
    }
},

{
    id: 'dva_488',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A serverless application with 100+ Lambda functions requires coordinated deployment with shared layers, function-specific environment configurations, gradual traffic shifting, and automatic rollback on CloudWatch alarms. Functions have complex dependencies.",
    question: "How should large-scale serverless deployment be managed?",
    options: [
        "SAM with nested applications for organization, CodeDeploy for traffic shifting, Lambda layers for shared code, Systems Manager Parameter Store for hierarchical configs, CloudWatch Synthetics for validation, Step Functions for dependency orchestration",
        "Serverless Framework with plugins, manual alias updates, individual function packages, environment variables for config, CloudWatch Logs for monitoring, custom scripts",
        "CDK with constructs, API Gateway stages, Lambda versions, Secrets Manager for all configs, X-Ray for monitoring, EventBridge for coordination",
        "Terraform modules, Lambda versioning, S3 for shared libraries, DynamoDB for configuration, CloudTrail for monitoring, SQS for orchestration"
    ],
    correct: 0,
    explanation: {
        correct: "SAM nested applications organize 100+ functions effectively. CodeDeploy provides gradual traffic shifting with automatic rollback. Lambda layers efficiently share code. Parameter Store hierarchical configs scale well. Synthetics validates deployment. Step Functions handles complex dependencies.",
        whyWrong: {
            1: "Manual alias updates error-prone for 100+ functions; individual packages inefficient; lacks automated rollback",
            2: "API Gateway stages don't handle Lambda traffic shifting; Secrets Manager expensive for all configs",
            3: "S3 for libraries adds cold start latency; DynamoDB not ideal for configuration; SQS doesn't orchestrate dependencies"
        },
        examStrategy: "Serverless scale deployment: SAM nested apps + CodeDeploy + layers + hierarchical config + orchestration."
    }
},

{
    id: 'dva_489',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A legacy modernization project requires strangler fig pattern implementation, gradually migrating from EC2 monolith to microservices. Migration must maintain zero downtime, with ability to instantly route traffic back to legacy system if issues arise.",
    question: "Which deployment approach implements strangler fig pattern effectively?",
    options: [
        "Application Load Balancer with weighted target groups for gradual migration, AWS Migration Hub for progress tracking, CloudWatch RUM for user experience monitoring, Route 53 for instant failback, API Gateway for new microservices facade",
        "Classic Load Balancer with instances, manual traffic management, CloudWatch Logs for monitoring, DNS TTL changes for failback, direct microservice exposure",
        "Network Load Balancer with target groups, custom migration scripts, X-Ray for monitoring, EIP reassignment for failback, Lambda for routing",
        "Route 53 with multivalue answers, spreadsheet for tracking, CloudTrail for monitoring, manual failback procedures, EC2 for microservices"
    ],
    correct: 0,
    explanation: {
        correct: "ALB weighted target groups enable gradual traffic shifting. Migration Hub tracks modernization progress. CloudWatch RUM monitors real user impact. Route 53 instant failback ensures safety. API Gateway provides unified facade over new microservices.",
        whyWrong: {
            1: "Classic Load Balancer lacks advanced routing; manual traffic management is error-prone; DNS TTL delays failback",
            2: "NLB operates at Layer 4, missing application routing features; EIP reassignment is disruptive",
            3: "Multivalue answers don't provide controlled traffic percentages; manual procedures delay critical failback"
        },
        examStrategy: "Strangler fig pattern: ALB weighted routing + progress tracking + instant failback + API facade."
    }
},

{
    id: 'dva_490',
    domain: "Domain 3: Deployment",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "A platform requires deploying infrastructure changes that affect 1000+ resources across 50 AWS accounts. Changes must be tested in dev, validated in staging, and rolled out to production with account-specific configurations and automatic rollback on failure.",
    question: "How should large-scale multi-account infrastructure deployment be orchestrated?",
    options: [
        "Control Tower with customizations, CloudFormation StackSets with service-managed permissions, Config conformance packs for validation, Step Functions for multi-stage orchestration, Organizations SCPs for guardrails, Systems Manager Change Calendar for deployment windows",
        "Individual account access, separate CloudFormation stacks, manual validation, email coordination, IAM roles, calendar invites for deployment",
        "Terraform with remote state, GitHub Actions for deployment, AWS SSO for access, CloudWatch for validation, SNS for coordination, Excel for scheduling",
        "CDK with pipelines, CodeCommit for source, Inspector for validation, EventBridge for orchestration, Resource Groups for organization, Parameter Store for configuration"
    ],
    correct: 0,
    explanation: {
        correct: "Control Tower provides governance foundation. StackSets with service-managed permissions handles 50 accounts efficiently. Config validates compliance. Step Functions orchestrates complex multi-stage deployments. SCPs prevent dangerous actions. Change Calendar enforces deployment windows.",
        whyWrong: {
            1: "Individual account management doesn't scale to 50 accounts; manual validation/coordination is error-prone",
            2: "Terraform state management complex for 50 accounts; lacks native AWS governance features",
            3: "Inspector scans instances not infrastructure; Parameter Store not ideal for complex multi-account configs"
        },
        examStrategy: "Multi-account deployment: Control Tower + StackSets + automated validation + orchestration + governance."
    }
},

// Domain 4: Troubleshooting and Optimization (491-500) - 10 questions - Complex Troubleshooting

{
    id: 'dva_491',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "Lambda functions experience sporadic cold starts of 15+ seconds despite provisioned concurrency set to 100. Functions are in VPC accessing RDS and ElastiCache. CloudWatch shows provisioned concurrency is utilized but InitDuration spikes occur. The issue happens more frequently during traffic surges.",
    question: "What troubleshooting approach identifies the root cause of cold starts despite provisioned concurrency?",
    options: [
        "Analyze CloudWatch Logs for provisioned concurrency spillover, check ENI attachment times in VPC flow logs, verify RDS Proxy connection pooling, monitor Lambda concurrent execution limits versus actual usage, review Lambda layer initialization impact",
        "Increase provisioned concurrency to 500, remove VPC configuration, use DynamoDB instead of RDS, add more memory to Lambda functions",
        "Enable X-Ray tracing, check Security Group rules, analyze CloudWatch metrics, restart RDS instances, clear ElastiCache",
        "Review Lambda code for memory leaks, check IAM permissions, analyze API Gateway logs, update Lambda runtime version"
    ],
    correct: 0,
    explanation: {
        correct: "Spillover occurs when traffic exceeds provisioned concurrency causing cold starts. ENI attachment in VPC adds latency. RDS Proxy prevents connection exhaustion. Concurrent execution limits may be hit. Large Lambda layers increase initialization time.",
        whyWrong: {
            1: "Simply increasing resources doesn't identify root cause; removing VPC breaks architecture requirements",
            2: "X-Ray helps but doesn't specifically identify provisioned concurrency spillover; restarting services is temporary",
            3: "Code issues don't explain provisioned concurrency cold starts; runtime updates unlikely to solve VPC-related delays"
        },
        examStrategy: "Lambda cold start diagnosis: Spillover analysis + ENI attachment times + connection pooling + concurrency limits + layer overhead."
    }
},

{
    id: 'dva_492',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "DynamoDB Global Tables show inconsistent data across regions with some items missing in secondary regions. Write operations succeed but EventualConsistency reads return different results. CloudWatch shows no throttling. The issue started after enabling auto-scaling.",
    question: "Which troubleshooting steps identify Global Table replication issues?",
    options: [
        "Check CloudWatch Global Table metrics for ReplicationLatency and UserErrors, analyze ConditionalCheckFailedException in logs, verify auto-scaling isn't causing write throttling in replicas, review application's conflict resolution logic, validate IAM permissions for cross-region replication",
        "Increase read and write capacity units, disable auto-scaling, switch to strong consistency reads, recreate Global Tables",
        "Enable DynamoDB Streams, check network connectivity, analyze VPC endpoints, review Security Groups, restart application",
        "Run DynamoDB consistency checker, enable point-in-time recovery, check AWS service health, contact AWS Support"
    ],
    correct: 0,
    explanation: {
        correct: "ReplicationLatency metrics show cross-region delays. UserErrors indicate application-level issues. ConditionalCheckFailedException suggests conflict resolution problems. Auto-scaling in replicas might throttle replication. IAM permissions affect cross-region replication.",
        whyWrong: {
            1: "Increasing capacity without identifying issues wastes resources; strong consistency not available for Global Tables",
            2: "Streams already enabled for Global Tables; network/VPC not related to Global Table replication",
            3: "No built-in consistency checker tool; PITR doesn't fix replication; service health rarely causes selective replication issues"
        },
        examStrategy: "Global Table troubleshooting: Replication metrics + conflict resolution + auto-scaling impact + IAM permissions."
    }
},

{
    id: 'dva_493',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "API Gateway REST API returns 502 errors intermittently. Backend Lambda functions complete successfully according to CloudWatch Logs. X-Ray traces show gaps between API Gateway and Lambda. The issue occurs more frequently during high load but also happens during low traffic.",
    question: "What diagnostic approach identifies the cause of API Gateway 502 errors?",
    options: [
        "Review API Gateway CloudWatch Logs for integration timeout versus Lambda duration, check Lambda reserved concurrency versus API Gateway burst limits, analyze Lambda response payload size versus API Gateway limits, verify Lambda response format matches API Gateway integration response mapping",
        "Increase API Gateway timeout to maximum, add more Lambda memory, enable API Gateway caching, implement retry logic",
        "Check API Gateway throttling settings, review Lambda error logs, analyze VPC configuration, update API Gateway stage",
        "Enable detailed CloudWatch metrics, review AWS service limits, check Lambda environment variables, redeploy API Gateway"
    ],
    correct: 0,
    explanation: {
        correct: "Integration timeout mismatch causes 502s when Lambda runs longer than API Gateway expects. Reserved concurrency limiting Lambda while API Gateway accepts requests causes failures. Payload size over 10MB limit causes 502s. Incorrect response format mapping results in integration failures.",
        whyWrong: {
            1: "Increasing timeouts masks the problem; caching doesn't help with integration failures",
            2: "Throttling returns 429 not 502; VPC not related if Lambda succeeds; stage updates don't fix integration issues",
            3: "Detailed metrics help but don't pinpoint integration issues; service limits return different errors"
        },
        examStrategy: "API Gateway 502 diagnosis: Integration timeout + concurrency mismatch + payload size + response mapping."
    }
},

{
    id: 'dva_494',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "ECS Fargate tasks randomly fail with 'ResourceInitializationError: unable to pull secrets or registry auth'. Tasks use Secrets Manager for database credentials and ECR for images. Some tasks in the same service start successfully while others fail. VPC endpoints are configured.",
    question: "Which troubleshooting approach identifies Fargate task initialization failures?",
    options: [
        "Verify VPC endpoint policies allow Secrets Manager and ECR access, check task execution role permissions for specific secret ARNs, analyze subnet route tables for endpoint routes, confirm security groups allow HTTPS to VPC endpoints, review CloudTrail for permission denied events",
        "Recreate task definition, increase task memory and CPU, restart ECS service, clear ECR cache",
        "Check Secrets Manager rotation, update ECS agent, review task role permissions, enable ECS Exec for debugging",
        "Analyze CloudWatch Container Insights, check AWS service health, update Fargate platform version, contact support"
    ],
    correct: 0,
    explanation: {
        correct: "VPC endpoint policies might block specific resources. Task execution role needs access to specific secrets and ECR. Route tables must include endpoint routes. Security groups must allow HTTPS (443) to endpoints. CloudTrail reveals permission issues.",
        whyWrong: {
            1: "Recreating resources doesn't identify root cause; Fargate manages resources automatically",
            2: "ECS agent is managed by AWS in Fargate; task role is for application not initialization",
            3: "Container Insights shows runtime not initialization issues; platform version updates rarely fix permission issues"
        },
        examStrategy: "Fargate initialization failures: VPC endpoint policies + execution role permissions + network routes + security groups."
    }
},

{
    id: 'dva_495',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "S3 uploads using multipart upload API fail intermittently with 'RequestTimeout' errors. Files range from 100MB to 5GB. The issue occurs across multiple regions and affects 5% of uploads. Network connectivity tests show no packet loss. Direct PUT operations for small files work fine.",
    question: "What systematic approach diagnoses multipart upload failures?",
    options: [
        "Analyze S3 request IDs in CloudTrail for timeout patterns, check multipart upload part size versus network bandwidth, review concurrent part upload limits, verify S3 Transfer Acceleration endpoint health, examine client-side retry logic and exponential backoff implementation",
        "Increase multipart threshold, use single PUT for all files, enable S3 Transfer Acceleration globally, upgrade SDK version",
        "Check S3 bucket policies, review CORS configuration, analyze IAM permissions, enable versioning",
        "Monitor S3 service health, check VPC endpoints, review CloudFront distribution, contact AWS Support"
    ],
    correct: 0,
    explanation: {
        correct: "Request IDs help AWS trace specific failures. Part size too large for network bandwidth causes timeouts. Concurrent part limits affect large file uploads. Transfer Acceleration endpoint issues cause regional problems. Missing exponential backoff causes retry storms.",
        whyWrong: {
            1: "Single PUT has 5GB limit; blindly enabling Transfer Acceleration may not help; SDK updates don't fix network issues",
            2: "Permissions work since 95% succeed; CORS doesn't affect API uploads; versioning unrelated to timeouts",
            3: "Service health affects all uploads not 5%; VPC endpoints don't affect multi-region; CloudFront not involved in direct uploads"
        },
        examStrategy: "Multipart upload troubleshooting: Request ID analysis + part sizing + concurrency limits + retry logic."
    }
},

{
    id: 'dva_496',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "RDS Aurora MySQL experiences query performance degradation where previously fast queries now take 10x longer. Performance Insights shows high wait events for 'synch/mutex/innodb'. CPU utilization is 40%. The issue started after a minor version upgrade. No schema changes were made.",
    question: "Which optimization approach addresses Aurora performance degradation?",
    options: [
        "Analyze Performance Insights for mutex contention patterns, review Aurora MySQL release notes for version-specific issues, check Query Store for execution plan changes, evaluate Aurora parallel query eligibility, verify parameter group settings match workload after upgrade",
        "Increase instance size, add read replicas, enable Performance Schema, restart database",
        "Rebuild indexes, update table statistics, increase buffer pool size, run OPTIMIZE TABLE",
        "Enable slow query log, check tablespace, review binlog settings, upgrade to latest version"
    ],
    correct: 0,
    explanation: {
        correct: "Mutex contention indicates locking issues possibly introduced by version changes. Release notes reveal known performance impacts. Query Store shows if optimizer behavior changed. Parallel query might bypass mutex issues. Parameter group defaults may have changed with version.",
        whyWrong: {
            1: "More resources don't fix mutex contention; Performance Schema adds overhead; restart is temporary",
            2: "Index rebuilding doesn't address mutex issues; Aurora manages buffer pool automatically",
            3: "Slow query log shows symptoms not causes; upgrading further may introduce more issues"
        },
        examStrategy: "Aurora performance degradation: Mutex contention analysis + version-specific issues + execution plans + parameter validation."
    }
},

{
    id: 'dva_497',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "CloudFront distribution serving dynamic content shows cache hit ratio of 0% despite Cache-Control headers set to 'public, max-age=3600'. Origin is API Gateway. Some users report stale content while others see updates immediately. CloudWatch shows high origin requests.",
    question: "What investigation identifies CloudFront cache inefficiency causes?",
    options: [
        "Review CloudFront cache behaviors for query string and header forwarding settings, analyze origin response Vary headers affecting cache keys, check CloudFront cache policies versus Cache-Control precedence, verify API Gateway isn't sending no-cache headers, examine personalized content patterns",
        "Increase default TTL, disable query string forwarding, remove all cache headers, enable CloudFront compression",
        "Check origin health, review SSL certificates, analyze CloudFront access logs, update price class",
        "Enable all caching, review AWS WAF rules, check Route 53 routing, clear CloudFront cache"
    ],
    correct: 0,
    explanation: {
        correct: "Forwarding all headers/query strings creates unique cache keys preventing hits. Vary headers multiply cache entries. CloudFront policies can override Cache-Control. API Gateway may add no-cache headers. Personalized content prevents caching.",
        whyWrong: {
            1: "Disabling features reduces functionality; removing cache headers worsens the problem",
            2: "Origin health doesn't affect cache ratio; certificates unrelated to caching; price class affects locations not caching",
            3: "WAF doesn't affect caching; Route 53 routing unrelated; clearing cache doesn't fix configuration"
        },
        examStrategy: "CloudFront cache optimization: Cache key analysis + header forwarding + policy precedence + origin headers."
    }
},

{
    id: 'dva_498',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "Kinesis Data Streams consumers experience increasing iterator age up to 24 hours. Shard count was recently doubled. Enhanced fan-out is enabled. Lambda consumers show no errors but process fewer records than produced. Shard metrics show uneven distribution.",
    question: "How should Kinesis consumer lag be diagnosed and resolved?",
    options: [
        "Analyze shard-level metrics for hot shards causing backpressure, review partition key strategy for even distribution, check Lambda IteratorAge metric versus shard iterator expiration, verify resharding properly distributed existing records, evaluate consumer parallelization matching shard count",
        "Add more shards, increase Lambda concurrency, enable Kinesis auto-scaling, implement DLQ",
        "Check Kinesis limits, review Lambda timeout, analyze network connectivity, upgrade SDK",
        "Enable detailed monitoring, check service quotas, review IAM permissions, restart consumers"
    ],
    correct: 0,
    explanation: {
        correct: "Hot shards create processing bottlenecks even with sufficient total capacity. Poor partition keys cause uneven distribution. Iterator expiration causes data skipping. Resharding might not redistribute existing backlogs. Consumer parallelization must match increased shard count.",
        whyWrong: {
            1: "Adding shards without fixing distribution worsens hot shard problem; DLQ doesn't help with lag",
            2: "Service limits show as throttling; timeout doesn't explain reduced processing; network works if some records process",
            3: "Detailed monitoring shows symptoms not causes; permissions work since some processing occurs"
        },
        examStrategy: "Kinesis lag diagnosis: Hot shard identification + partition key analysis + iterator management + consumer parallelization."
    }
},

{
    id: 'dva_499',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "Step Functions state machine with 50 states experiences sporadic 'States.Timeout' errors in random states. Individual Lambda functions complete within their timeout limits. The issue occurs more frequently during business hours. CloudWatch shows no throttling.",
    question: "Which troubleshooting approach identifies Step Functions timeout root causes?",
    options: [
        "Analyze state machine definition for HeartbeatSeconds in long-running tasks, check state transition history for cumulative delays, review Lambda payload sizes affecting state transition time, verify state machine execution timeout versus sum of state timeouts, examine concurrent execution limits",
        "Increase all state timeouts, add retry configuration, enable Step Functions tracing, reduce state count",
        "Check Lambda reserved concurrency, review VPC configuration, analyze CloudWatch Logs, update IAM roles",
        "Monitor Step Functions service health, check API Gateway integration, review S3 bucket policies, enable X-Ray"
    ],
    correct: 0,
    explanation: {
        correct: "Missing HeartbeatSeconds causes timeout in long tasks. Cumulative small delays across 50 states exceed execution timeout. Large payloads between states add transition overhead. State machine timeout must accommodate all states. Concurrent execution limits cause queuing delays.",
        whyWrong: {
            1: "Increasing timeouts masks underlying issues; reducing states requires architecture changes",
            2: "Lambda concurrency shows as throttling; VPC doesn't affect Step Functions directly",
            3: "Service health affects all executions; unrelated service permissions don't cause timeouts in random states"
        },
        examStrategy: "Step Functions timeout diagnosis: Heartbeat configuration + cumulative delays + payload overhead + execution limits."
    }
},

{
    id: 'dva_500',
    domain: "Domain 4: Troubleshooting",
    difficulty: "hard",
    timeRecommendation: 180,
    scenario: "Application experiences cascading failures where one Lambda timeout causes entire system degradation. SQS DLQ fills rapidly, DynamoDB throttling occurs, and CloudWatch Logs show exponential growth in error rates. Recovery takes hours after fixing initial issue.",
    question: "What systematic approach prevents and recovers from cascading failures?",
    options: [
        "Implement circuit breakers in Lambda functions to fail fast, configure SQS redrive policy with exponential backoff, set DynamoDB auto-scaling with target tracking, use Step Functions for orchestration with error boundaries, deploy CloudWatch Composite Alarms for multi-metric correlation",
        "Increase all service limits, add more Lambda concurrency, scale DynamoDB to maximum, implement global retry logic",
        "Enable X-Ray for all services, add CloudWatch dashboards, increase logging, contact AWS Support",
        "Implement health checks, add monitoring alerts, review service dependencies, document runbooks"
    ],
    correct: 0,
    explanation: {
        correct: "Circuit breakers prevent cascade by failing fast when downstream services struggle. Exponential backoff in redrive policy prevents DLQ overwhelming systems. Auto-scaling with target tracking adapts to load. Step Functions error boundaries isolate failures. Composite Alarms detect complex failure patterns early.",
        whyWrong: {
            1: "Blindly increasing limits may worsen cascading failures; global retry without backoff amplifies problems",
            2: "X-Ray shows problems but doesn't prevent them; more logging can overwhelm during failures",
            3: "Health checks and documentation help but don't provide automatic failure isolation and recovery"
        },
        examStrategy: "Cascading failure prevention: Circuit breakers + exponential backoff + auto-scaling + error boundaries + composite alarms."
    }
}
]; // Close the questions array
// Find the exact line where questionBank array closes
fetch(window.location.href)
    .then(response => response.text())
    .then(html => {
        const lines = html.split('\n');
        
        // Find where questionBank ends
        for(let i = 0; i < lines.length; i++) {
            if (lines[i].includes('const questionBank = [')) {
                console.log(`✅ questionBank starts at line ${i + 1}`);
            }
            
            // Look for the closing of questionBank
            if (lines[i].trim() === '];' && i > 1000) {  // After line 1000 to skip early code
                console.log(`📍 Possible questionBank end at line ${i + 1}`);
                console.log("Previous line:", lines[i-1].substring(0, 100));
                console.log("This line:", lines[i]);
                console.log("Next line:", lines[i+1].substring(0, 100));
                
                // Check if this is really the end of questionBank
                if (lines[i-1].includes('}')) {
                    console.log(`✅ This looks like the END of questionBank array at line ${i + 1}`);
                    console.log("\n⚠️ ADD THE startMode FUNCTION AFTER LINE", i + 1);
                    break;
                }
            }
        }
    });        

// PASTE ALL OF THIS AFTER THE LINE ABOVE:

// Make questions available
const questions = questionBank;
console.log("Questions loaded:", questions.length);

// Quiz state variables
let currentQuestionIndex = 0;
let score = 0;
let userAnswers = [];
let currentMode = 'practice';
let modeQuestions = [];

// Define mode configurations
const modeConfigs = {
    triage: { name: 'Triage Training', questionCount: 20 },
    speed: { name: 'Speed Round', questionCount: 30 },
    exam: { name: 'Full Exam', questionCount: 65 },
    strategic: { name: 'Strategic Practice', questionCount: 40 },
    practice: { name: 'Practice Mode', questionCount: 50 }
};

// Enhanced quiz functionality
let currentQuiz = {
    mode: null,
    questions: [],
    currentIndex: 0,
    answers: [],
    startTime: null,
    questionTimes: [],
    flagged: [],
    score: 0
};

function startMode(mode) {
    console.log("Starting quiz mode:", mode);
    
    // Get mode configuration
    const config = modeConfigs[mode] || modeConfigs.practice;
    
    // Reset quiz data
    currentQuiz = {
        mode: mode,
        questions: [],
        currentIndex: 0,
        answers: [],
        startTime: Date.now(),
        questionTimes: [],
        flagged: [],
        score: 0
    };
    
    // Select random questions for this mode
    const shuffled = [...questions].sort(() => Math.random() - 0.5);
    currentQuiz.questions = shuffled.slice(0, config.questionCount);
    
    // Hide welcome screen
    const welcomeScreen = document.querySelector('.welcome-container, .home-screen, #welcome');
    if (welcomeScreen) welcomeScreen.style.display = 'none';
    
    // Create or show quiz container
    showQuizScreen();
    
    // Load first question
    loadQuestion(0);
    
    // Start timer
    startQuizTimer();
}

function showQuizScreen() {
    let quizContainer = document.getElementById('quiz-container');
    
    if (!quizContainer) {
        quizContainer = document.createElement('div');
        quizContainer.id = 'quiz-container';
        quizContainer.innerHTML = `
            <div class="quiz-header">
                <h2 id="mode-title">${modeConfigs[currentQuiz.mode]?.name || 'Practice Mode'}</h2>
                <div class="quiz-progress">
                    <span id="question-counter">Question 1 of ${currentQuiz.questions.length}</span>
                    <span id="timer">00:00</span>
                </div>
            </div>
            
            <div class="question-section">
                <h3 id="question-text"></h3>
                <div id="options-list" class="options-container"></div>
            </div>
            
            <div class="quiz-navigation">
                <button onclick="previousQuestion()" id="prev-btn">Previous</button>
                <button onclick="flagQuestion()" id="flag-btn">Flag</button>
                <button onclick="nextQuestion()" id="next-btn">Next</button>
                <button onclick="finishQuiz()" id="finish-btn" style="display:none;">Finish</button>
            </div>
        `;
        document.body.appendChild(quizContainer);
    }
    
    quizContainer.style.display = 'block';
}

function loadQuestion(index) {
    if (index >= currentQuiz.questions.length || index < 0) return;
    
    const question = currentQuiz.questions[index];
    currentQuiz.currentIndex = index;
    
    // Update question text
    document.getElementById('question-text').textContent = question.question;
    document.getElementById('question-counter').textContent = 
        `Question ${index + 1} of ${currentQuiz.questions.length}`;
    
    // Update options
    const optionsList = document.getElementById('options-list');
    optionsList.innerHTML = '';
    
    question.options.forEach((option, i) => {
        const optionEl = document.createElement('div');
        optionEl.className = 'option-item';
        optionEl.innerHTML = `
            <input type="radio" id="option-${i}" name="answer" value="${i}">
            <label for="option-${i}">${option}</label>
        `;
        optionEl.onclick = () => selectOption(i);
        optionsList.appendChild(optionEl);
    });
    
    // Restore previous answer if exists
    if (currentQuiz.answers[index] !== undefined) {
        const radio = document.getElementById(`option-${currentQuiz.answers[index]}`);
        if (radio) radio.checked = true;
    }
    
    // Update navigation buttons
    document.getElementById('prev-btn').disabled = (index === 0);
    document.getElementById('next-btn').style.display = 
        (index === currentQuiz.questions.length - 1) ? 'none' : 'inline-block';
    document.getElementById('finish-btn').style.display = 
        (index === currentQuiz.questions.length - 1) ? 'inline-block' : 'none';
}

function selectOption(optionIndex) {
    currentQuiz.answers[currentQuiz.currentIndex] = optionIndex;
}

function nextQuestion() {
    saveCurrentAnswer();
    if (currentQuiz.currentIndex < currentQuiz.questions.length - 1) {
        loadQuestion(currentQuiz.currentIndex + 1);
    }
}

function previousQuestion() {
    saveCurrentAnswer();
    if (currentQuiz.currentIndex > 0) {
        loadQuestion(currentQuiz.currentIndex - 1);
    }
}

function flagQuestion() {
    const index = currentQuiz.currentIndex;
    if (currentQuiz.flagged.includes(index)) {
        currentQuiz.flagged = currentQuiz.flagged.filter(i => i !== index);
        document.getElementById('flag-btn').textContent = 'Flag';
    } else {
        currentQuiz.flagged.push(index);
        document.getElementById('flag-btn').textContent = 'Unflag';
    }
}

function saveCurrentAnswer() {
    const selected = document.querySelector('input[name="answer"]:checked');
    if (selected) {
        currentQuiz.answers[currentQuiz.currentIndex] = parseInt(selected.value);
    }
}

function finishQuiz() {
    saveCurrentAnswer();
    calculateResults();
}

function calculateResults() {
    let correct = 0;
    
    currentQuiz.questions.forEach((question, i) => {
        if (currentQuiz.answers[i] === question.correct) {
            correct++;
        }
    });
    
    const totalTime = Math.round((Date.now() - currentQuiz.startTime) / 1000);
    const score = Math.round((correct / currentQuiz.questions.length) * 100);
    
    const results = {
        score: score,
        correct: correct,
        total: currentQuiz.questions.length,
        time: totalTime,
        flagged: currentQuiz.flagged.length,
        mode: currentQuiz.mode,
        date: new Date().toISOString()
    };
    
    // Save results
    localStorage.setItem('lastQuizResults', JSON.stringify(results));
    
    // Show results screen
    showResultsScreen(results);
}

function showResultsScreen(results) {
    // Hide quiz container
    document.getElementById('quiz-container').style.display = 'none';
    
    // Create or update results screen
    let resultsScreen = document.getElementById('results-screen');
    if (!resultsScreen) {
        resultsScreen = document.createElement('div');
        resultsScreen.id = 'results-screen';
        document.body.appendChild(resultsScreen);
    }
    
    resultsScreen.innerHTML = `
        <h2>📊 Performance Analysis</h2>
        <div class="results-stats">
            <div class="stat-box">
                <div class="stat-value">${results.score}%</div>
                <div class="stat-label">Final Score</div>
            </div>
            <div class="stat-box">
                <div class="stat-value">${results.correct}/${results.total}</div>
                <div class="stat-label">Correct Answers</div>
            </div>
            <div class="stat-box">
                <div class="stat-value">${Math.floor(results.time / 60)}</div>
                <div class="stat-label">Total Minutes</div>
            </div>
            <div class="stat-box">
                <div class="stat-value">${results.score >= 70 ? 'PASS ✅' : 'FAIL ❌'}</div>
                <div class="stat-label">Result</div>
            </div>
        </div>
        <div class="results-actions">
            <button onclick="location.reload()">Take Another Exam</button>
            <button onclick="reviewQuiz()">Review Answers</button>
        </div>
    `;
    
    resultsScreen.style.display = 'block';
}

function startQuizTimer() {
    setInterval(() => {
        if (currentQuiz.startTime) {
            const elapsed = Math.floor((Date.now() - currentQuiz.startTime) / 1000);
            const minutes = Math.floor(elapsed / 60);
            const seconds = elapsed % 60;
            const timerEl = document.getElementById('timer');
            if (timerEl) {
                timerEl.textContent = `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
            }
        }
    }, 1000);
}

// Make functions globally available
window.startMode = startMode;
window.nextQuestion = nextQuestion;
window.previousQuestion = previousQuestion;
window.flagQuestion = flagQuestion;
window.finishQuiz = finishQuiz;
window.reviewQuiz = function() {
    alert('Review feature coming soon!');
};

console.log("Quiz system initialized!");

// KEEP ALL YOUR SERVICE WORKER CODE BELOW THIS LINE
// Don't delete anything below here!

if ('serviceWorker' in navigator) {
    window.addEventListener('load', () => {
        navigator.serviceWorker.register('./sw.js')
            .then(registration => {
                console.log('ServiceWorker registration successful');
                
                // Listen for updates
                registration.addEventListener('updatefound', () => {
                    console.log('New service worker available');
                });
            })
            .catch(err => console.log('ServiceWorker registration failed: ', err));
    });
}
      
let deferredPrompt;
window.addEventListener('beforeinstallprompt', (e) => {
    // Prevent Chrome 67 and earlier from automatically showing the prompt
    e.preventDefault();
    // Stash the event so it can be triggered later
    deferredPrompt = e;
    console.log('Install prompt ready');
    
    // You could show a custom install button here if you want
// ============= DVA QUIZ COMPLETE FIX =============
// Add this at the END of your script, right before </script>

// Fix 1: Map questionBank to questions if needed
if (typeof questions === 'undefined' && typeof questionBank !== 'undefined') {
    window.questions = [];
    for (let category in questionBank) {
        if (Array.isArray(questionBank[category])) {
            window.questions = window.questions.concat(questionBank[category]);
        }
    }
    console.log('Questions loaded:', window.questions.length);
}

// Fix 2: Override broken startMode
window.startMode = function(mode) {
    console.log('Starting DVA Quiz - Mode:', mode);
    
    // Hide all screens
    document.querySelectorAll('.welcome-container, .mode-selection, .container, #welcomeContainer').forEach(el => {
        if (el) el.style.display = 'none';
    });
    
    // Show quiz container
    const quizContainer = document.getElementById('quizContainer');
    if (quizContainer) {
        quizContainer.style.display = 'block';
        quizContainer.style.opacity = '1';
        quizContainer.style.filter = 'none';
        
        // Remove any modal overlays
        document.querySelectorAll('.modal, .modal-backdrop, .overlay').forEach(el => {
            el.style.display = 'none';
        });
        
        // If using trainer object
        if (typeof trainer !== 'undefined') {
            trainer.currentMode = mode;
            trainer.selectedQuestions = questions.slice(0, 10); // Start with 10 questions
            trainer.currentQuestionIndex = 0;
            trainer.userAnswers = [];
            trainer.showQuestion();
        }
    } else {
        // Create simple quiz if container doesn't exist
        createSimpleQuiz(mode);
    }
};

// Fix 3: Create simple quiz UI if needed
function createSimpleQuiz(mode) {
    const quizHTML = `
        <div id="simpleQuiz" style="
            position: fixed;
            top: 20px;
            left: 50%;
            transform: translateX(-50%);
            width: 90%;
            max-width: 800px;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.3);
            z-index: 9999;
        ">
            <h2>DVA Practice Quiz - ${mode}</h2>
            <div id="questionArea">Loading questions...</div>
            <div id="quizControls" style="margin-top: 20px;">
                <button onclick="nextQuestion()" style="
                    padding: 10px 30px;
                    background: #007bff;
                    color: white;
                    border: none;
                    border-radius: 5px;
                    cursor: pointer;
                ">Next Question</button>
            </div>
        </div>
    `;
    
    document.body.insertAdjacentHTML('beforeend', quizHTML);
    
    // Initialize quiz
    window.currentQuizState = {
        mode: mode,
        questions: questions.slice(0, 20),
        currentIndex: 0,
        answers: [],
        score: 0
    };
    
    showQuestion();
}

// Clean wallet pollution on load and continuously
(function() {
    setInterval(() => {
        if (window.ethereum && window.ethereum.isTally) delete window.ethereum;
        if (window.tronWeb) delete window.tronWeb;
        if (window.solana) delete window.solana;
        if (window.walletRouter) delete window.walletRouter;
    }, 2000);
})();

// Fix 4: Show question function
window.showQuestion = function() {
    if (!window.currentQuizState) return;
    
    const state = window.currentQuizState;
    const question = state.questions[state.currentIndex];
    
    if (!question) {
        showResults();
        return;
    }
    
    const questionHTML = `
        <h3>Question ${state.currentIndex + 1} of ${state.questions.length}</h3>
        <p style="font-size: 18px; margin: 20px 0;">${question.question}</p>
        <div>
            ${question.options.map((opt, i) => `
                <div style="
                    margin: 10px 0;
                    padding: 15px;
                    background: #f8f9fa;
                    border: 2px solid #dee2e6;
                    border-radius: 5px;
                    cursor: pointer;
                " onclick="selectAnswer(${i}, this)" onmouseover="this.style.background='#e9ecef'" onmouseout="this.style.background='#f8f9fa'">
                    <input type="radio" name="answer" value="${i}" id="opt${i}">
                    <label for="opt${i}" style="cursor: pointer;">
                        ${String.fromCharCode(65 + i)}. ${opt}
                    </label>
                </div>
            `).join('')}
        </div>
    `;
    
    const questionArea = document.getElementById('questionArea');
    if (questionArea) {
        questionArea.innerHTML = questionHTML;
    }
};

// Fix 5: Answer selection
window.selectAnswer = function(index, element) {
    window.currentQuizState.answers[window.currentQuizState.currentIndex] = index;
    document.getElementById(`opt${index}`).checked = true;
    
    // Visual feedback
    document.querySelectorAll('[name="answer"]').forEach(el => {
        el.parentElement.style.background = '#f8f9fa';
    });
    element.style.background = '#d1ecf1';
};

// Fix 6: Next question
window.nextQuestion = function() {
    if (!window.currentQuizState) return;
    
    window.currentQuizState.currentIndex++;
    
    if (window.currentQuizState.currentIndex >= window.currentQuizState.questions.length) {
        showResults();
    } else {
        showQuestion();
    }
};

// Fix 7: Show results without NaN - IMPROVED VERSION
window.showResults = function() {
    if (!window.currentQuizState) return;
    
    const state = window.currentQuizState;
    let correct = 0;
    
    state.questions.forEach((q, i) => {
        if (state.answers[i] === q.correct) correct++;
    });
    
    const total = state.questions.length || 1;
    const score = Math.round((correct / total) * 100) || 0;
    
    const resultsHTML = `
        <div style="text-align: center;">
            <h2>Quiz Complete!</h2>
            <div style="font-size: 48px; font-weight: bold; color: ${score >= 70 ? '#28a745' : '#dc3545'};">
                ${score}%
            </div>
            <p style="font-size: 20px;">You got ${correct} out of ${total} questions correct</p>
            <div style="margin-top: 30px;">
                <button onclick="takeAnotherQuiz()" style="
                    padding: 15px 40px;
                    background: #28a745;
                    color: white;
                    border: none;
                    border-radius: 5px;
                    font-size: 18px;
                    cursor: pointer;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.2);
                " onmouseover="this.style.background='#218838'" onmouseout="this.style.background='#28a745'">Take Another Quiz</button>
            </div>
        </div>
    `;
    
    const container = document.getElementById('simpleQuiz') || document.getElementById('questionArea');
    if (container) {
        container.innerHTML = resultsHTML;
    }
};

// NEW FUNCTION: Properly reset quiz with wallet cleanup
window.takeAnotherQuiz = function() {
    console.log('🔄 Resetting quiz and cleaning wallet interference...');
    
    // Clean wallet interference
    if (window.ethereum) delete window.ethereum;
    if (window.tronWeb) delete window.tronWeb;
    if (window.solana) delete window.solana;
    if (window.walletRouter) delete window.walletRouter;
    
    // Clear all quiz state
    localStorage.clear();
    sessionStorage.clear();
    
    // Reset quiz state
    window.currentQuizState = null;
    
    // Clean service worker cache
    if ('caches' in window) {
        caches.keys().then(names => {
            names.forEach(name => {
                if (name.includes('quiz') || name.includes('exam')) {
                    caches.delete(name);
                }
            });
        });
    }
    
    // Unregister corrupted service workers
    if ('serviceWorker' in navigator) {
        navigator.serviceWorker.getRegistrations().then(function(registrations) {
            for(let registration of registrations) {
                registration.unregister();
            }
        });
    }
    
    // Force reload with cache bypass
    setTimeout(() => {
        window.location.href = window.location.origin + window.location.pathname + '?t=' + Date.now();
    }, 100);
};

// Fix 8: Remove grey overlay on page load
document.addEventListener('DOMContentLoaded', function() {
    // Remove any blocking overlays
    setTimeout(() => {
        document.querySelectorAll('.modal, .modal-backdrop, .overlay').forEach(el => {
            if (el.style.display !== 'none') {
                console.log('Removing overlay:', el.className);
                el.style.display = 'none';
            }
        });
    }, 100);
    
    // Additional: Fix "Take Another Exam" button if it exists on the page
    document.addEventListener('click', function(e) {
        const target = e.target;
        const buttonText = target.textContent || target.innerText || '';
        
        if (buttonText.includes('Take Another Exam') || buttonText.includes('Take Another Quiz')) {
            e.preventDefault();
            e.stopPropagation();
            takeAnotherQuiz();
        }
    }, true);

    console.log('✅ DVA Quiz Fix Loaded with Wallet Interference Protection - Click any mode to start!');
});

</script>
</body>
</html>
        
